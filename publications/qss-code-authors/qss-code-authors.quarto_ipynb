{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Code Contribution and Scientific Authorship\"\n",
        "authors:\n",
        "  - name: \"Eva Maxfield Brown\"\n",
        "    email: evamxb@uw.edu\n",
        "    orcid: 0000-0003-2564-0373\n",
        "    affiliations:\n",
        "      - ref: 1\n",
        "    attributes:\n",
        "        corresponding: true\n",
        "  - name: \"Isaac Slaughter\"\n",
        "    orcid: 0000-0002-1911-2374\n",
        "    affiliations:\n",
        "      - ref: 1\n",
        "  - name: \"Nicholas Weber\"\n",
        "    orcid: 0000-0002-6008-3763\n",
        "    affiliations:\n",
        "      - ref: 1\n",
        "\n",
        "affiliations:\n",
        "  - id: 1\n",
        "    name: University of Washington Information School\n",
        "\n",
        "abstract: |\n",
        "  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur eget porta erat. Morbi consectetur est vel gravida pretium. Suspendisse ut dui eu ante cursus gravida non sed sem. Nullam sapien tellus, commodo id velit id, eleifend volutpat quam. Phasellus mauris velit, dapibus finibus elementum vel, pulvinar non tellus. Nunc pellentesque pretium diam, quis maximus dolor faucibus id. Nunc convallis sodales ante, ut ullamcorper est egestas vitae. Nam sit amet enim ultrices, ultrices elit pulvinar, volutpat risus.\n",
        "\n",
        "## Basics\n",
        "bibliography: main.bib\n",
        "\n",
        "## Number sections (required for section cross ref)\n",
        "number-sections: true\n",
        "\n",
        "## Citation Style Language\n",
        "# See https://github.com/citation-style-language/styles for more options\n",
        "# We default to PNAS (Proceedings of the National Academy of Sciences)\n",
        "# csl: support/acm-proceedings.csl\n",
        "\n",
        "## Specific for target format\n",
        "format:\n",
        "  html:\n",
        "    standalone: true\n",
        "    embed-resources: true\n",
        "    toc: true\n",
        "    toc-location: left\n",
        "    reference-location: margin\n",
        "    citation-location: document\n",
        "    execute:\n",
        "      echo: false\n",
        "      warning: false\n",
        "\n",
        "  elsevier-pdf:\n",
        "    toc: false\n",
        "    execute:\n",
        "      echo: false\n",
        "      warning: false\n",
        "    include-in-header:  \n",
        "      - text: |\n",
        "          \\usepackage{booktabs}    % For professional-looking tables\n",
        "          \\usepackage{multirow}    % For merged cells\n",
        "          \\usepackage{siunitx}     % For number alignment\n",
        "          \\usepackage[table]{xcolor}  % The 'table' option is crucial for \\rowcolors to work\n",
        "          \\usepackage{float}\n",
        "          \\floatplacement{table}{H}\n",
        "          \\usepackage{lineno}\n",
        "          \\linenumbers\n",
        "        \n",
        "    journal:\n",
        "      cite-style: authoryear\n",
        "      name: \"in-progress\"\n",
        "      model: \"3p\"\n",
        "      layout: \"onecolumn\"\n",
        "\n",
        "    # Word count\n",
        "    citeproc: false\n",
        "    filters:\n",
        "      - at: pre-quarto\n",
        "        path: _extensions/andrewheiss/wordcount/citeproc.lua\n",
        "      - at: pre-quarto\n",
        "        path: _extensions/andrewheiss/wordcount/wordcount.lua\n",
        "---\n",
        "\n",
        "\n",
        "# Introduction\n"
      ],
      "id": "bb199b5e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "\n",
        "from collections import OrderedDict\n",
        "from datetime import datetime\n",
        "from io import StringIO\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import colormaps as cmaps\n",
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import opinionated  # noqa\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "from sci_soft_models.dev_author_em import data as sci_soft_models_data\n",
        "from scipy.stats import (\n",
        "    binomtest,\n",
        "    chi2_contingency,\n",
        "    pearsonr,\n",
        ")\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "from sqlalchemy import text, create_engine\n",
        "\n",
        "from rs_graph.db import models as db_models\n",
        "from rs_graph.db.constants import PROD_DATABASE_FILEPATH\n",
        "\n",
        "# Get db engine for production database\n",
        "db_conn = create_engine(f\"sqlite:///{PROD_DATABASE_FILEPATH}\")\n",
        "\n",
        "# Set seaborn style\n",
        "plt.style.use(\"opinionated_rc\")\n",
        "sns.set_palette(\n",
        "    cmaps.bold[2:]._colors.tolist(),\n",
        ")\n",
        "\n",
        "# Get or set USE_SAMPLE\n",
        "if \"QSS_CODE_AUTHORSHIP_USE_SAMPLE\" in os.environ:\n",
        "    USE_SAMPLE = bool(int(os.environ[\"QSS_CODE_AUTHORSHIP_USE_SAMPLE\"]))\n",
        "else:\n",
        "    USE_SAMPLE = True\n",
        "# USE_SAMPLE = False"
      ],
      "id": "f2028163",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def read_table(table: str) -> pd.DataFrame:\n",
        "    return pd.read_sql(text(f\"SELECT * FROM {table}\"), db_conn)\n",
        "\n",
        "\n",
        "# Read all data from database\n",
        "doc_repo_links = read_table(db_models.DocumentRepositoryLink.__tablename__)\n",
        "researchers = read_table(db_models.Researcher.__tablename__)\n",
        "devs = read_table(db_models.DeveloperAccount.__tablename__)\n",
        "documents = read_table(db_models.Document.__tablename__)\n",
        "document_contributors = read_table(db_models.DocumentContributor.__tablename__)\n",
        "repositories = read_table(db_models.Repository.__tablename__)\n",
        "repository_contributors = read_table(db_models.RepositoryContributor.__tablename__)\n",
        "topics = read_table(db_models.Topic.__tablename__)\n",
        "document_topics = read_table(db_models.DocumentTopic.__tablename__)\n",
        "dataset_sources = read_table(db_models.DatasetSource.__tablename__)\n",
        "researcher_dev_links = read_table(\n",
        "    db_models.ResearcherDeveloperAccountLink.__tablename__\n",
        ")\n",
        "document_alternate_dois = read_table(\n",
        "    db_models.DocumentAlternateDOI.__tablename__\n",
        ")\n",
        "\n",
        "# Drop all \"updated_datetime\" and \"created_datetime\" columns\n",
        "for df in [\n",
        "    doc_repo_links,\n",
        "    researchers,\n",
        "    devs,\n",
        "    documents,\n",
        "    document_contributors,\n",
        "    repositories,\n",
        "    repository_contributors,\n",
        "    topics,\n",
        "    document_topics,\n",
        "    dataset_sources,\n",
        "    researcher_dev_links,\n",
        "]:\n",
        "    df.drop(columns=[\"updated_datetime\", \"created_datetime\"], inplace=True)\n",
        "\n",
        "# Specifically drop doc_repo_links \"id\" column\n",
        "# It isn't used and will get in the way later when we do a lot of joins\n",
        "doc_repo_links.drop(columns=[\"id\"], inplace=True)\n",
        "\n",
        "# Construct reduced doc_repo_links\n",
        "original_doc_repo_links_len = len(doc_repo_links)\n",
        "doc_repo_links = doc_repo_links.drop_duplicates(subset=[\"document_id\"], keep=False)\n",
        "doc_repo_links = doc_repo_links.drop_duplicates(subset=[\"repository_id\"], keep=False)\n",
        "\n",
        "# Reduce other tables to only documents / repositories in the updated doc_repo_links\n",
        "documents = documents[documents[\"id\"].isin(doc_repo_links[\"document_id\"])]\n",
        "repositories = repositories[repositories[\"id\"].isin(doc_repo_links[\"repository_id\"])]\n",
        "document_contributors = document_contributors[\n",
        "    document_contributors[\"document_id\"].isin(documents[\"id\"])\n",
        "]\n",
        "repository_contributors = repository_contributors[\n",
        "    repository_contributors[\"repository_id\"].isin(repositories[\"id\"])\n",
        "]\n",
        "document_topics = document_topics[document_topics[\"document_id\"].isin(documents[\"id\"])]\n",
        "\n",
        "# Reduce researchers and devs to only those in the\n",
        "# updated document_contributors and repository_contributors\n",
        "researchers = researchers[\n",
        "    researchers[\"id\"].isin(document_contributors[\"researcher_id\"])\n",
        "]\n",
        "devs = devs[devs[\"id\"].isin(repository_contributors[\"developer_account_id\"])]\n",
        "researcher_dev_links = researcher_dev_links[\n",
        "    (\n",
        "        researcher_dev_links[\"researcher_id\"].isin(researchers[\"id\"])\n",
        "        & researcher_dev_links[\"developer_account_id\"].isin(devs[\"id\"])\n",
        "    )\n",
        "]\n",
        "\n",
        "# Sort document topics and keep first\n",
        "document_topics = document_topics.sort_values(\"score\", ascending=False)\n",
        "document_topics = document_topics.drop_duplicates(subset=[\"document_id\"], keep=\"first\")\n",
        "\n",
        "# Create document, document topic merged table\n",
        "merged_document_topics = pd.merge(\n",
        "    document_topics, topics, left_on=\"topic_id\", right_on=\"id\"\n",
        ")\n",
        "\n",
        "# Create basic merged tables\n",
        "merged_document_contributor_doc_repo_links = pd.merge(\n",
        "    document_contributors, doc_repo_links, left_on=\"document_id\", right_on=\"document_id\"\n",
        ")\n",
        "merged_repository_contributor_doc_repo_links = pd.merge(\n",
        "    repository_contributors,\n",
        "    doc_repo_links,\n",
        "    left_on=\"repository_id\",\n",
        "    right_on=\"repository_id\",\n",
        ")\n",
        "\n",
        "# Compute stats for data sources\n",
        "data_source_stats = []\n",
        "for _, data_source in dataset_sources.iterrows():\n",
        "    # Get total article-repo pairs\n",
        "    data_source_stats.append(\n",
        "        {\n",
        "            \"data_source\": data_source[\"name\"],\n",
        "            \"n_article_repo_pairs\": len(\n",
        "                doc_repo_links[doc_repo_links[\"dataset_source_id\"] == data_source[\"id\"]]\n",
        "            ),\n",
        "            \"n_authors\": merged_document_contributor_doc_repo_links.loc[\n",
        "                merged_document_contributor_doc_repo_links[\"dataset_source_id\"]\n",
        "                == data_source[\"id\"]\n",
        "            ][\"researcher_id\"].nunique(),\n",
        "            \"n_devs\": merged_repository_contributor_doc_repo_links.loc[\n",
        "                merged_repository_contributor_doc_repo_links[\"dataset_source_id\"]\n",
        "                == data_source[\"id\"]\n",
        "            ][\"developer_account_id\"].nunique(),\n",
        "        }\n",
        "    )\n",
        "\n",
        "# Create topic merged tables\n",
        "merged_doc_repo_links_topics = pd.merge(\n",
        "    doc_repo_links, document_topics, left_on=\"document_id\", right_on=\"document_id\"\n",
        ").merge(topics, left_on=\"topic_id\", right_on=\"id\")\n",
        "merged_doc_repo_links_topics_document_contributors = pd.merge(\n",
        "    merged_doc_repo_links_topics,\n",
        "    document_contributors,\n",
        "    left_on=\"document_id\",\n",
        "    right_on=\"document_id\",\n",
        ")\n",
        "merged_doc_repo_links_topics_repository_contributors = pd.merge(\n",
        "    merged_doc_repo_links_topics,\n",
        "    repository_contributors,\n",
        "    left_on=\"repository_id\",\n",
        "    right_on=\"repository_id\",\n",
        ")\n",
        "\n",
        "# Compute stats for domains\n",
        "domain_stats = []\n",
        "for domain in merged_doc_repo_links_topics.domain_name.unique():\n",
        "    # Get total article-repo pairs\n",
        "    domain_stats.append(\n",
        "        {\n",
        "            \"domain\": domain,\n",
        "            \"n_article_repo_pairs\": len(\n",
        "                merged_doc_repo_links_topics[\n",
        "                    merged_doc_repo_links_topics[\"domain_name\"] == domain\n",
        "                ]\n",
        "            ),\n",
        "            \"n_authors\": merged_doc_repo_links_topics_document_contributors.loc[\n",
        "                merged_doc_repo_links_topics_document_contributors[\"domain_name\"]\n",
        "                == domain\n",
        "            ][\"researcher_id\"].nunique(),\n",
        "            \"n_devs\": merged_doc_repo_links_topics_repository_contributors.loc[\n",
        "                merged_doc_repo_links_topics_repository_contributors[\"domain_name\"]\n",
        "                == domain\n",
        "            ][\"developer_account_id\"].nunique(),\n",
        "        }\n",
        "    )\n",
        "\n",
        "# Create document merged tables\n",
        "merged_doc_repo_links_documents = pd.merge(\n",
        "    doc_repo_links, documents, left_on=\"document_id\", right_on=\"id\"\n",
        ")\n",
        "merged_doc_repo_links_documents_document_contributors = pd.merge(\n",
        "    merged_doc_repo_links_documents,\n",
        "    document_contributors,\n",
        "    left_on=\"document_id\",\n",
        "    right_on=\"document_id\",\n",
        ")\n",
        "merged_doc_repo_links_documents_repository_contributors = pd.merge(\n",
        "    merged_doc_repo_links_documents,\n",
        "    repository_contributors,\n",
        "    left_on=\"repository_id\",\n",
        "    right_on=\"repository_id\",\n",
        ")\n",
        "\n",
        "# Compute stats for document types\n",
        "# This isn't a standard data pull\n",
        "# In short:\n",
        "# - pairs from PLOS are \"research articles\"\n",
        "# - pairs from JOSS are \"software articles\"\n",
        "# - pairs from SoftwareX are \"software articles\"\n",
        "# - pairs from Papers with Code / ArXiv are \"pre-prints\"\n",
        "#   UNLESS they have been published in a journal\n",
        "# All of those should be easy to assert / apply a label to with the exception\n",
        "# of Papers with Code / ArXiv pre-prints that have been published in a journal\n",
        "# In that case, we need to look at the existing document type in the database\n",
        "# If the document type is \"preprint\" use preprint, otherwise, if it's anything else,\n",
        "# use \"research article\"\n",
        "\n",
        "# Create a \"reduced_doc_types\" dataframe with document_id and \"reduced_doc_type\"\n",
        "# columns\n",
        "reduced_doc_types_rows = []\n",
        "# We can use the \"reduced_doc_types\" dataframe to calculate the stats\n",
        "\n",
        "# Iter over data sources even though we are looking for doc types\n",
        "for _, data_source in dataset_sources.iterrows():\n",
        "    # Get total article-repo pairs\n",
        "    doc_type = None\n",
        "    if data_source[\"name\"] in [\"plos\", \"joss\", \"softwarex\"]:\n",
        "        if data_source[\"name\"] == \"plos\":\n",
        "            doc_type = \"research article\"\n",
        "        else:\n",
        "            doc_type = \"software article\"\n",
        "\n",
        "        # Add all document_ids to reduced_doc_types_rows\n",
        "        reduced_doc_types_rows.extend(\n",
        "            [\n",
        "                {\"document_id\": doc_id, \"reduced_doc_type\": doc_type}\n",
        "                for doc_id in doc_repo_links[\n",
        "                    (doc_repo_links[\"dataset_source_id\"] == data_source[\"id\"])\n",
        "                ][\"document_id\"]\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    # Handle PwC\n",
        "    else:\n",
        "        # Get preprint pairs\n",
        "        preprint_pairs = merged_doc_repo_links_documents[\n",
        "            (merged_doc_repo_links_documents[\"dataset_source_id\"] == data_source[\"id\"])\n",
        "            & (merged_doc_repo_links_documents[\"document_type\"] == \"preprint\")\n",
        "        ]\n",
        "\n",
        "        # Add all document_ids to reduced_doc_types_rows\n",
        "        reduced_doc_types_rows.extend(\n",
        "            [\n",
        "                {\"document_id\": doc_id, \"reduced_doc_type\": \"preprint\"}\n",
        "                for doc_id in preprint_pairs[\"document_id\"]\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Get research article pairs\n",
        "        # This is the same just inverted to != \"preprint\"\n",
        "        research_article_pairs = merged_doc_repo_links_documents[\n",
        "            (merged_doc_repo_links_documents[\"dataset_source_id\"] == data_source[\"id\"])\n",
        "            & (merged_doc_repo_links_documents[\"document_type\"] != \"preprint\")\n",
        "        ]\n",
        "\n",
        "        # Add all document_ids to reduced_doc_types_rows\n",
        "        reduced_doc_types_rows.extend(\n",
        "            [\n",
        "                {\"document_id\": doc_id, \"reduced_doc_type\": \"research article\"}\n",
        "                for doc_id in research_article_pairs[\"document_id\"]\n",
        "            ]\n",
        "        )\n",
        "\n",
        "# Create reduced_doc_types dataframe\n",
        "reduced_doc_types = pd.DataFrame(reduced_doc_types_rows)"
      ],
      "id": "d9288fe1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Precompute DOI resolution statistics as that will be referenced\n",
        "# in the Data and Methods section\n",
        "\n",
        "# Groupby \"name\" and get percent have resolved dois\n",
        "per_dataset_doi_resolution = {}\n",
        "for dataset_source, group in doc_repo_links.merge(\n",
        "    dataset_sources,\n",
        "    left_on=\"dataset_source_id\",\n",
        "    right_on=\"id\",\n",
        ").groupby(\"name\"):\n",
        "    # Get unique articles in this group\n",
        "    unique_group_articles = group.drop_duplicates(subset=[\"document_id\"])\n",
        "\n",
        "    # Get the number of articles from this dataset\n",
        "    n_articles_in_group = len(unique_group_articles)\n",
        "    \n",
        "    # Get the number of articles with an alternate doi in this group\n",
        "    # Where the alternate doi isn't the same as the document doi\n",
        "    group_document_alternate_dois = document_alternate_dois[\n",
        "        (document_alternate_dois[\"document_id\"].isin(unique_group_articles[\"document_id\"]))\n",
        "        & (~document_alternate_dois[\"doi\"].str.lower().isin(documents[\"doi\"].str.lower()))\n",
        "    ]\n",
        "    n_articles_with_alt_doi = len(group_document_alternate_dois)\n",
        "\n",
        "    # Add to results\n",
        "    per_dataset_doi_resolution[dataset_source] = n_articles_with_alt_doi / n_articles_in_group\n",
        "\n",
        "# Get overall doi resolution\n",
        "n_articles = len(documents)\n",
        "\n",
        "# Get the number of articles with an alternate doi\n",
        "# Where the alternate doi isn't the same as the document doi\n",
        "unique_document_alternate_dois = document_alternate_dois[\n",
        "    ~document_alternate_dois[\"doi\"].str.lower().isin(documents[\"doi\"].str.lower())\n",
        "]\n",
        "n_articles_with_alt_doi = len(unique_document_alternate_dois)\n",
        "\n",
        "# Compute overall doi resolution\n",
        "overall_doi_resolution = n_articles_with_alt_doi / n_articles"
      ],
      "id": "4d75c8c6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The communal sharing of new knowledge versus individual recognition for discovery is an essential tension in science \\[\\@Kuhn77\\]. The resolution of this tension is perhaps most pointed in the publishing of articles where new scientific claims are made and credit is imperfectly allocated to the authors. The quantitative study of science has historically used the published record of scientific claims as its primary data source - benefiting greatly from standardization and curation of bibliographic information from indexes like the Scientific Citation Index [@Bergstrom et al].\n",
        "\n",
        "Contemporary scientific research is increasingly collaborative, complex, and computationally dependent. These innovations present new challenges to the quantitative study of science, as well as new opportunities to understand how new scientific knowledge is produced and credit is allocated. There has been a significant literature focused on the growth of author lists, and the increasing role of technicians that support complex scientific projects. However, there is surprisingly little research that focuses on alternative data sources that might ...\n",
        "\n",
        "For example, emergence of public code repositories alongside published research provides a unique opportunity to study this disconnect. Source code repositories maintain detailed records of who contributes what code and when, providing a window into the patterns of software development in scientific research. Like SCI - these data are accessible and standized - enabling robust quantitative research\n",
        "\n",
        "In this article we examine this tension anew -\n",
        "\n",
        "Using transaction histories from these repositories, we develop a predictive model that enables systematic matching between the authors of a scientific article and the developer accounts of related source code repositories. We identify several patterns that distinguish code contributions from scientific recognition by applying our predictive model across almost 140,000 paired research articles and repositories. We show that nearly 30% (n=X) of articles have non-author code-contributors - individuals who may have helped create the software but received no formal authorship credit. We find that code-contributing authors are associated with a modest increase in article-level impact metrics (\\~5.1% increase in citations per code-contributing author). However, these effects become statistically non-significant when controlling for domain, article type, and open access status. First authors are significantly more likely to be code contributors than other author positions across all conditions tested. We also find a negative relationship between coding frequency and scholarly impact: Authors who contribute code more frequently show progressively lower h-indices than their non-coding peers, a pattern that persists when controlling for publication count, and author's most common author position, domain, and article type.\n",
        "\n",
        "The primary contributions of this article are: (1) a predictive model for matching authors with developer account information that addresses challenges in identity resolution across platforms[^1]; (2) a dataset of linked authors and developers for \\~140,000 article-repository pairs, providing a resource for analyzing scientific software development patterns[^2]; and (3) analyses that reveal insights into the software development dynamics of research teams, including patterns of recognition, impact, and career implications for code contributors. These contributions provide evidence for the ongoing discussions about scientific recognition systems and raise questions about aligning institutional incentives with the spectrum of contributions that drive modern scientific progress.\n",
        "\n",
        "[^1]: Our predictive model is made available via Huggingface at <https://huggingface.co/evamxb/dev-author-em-clf> or via our Python package, sci-soft-models (<https://github.com/evamaxfield/sci-soft-models/>).\n",
        "\n",
        "[^2]: Our dataset is made available via Harvard Dataverse at <https://doi.org/10.7910/DVN/KPYVI1>. Portions of the dataset are restricted to keep researcher and developer information private. The full dataset is available upon request via the Harvard Dataverse. For details on interacting with this dataset, see our documentation at <https://github.com/evamaxfield/rs-graph>.\n",
        "\n",
        "The remainder of this paper proceeds as follows. First, we review related work regarding scientific software development and the recognition of code contributors in scientific credit systems. In addition, we introduce the specific hypotheses that guide our investigation. Next, we detail our data and methods, describing how we created a dataset of linked article-repository pairs, trained and evaluated our predictive model for entity matching, and applied our model across each article-repository pair. We then present our data analysis, focusing on article-level dynamics before moving to individual-level patterns, formally accepting or rejecting each hypothesis based on our findings. We conclude by discussing the results, limitations of our work, and areas for future improvement.\n",
        "\n",
        "# Background {#sec-background}\n",
        "\n",
        "## Research Team Composition and Scientific Impact\n",
        "\n",
        "Except for foundational methodological shifts, scientific recognition systems have favored experimental and theoretical contributions more than their methodological counterparts, with experimental and theoretical articles receiving higher citation rates [@Aksnes2006CitationRA; @LIU2023101432; @Chen2024ExploringSC]. However, computational methods have transformed researchers' work across all scientific disciplines. Modern scientific endeavors increasingly depend on sophisticated computational approaches, whether for processing large-scale experimental data, running complex simulations, or developing new methodological tools [@jin2015significance; @hampton2013big; @edwards2013knowledge; @mayernik2017assessing; @hasselbring2024researchsoftwarecategories].\n",
        "\n",
        "The computational evolution in scientific practice intersects with established findings about team dynamics in both research and software development. Prior research has shown that larger and more diverse teams typically produce higher-impact scientific work [@Franceschet2010TheEO; @Larivire2014TeamSM], while in software engineering, larger development teams tend to create more reliable software with fewer defects [@Wyss2023NothingBut]. These findings suggest that team size may be particularly important in scientific software development, where technical robustness and scientific innovation are crucial.\n",
        "\n",
        "The unique characteristics of scientific software development - including implementing novel algorithms, requiring deep domain knowledge, and an increased emphasis on reproducibility [@muna2016astropyproblem; @Howison2013IncentivesAI] - make team composition especially relevant. Larger development teams may enhance scientific impact through multiple mechanisms: they can produce more robust and generalizable software tools for methodological contributions while enabling more sophisticated computational analyses and larger-scale data processing for experimental work. Given these patterns in team dynamics, software development practices, and the computational transformation of scientific work, we propose:\n",
        "\n",
        "*H1: The number of individuals contributing code to a publication's associated repository positively correlates with the article's citation count.*\n",
        "\n",
        "## Author Roles and Technical Contributions\n",
        "\n",
        "Author positions in scientific publications signal specific roles and responsibilities, a relationship extensively studied through contribution role taxonomies like CRediT [@lariviere2016contributorship]. These studies reveal that first authors and corresponding authors, while occasionally the same individual [@chinchilla2022relationship], take on distinct responsibilities. Analyses of contribution patterns consistently show that software development, data analysis, and visualization tasks typically fall to first authors [@lariviere2016contributorship; @Jnior2016PatternsOA; @Larivire2020InvestigatingTD; @sauermann2017authorship]. Meanwhile, corresponding authors, whether or not they are also first authors, often maintain responsibility for research artifacts' long-term sustainability and reuse, which we believe may include the maintenance and documentation of software tools.\n",
        "\n",
        "Contribution records from source code repositories provide a unique method to verify these established contribution patterns. Given prior findings about the distribution of technical responsibilities within research teams, we expect these repository records to reflect similar patterns of engagement with software development:\n",
        "\n",
        "*H2a: First authors have higher code contribution rates than authors in other positions.*\n",
        "\n",
        "*H2b: Corresponding authors have higher code contribution rates than non-corresponding authors.*\n",
        "\n",
        "## Code Contribution and Individual Scientific Impact\n",
        "\n",
        "Despite the increasingly central role of software in science, researchers who develop scientific software face persistent challenges in receiving formal scientific recognition for their contributions. Prior work has shown that software developers in research settings are often relegated to acknowledgment sections rather than receiving authorship credit, even when their technical contributions are fundamental to the research [@Carver2022ASO; @olivier_philippe_2019_2585783].\n",
        "\n",
        "The challenge of recognition is compounded by inconsistent practices in software citation. While researchers have tried to standardize software citation, actual citation practices remain highly variable across fields and journals [@Lamprecht2020TowardsFP; @Katz2020RecognizingTV; @Smith2016SoftwareCP]. This variability challenges researchers who maintain and update existing software packages. While creating entirely new software may lead to dedicated publications and citations, the ongoing work of maintaining, debugging, and extending existing software - often crucial for scientific progress - typically generates less visible scientific credit [@howison2011incentives; @Howison2013IncentivesAI].\n",
        "\n",
        "These structural challenges in recognizing and citing software contributions suggest a potential misalignment between technical contributions and traditional scientific impact metrics. When researchers dedicate significant time to software development and maintenance, conventional bibliometric measures may not fully capture their contributions, regardless of the software's importance to the field. Whether through attribution practices that favor acknowledgments over authorship or citation patterns that undervalue maintenance work, multiple mechanisms could lead to lower traditional impact metrics for active code contributors. Based on these patterns in software recognition and citation, we hypothesize:\n",
        "\n",
        "*H3: The frequency with which individual researchers contribute code to their research projects is negatively correlated with their h-index.*\n",
        "\n",
        "# Data and Methods\n",
        "\n",
        "Our analysis examines the relationship between software development contributions and scientific credit attribution through a three-step process: (1) building a dataset of linked scientific articles and code repositories, (2) developing a predictive model to match article authors with developer accounts, and (3) analyzing patterns in these relationships.\n",
        "\n",
        "Our dataset integrates article-repository pairs from four sources, each with explicit mechanisms for code sharing: the Journal of Open Source Software (JOSS) and SoftwareX require code repositories as part of publication, Papers with Code directly connects preprints with implementations, and Public Library of Science (PLOS) articles include mandatory data and code availability statements that we mined for repository links. We focused exclusively on GitHub-hosted repositories, which represent the predominant platform for scientific software sharing [@Cao2023TheRO; @escamilla2022riseofgithub]. For each article-repository pair, we resolved DOIs via Semantic Scholar to ensure we captured the latest version of each publication, extracted publication metadata and author metrics through OpenAlex, and collected repository information via the GitHub API This created a comprehensive dataset with information about both the scientific and software development aspects of each project. All data was collected between October and November 2024.\n"
      ],
      "id": "e95661ad"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "exp_results = sci_soft_models_data.load_experimental_training_results()\n",
        "\n",
        "# Remove the \"time_pred\" and \"epoch_val\" columns\n",
        "exp_results = exp_results.drop(columns=[\"time_pred\", \"epoch_val\"])\n",
        "\n",
        "# Round accuracy, precision, recall, and f1 to 3 decimal places\n",
        "exp_results = exp_results.round(\n",
        "    {\n",
        "        \"accuracy\": 3,\n",
        "        \"precision\": 3,\n",
        "        \"recall\": 3,\n",
        "        \"f1\": 3,\n",
        "    }\n",
        ")\n",
        "\n",
        "# Replace \"no-optional-data\" with \"n/a\"\n",
        "exp_results[\"fieldset\"] = exp_results[\"fieldset\"].replace(\n",
        "    \"no-optional-data\", \"n/a\"\n",
        ")\n",
        "\n",
        "# Str replace references to \"dev_\" in \"fieldset\" column\n",
        "exp_results[\"fieldset\"] = exp_results[\"fieldset\"].str.replace(\"dev_\", \"\")\n",
        "\n",
        "# Str replace \"_\" with \" \" in \"fieldset\" column\n",
        "exp_results[\"fieldset\"] = exp_results[\"fieldset\"].str.replace(\"_\", \" \")\n",
        "\n",
        "# Str split fieldset values on \"-\" and rejoin with \", \"\n",
        "exp_results[\"fieldset\"] = exp_results[\"fieldset\"].str.split(\"-\").apply(\n",
        "    lambda x: \", \".join(x)\n",
        ")\n",
        "\n",
        "# Rename fieldset column to \"Optional Features\"\n",
        "exp_results = exp_results.rename(columns={\"fieldset\": \"Optional Feats.\"})\n",
        "\n",
        "# Capitalize column names\n",
        "exp_results.columns = exp_results.columns.str.title()\n",
        "\n",
        "# Get best model f1 and get the accuracy, precision, recall, and f1 for that model\n",
        "best_model_idx = exp_results[\"F1\"].idxmax()\n",
        "best_model_acc = exp_results.loc[best_model_idx, \"Accuracy\"]\n",
        "best_model_prec = exp_results.loc[best_model_idx, \"Precision\"]\n",
        "best_model_rec = exp_results.loc[best_model_idx, \"Recall\"]\n",
        "best_model_f1 = exp_results.loc[best_model_idx, \"F1\"]"
      ],
      "id": "7e5eaf55",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To match article authors with repository developer accounts, we developed a machine learning approach using transformer-based architectures. Entity matching presents unique challenges, as exact name or email matching is insufficient due to formatting variations and incomplete information (e.g., \"J. Doe\" vs. \"Jane Doe\" in publications, or use of institutional versus personal email addresses). While exact matching fails, there is typically high semantic overlap between author information and developer account details that our model can leverage. We created a gold-standard dataset of 3,000 annotated author-developer account pairs from JOSS publications, where two independent reviewers classified each pair as matching or non-matching. After systematic evaluation of three transformer architectures with various feature combinations, our optimal model (fine-tuned DeBERTa-v3-base including developer account username and display name in the training data) achieved an F1 score of `{python} float(best_model_f1)`, with `{python} float(best_model_prec)` precision and `{python} float(best_model_rec)` recall (detailed comparison available in the Appendix).\n",
        "\n",
        "Applying our model across all article-repository pairs yielded a large-scale dataset linking scientific authors and code contributors. As shown in @tbl-rs-graph-overall-counts, our full, unfiltered dataset contains approximately 140,000 unique article-repository pairs spanning multiple domains and publication types, with nearly 300,000 distinct authors and more than 150,000 developer accounts. From these, we identified almost 110,000 author-developer account relationships, creating a unique resource for investigating code contribution patterns in scientific teams. This dataset enables systematic examination of how software development work relates to scientific recognition and career trajectories. Both the dataset (<https://doi.org/10.7910/DVN/KPYVI1>) and model (<https://huggingface.co/evamxb/dev-author-em-clf>) are made publicly available to support further research.\n"
      ],
      "id": "bfb87c1e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compute stats\n",
        "doc_type_stats = reduced_doc_types.groupby(\"reduced_doc_type\").apply(\n",
        "    lambda x: {\n",
        "        \"doc_type\": x.name,\n",
        "        \"n_article_repo_pairs\": len(x),\n",
        "        \"n_authors\": merged_doc_repo_links_documents_document_contributors.loc[\n",
        "            merged_doc_repo_links_documents_document_contributors[\"document_id\"].isin(\n",
        "                x[\"document_id\"]\n",
        "            )\n",
        "        ][\"researcher_id\"].nunique(),\n",
        "        \"n_devs\": merged_doc_repo_links_documents_repository_contributors.loc[\n",
        "            merged_doc_repo_links_documents_repository_contributors[\"document_id\"].isin(\n",
        "                x[\"document_id\"]\n",
        "            )\n",
        "        ][\"developer_account_id\"].nunique(),\n",
        "    },\n",
        "    include_groups=False,\n",
        ")\n",
        "\n",
        "# Compute stats for access status\n",
        "access_stats = []\n",
        "for access_status_int, access_status_name in [\n",
        "    (0, \"Closed\"),\n",
        "    (1, \"Open\"),\n",
        "]:\n",
        "    # Get total article-repo pairs\n",
        "    access_stats.append(\n",
        "        {\n",
        "            \"access_status\": access_status_name,\n",
        "            \"n_article_repo_pairs\": len(\n",
        "                merged_doc_repo_links_documents[\n",
        "                    merged_doc_repo_links_documents[\"is_open_access\"]\n",
        "                    == access_status_int\n",
        "                ]\n",
        "            ),\n",
        "            \"n_authors\": merged_doc_repo_links_documents_document_contributors.loc[\n",
        "                merged_doc_repo_links_documents_document_contributors[\"is_open_access\"]\n",
        "                == access_status_int\n",
        "            ][\"researcher_id\"].nunique(),\n",
        "            \"n_devs\": merged_doc_repo_links_documents_repository_contributors.loc[\n",
        "                merged_doc_repo_links_documents_repository_contributors[\n",
        "                    \"is_open_access\"\n",
        "                ]\n",
        "                == access_status_int\n",
        "            ][\"developer_account_id\"].nunique(),\n",
        "        }\n",
        "    )\n",
        "\n",
        "# Compute totals\n",
        "total_article_repo_pairs = len(doc_repo_links)\n",
        "total_authors = merged_document_contributor_doc_repo_links[\"researcher_id\"].nunique()\n",
        "total_devs = merged_repository_contributor_doc_repo_links[\n",
        "    \"developer_account_id\"\n",
        "].nunique()"
      ],
      "id": "c96add8d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-format=\"html\"}"
      ],
      "id": "11c0de2e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def _get_html_for_counts_table(\n",
        "    domain_stats,\n",
        "    doc_type_stats,\n",
        "    access_stats,\n",
        "    data_source_stats,\n",
        "    total_article_repo_pairs,\n",
        "    total_authors,\n",
        "    total_devs,\n",
        "):\n",
        "    # Construct multi-row span HTML table\n",
        "    # Columns should be: \"n_article_repo_pairs\", \"n_authors\", \"n_devs\"\n",
        "    # Rows should be:\n",
        "    # \"By Data Source\", \"By Domain\", \"By Document Type\", \"By Access Status\", and \"Total\"\n",
        "\n",
        "    # HTML templates\n",
        "    stats_piece_inital_row_template = \"\"\"\n",
        "    <tr>\n",
        "    <td rowspan=\"{n_rows}\">{row_name}</td>\n",
        "    <td>{value_name}</td>\n",
        "    <td>{article_repo_pairs}</td>\n",
        "    <td>{authors}</td>\n",
        "    <td>{devs}</td>\n",
        "    </tr>\n",
        "    \"\"\".strip()\n",
        "\n",
        "    stats_piece_subsequent_row_template = \"\"\"\n",
        "    <tr>\n",
        "    <td>{value_name}</td>\n",
        "    <td>{article_repo_pairs}</td>\n",
        "    <td>{authors}</td>\n",
        "    <td>{devs}</td>\n",
        "    </tr>\n",
        "    \"\"\".strip()\n",
        "\n",
        "    # Iter over stats portions (and total)\n",
        "    stats_portions_html = []\n",
        "    for stats_portion, stats_name, value_key in [\n",
        "        (domain_stats, \"<b>By Domain</b>\", \"domain\"),\n",
        "        (doc_type_stats, \"<b>By Document Type</b>\", \"doc_type\"),\n",
        "        (access_stats, \"<b>By Access Status</b>\", \"access_status\"),\n",
        "        (data_source_stats, \"<b>By Data Source</b>\", \"data_source\"),\n",
        "        (\n",
        "            [\n",
        "                {\n",
        "                    \"empty\": \"\",\n",
        "                    \"n_article_repo_pairs\": f\"<b>{total_article_repo_pairs}</b>\",\n",
        "                    \"n_authors\": f\"<b>{total_authors}</b>\",\n",
        "                    \"n_devs\": f\"<b>{total_devs}</b>\",\n",
        "                }\n",
        "            ],\n",
        "            \"<b>Total</b>\",\n",
        "            \"empty\",\n",
        "        ),\n",
        "    ]:\n",
        "        # Order by value_key\n",
        "        if value_key != \"empty\":\n",
        "            stats_portion = sorted(\n",
        "                stats_portion, key=lambda x: x[value_key]\n",
        "            )\n",
        "\n",
        "        stats_portion_html = []\n",
        "        for i, stats_piece in enumerate(stats_portion):\n",
        "            if i == 0:\n",
        "                stats_portion_html.append(\n",
        "                    stats_piece_inital_row_template.format(\n",
        "                        n_rows=len(stats_portion),\n",
        "                        row_name=stats_name,\n",
        "                        value_name=stats_piece[value_key],\n",
        "                        article_repo_pairs=stats_piece[\"n_article_repo_pairs\"],\n",
        "                        authors=stats_piece[\"n_authors\"],\n",
        "                        devs=stats_piece[\"n_devs\"],\n",
        "                    )\n",
        "                )\n",
        "            else:\n",
        "                stats_portion_html.append(\n",
        "                    stats_piece_subsequent_row_template.format(\n",
        "                        value_name=stats_piece[value_key],\n",
        "                        article_repo_pairs=stats_piece[\"n_article_repo_pairs\"],\n",
        "                        authors=stats_piece[\"n_authors\"],\n",
        "                        devs=stats_piece[\"n_devs\"],\n",
        "                    )\n",
        "                )\n",
        "\n",
        "        stats_portions_html.append(\"\\n\".join(stats_portion_html))\n",
        "\n",
        "    # Concat and wrap in table\n",
        "    stats_table_html = f\"\"\"\n",
        "    <table>\n",
        "    <tr>\n",
        "        <th><b>Category</b></th>\n",
        "        <th><b>Subset</b></th>\n",
        "        <th><b>Article-Repository Pairs</b></th>\n",
        "        <th><b>Authors</b></th>\n",
        "        <th><b>Developers</b></th>\n",
        "    </tr>\n",
        "    {\" \".join(stats_portions_html)}\n",
        "    </table>\n",
        "    \"\"\".strip()\n",
        "\n",
        "    return stats_table_html"
      ],
      "id": "d318d080",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-rs-graph-overall-counts\n",
        "#| tbl-cap: Counts of Article-Repository Pairs, Authors, and Developers by Data Sources, Domains, Document Types, and Access Status in the Full, Unfiltered Dataset.\n",
        "\n",
        "stats_table_html = _get_html_for_counts_table(\n",
        "    domain_stats=domain_stats,\n",
        "    doc_type_stats=doc_type_stats,\n",
        "    access_stats=access_stats,\n",
        "    data_source_stats=data_source_stats,\n",
        "    total_article_repo_pairs=total_article_repo_pairs,\n",
        "    total_authors=total_authors,\n",
        "    total_devs=total_devs,\n",
        ")\n",
        "\n",
        "IPython.display.HTML(stats_table_html)"
      ],
      "id": "tbl-rs-graph-overall-counts",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.content-visible when-format=\"pdf\"}"
      ],
      "id": "029566c2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def _get_latex_for_counts_table(\n",
        "    caption,\n",
        "    label,\n",
        "    domain_stats,\n",
        "    doc_type_stats,\n",
        "    access_stats,\n",
        "    data_source_stats,\n",
        "    total_article_repo_pairs,\n",
        "    total_authors,\n",
        "    total_devs,\n",
        "):\n",
        "    # LaTeX templates\n",
        "    stats_piece_initial_row_template = \"\"\"    \\\\multirow{{{n_rows}}}{{*}}{{\\\\textbf{{{row_name}}}}} & \\\\cellcolor{{gray!10}}{value_name} & \\\\cellcolor{{gray!10}}{article_repo_pairs} & \\\\cellcolor{{gray!10}}{authors} & \\\\cellcolor{{gray!10}}{devs} \\\\\\\\\"\"\"\n",
        "\n",
        "    stats_piece_subsequent_row_template = \"\"\"    & {value_name} & {article_repo_pairs} & {authors} & {devs} \\\\\\\\\"\"\"\n",
        "\n",
        "    # Table header and footer templates\n",
        "    table_header_start = \"\"\"\\\\begin{{table}}\n",
        "    \\\\centering\n",
        "    \\\\small\n",
        "    \\\\caption{{{caption}}}\n",
        "    \\\\label{{{label}}}\n",
        "    \\\\begin{{tabular}}{{llrrr}}\n",
        "    \\\\toprule\n",
        "    \\\\textbf{{Category}} & \\\\textbf{{Subset}} & \\\\textbf{{Article-Repository Pairs}} & \\\\textbf{{Authors}} & \\\\textbf{{Developers}} \\\\\\\\\n",
        "    \\\\midrule\"\"\".format(\n",
        "        caption=caption,\n",
        "        label=label,\n",
        "    )\n",
        "\n",
        "    table_footer = \"\"\"\\\\bottomrule\n",
        "    \\\\end{tabular}\n",
        "    \\\\end{table}\"\"\"\n",
        "\n",
        "    # Generate table content\n",
        "    stats_portions_latex = []\n",
        "    for stats_portion, stats_name, value_key in [\n",
        "        (domain_stats, \"By Domain\", \"domain\"),\n",
        "        (doc_type_stats, \"By Document Type\", \"doc_type\"),\n",
        "        (access_stats, \"By Access Status\", \"access_status\"),\n",
        "        (data_source_stats, \"By Data Source\", \"data_source\"),\n",
        "        (\n",
        "            [\n",
        "                {\n",
        "                    \"empty\": \"\",\n",
        "                    \"n_article_repo_pairs\": total_article_repo_pairs,\n",
        "                    \"n_authors\": total_authors,\n",
        "                    \"n_devs\": total_devs,\n",
        "                }\n",
        "            ],\n",
        "            \"Total\",\n",
        "            \"empty\",\n",
        "        ),\n",
        "    ]:\n",
        "        # Order by value_key\n",
        "        if value_key != \"empty\":\n",
        "            stats_portion = sorted(\n",
        "                stats_portion, key=lambda x: x[value_key]\n",
        "            )\n",
        "\n",
        "        stats_portion_latex = []\n",
        "        if stats_name == \"Total\":\n",
        "            stats_portion_latex.append(\n",
        "                f\"    \\\\textbf{{Total}} & & \\\\textbf{{{stats_portion[0]['n_article_repo_pairs']}}} & \\\\textbf{{{stats_portion[0]['n_authors']}}} & \\\\textbf{{{stats_portion[0]['n_devs']}}} \\\\\\\\\"\n",
        "            )\n",
        "        else:\n",
        "            # Add the first row with the category label and colored cells\n",
        "            first_piece = stats_portion[0]\n",
        "            stats_portion_latex.append(\n",
        "                stats_piece_initial_row_template.format(\n",
        "                    n_rows=len(stats_portion),\n",
        "                    row_name=stats_name,\n",
        "                    value_name=first_piece[value_key],\n",
        "                    article_repo_pairs=first_piece[\"n_article_repo_pairs\"],\n",
        "                    authors=first_piece[\"n_authors\"],\n",
        "                    devs=first_piece[\"n_devs\"],\n",
        "                ).rstrip()\n",
        "            )\n",
        "            \n",
        "            # Add subsequent rows with alternating colors\n",
        "            for i, stats_piece in enumerate(stats_portion[1:]):\n",
        "                color_cmd = \"\\\\cellcolor{gray!10}\" if i % 2 == 1 else \"\"\n",
        "                stats_portion_latex.append(\n",
        "                    f\"    & {color_cmd}{stats_piece[value_key]} & {color_cmd}{stats_piece['n_article_repo_pairs']} & {color_cmd}{stats_piece['n_authors']} & {color_cmd}{stats_piece['n_devs']} \\\\\\\\\"\n",
        "                )\n",
        "\n",
        "        section_latex = \"\\n\".join(stats_portion_latex)\n",
        "        if stats_name != \"Total\":\n",
        "            section_latex += \"\\\\midrule\"\n",
        "        stats_portions_latex.append(section_latex)\n",
        "\n",
        "    # Combine all parts\n",
        "    stats_table_latex = f\"\"\"{table_header_start}\n",
        "    {\"\\n\".join(stats_portions_latex)}\n",
        "    {table_footer}\"\"\"\n",
        "\n",
        "    return stats_table_latex"
      ],
      "id": "854db2b6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "stats_table_latex = _get_latex_for_counts_table(\n",
        "    caption=\"Counts of Article-Repository Pairs, Authors, and Developers by Data Sources, Domains, Document Types, and Access Status in the Full, Unfiltered Dataset.\",\n",
        "    label=\"tbl-rs-graph-overall-counts\",\n",
        "    domain_stats=domain_stats,\n",
        "    doc_type_stats=doc_type_stats,\n",
        "    access_stats=access_stats,\n",
        "    data_source_stats=data_source_stats,\n",
        "    total_article_repo_pairs=total_article_repo_pairs,\n",
        "    total_authors=total_authors,\n",
        "    total_devs=total_devs,\n",
        ")\n",
        "\n",
        "IPython.display.Latex(stats_table_latex)"
      ],
      "id": "d4c36c42",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "# Analysis of Code Contributor Authorship and Development Dynamics of Research Teams\n",
        "\n",
        "## Software Development Dynamics Within Research Teams\n"
      ],
      "id": "35a26d37"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create subset documents\n",
        "docs_w_1_citation = documents.loc[documents[\"cited_by_count\"] >= 1].copy()\n",
        "\n",
        "# Take sample?\n",
        "if USE_SAMPLE:\n",
        "    docs_w_1_citation = docs_w_1_citation.sample(frac=0.02, random_state=12)\n",
        "\n",
        "# Subset to only certain columns\n",
        "docs_w_1_citation = docs_w_1_citation[\n",
        "    [\n",
        "        \"id\",\n",
        "        \"publication_date\",\n",
        "        \"cited_by_count\",\n",
        "        \"fwci\",\n",
        "        \"is_open_access\",\n",
        "    ]\n",
        "]\n",
        "\n",
        "# Rename id to document_id\n",
        "docs_w_1_citation = docs_w_1_citation.rename(columns={\"id\": \"document_id\"})\n",
        "\n",
        "# Merge repository id in\n",
        "docs_w_1_citation = docs_w_1_citation.merge(\n",
        "    doc_repo_links[[\n",
        "        \"document_id\",\n",
        "        \"repository_id\",\n",
        "    ]],\n",
        "    left_on=\"document_id\",\n",
        "    right_on=\"document_id\",\n",
        ")\n",
        "\n",
        "# Merge in document details (domain, document type)\n",
        "docs_w_1_citation = (\n",
        "    docs_w_1_citation.merge(\n",
        "        document_topics[[\"document_id\", \"topic_id\"]],\n",
        "        left_on=\"document_id\",\n",
        "        right_on=\"document_id\",\n",
        "    )\n",
        "    .merge(\n",
        "        repositories[[\"id\", \"creation_datetime\", \"last_pushed_datetime\"]],\n",
        "        left_on=\"repository_id\",\n",
        "        right_on=\"id\",\n",
        "    )\n",
        "    .drop(\n",
        "        columns=[\"id\"],\n",
        "    )\n",
        "    .merge(\n",
        "        topics[[\"id\", \"domain_name\"]],\n",
        "        left_on=\"topic_id\",\n",
        "        right_on=\"id\",\n",
        "    )\n",
        "    .drop(\n",
        "        columns=[\"id\", \"topic_id\"],\n",
        "    )\n",
        "    .merge(\n",
        "        reduced_doc_types,\n",
        "        left_on=\"document_id\",\n",
        "        right_on=\"document_id\",\n",
        "    )\n",
        "    .rename(\n",
        "        columns={\n",
        "            \"domain_name\": \"domain\",\n",
        "            \"reduced_doc_type\": \"article_type\",\n",
        "        }\n",
        "    )\n",
        ")\n",
        "\n",
        "# Drop any documents that have more than one repository (and vice versa)\n",
        "docs_w_1_citation = docs_w_1_citation.drop_duplicates(\n",
        "    subset=[\"document_id\"], keep=False\n",
        ")\n",
        "docs_w_1_citation = docs_w_1_citation.drop_duplicates(\n",
        "    subset=[\"repository_id\"], keep=False\n",
        ")\n",
        "\n",
        "# Iter over articles and get the team composition info\n",
        "team_composition_rows = []\n",
        "relationships_removed_from_confidence_thresh = set()\n",
        "for _, row in docs_w_1_citation.iterrows():\n",
        "    # Get a boolean value for \"no pushes after publication\"\n",
        "    repo_details = repositories.loc[repositories[\"id\"] == row[\"repository_id\"]].iloc[0]\n",
        "\n",
        "    # Get the number of authors\n",
        "    author_ids = document_contributors.loc[\n",
        "        document_contributors[\"document_id\"] == row[\"document_id\"]\n",
        "    ][\"researcher_id\"].unique()\n",
        "    n_authors = len(author_ids)\n",
        "\n",
        "    # Get the number of devs\n",
        "    dev_ids = repository_contributors.loc[\n",
        "        repository_contributors[\"repository_id\"] == row[\"repository_id\"]\n",
        "    ][\"developer_account_id\"].unique()\n",
        "    n_devs = len(dev_ids)\n",
        "\n",
        "    # Get the set of researcher_dev_links for the authors\n",
        "    author_dev_links = researcher_dev_links.loc[\n",
        "        researcher_dev_links[\"researcher_id\"].isin(author_ids)\n",
        "    ].sort_values(\"predictive_model_confidence\", ascending=False)\n",
        "\n",
        "    # Drop duplicates by developer_account_id (keeping first)\n",
        "    # as we may have accidently matched the same dev to the multiple authors\n",
        "    author_dev_links = author_dev_links.drop_duplicates(\n",
        "        subset=[\"developer_account_id\"],\n",
        "        keep=\"first\",\n",
        "    )\n",
        "\n",
        "    # Drop any author dev links that have less than 97% confidence\n",
        "    author_dev_links_filtered = author_dev_links.loc[\n",
        "        author_dev_links[\"predictive_model_confidence\"] >= 0.97\n",
        "    ]\n",
        "    for _, relationship_row in author_dev_links.loc[\n",
        "        author_dev_links[\"predictive_model_confidence\"] < 0.97\n",
        "    ].iterrows():\n",
        "        # If the relationship was removed, add it to the set\n",
        "        relationships_removed_from_confidence_thresh.add(\n",
        "            (relationship_row[\"researcher_id\"], relationship_row[\"developer_account_id\"])\n",
        "        )\n",
        "\n",
        "    # Get the set of dev_researcher_links for the devs\n",
        "    dev_researcher_links = researcher_dev_links.loc[\n",
        "        researcher_dev_links[\"developer_account_id\"].isin(dev_ids)\n",
        "    ].sort_values(\"predictive_model_confidence\", ascending=False)\n",
        "\n",
        "    # Drop duplicates by dev_id (keeping first)\n",
        "    # as we may have accidently matched the same author to the multiple devs\n",
        "    dev_researcher_links = dev_researcher_links.drop_duplicates(\n",
        "        subset=[\"researcher_id\"],\n",
        "        keep=\"first\",\n",
        "    )\n",
        "\n",
        "    # Drop any dev researcher links that have less than 97% confidence\n",
        "    dev_researcher_links_filtered = dev_researcher_links.loc[\n",
        "        dev_researcher_links[\"predictive_model_confidence\"] >= 0.97\n",
        "    ]\n",
        "    for _, relationship_row in dev_researcher_links.loc[\n",
        "        dev_researcher_links[\"predictive_model_confidence\"] < 0.97\n",
        "    ].iterrows():\n",
        "        # If the relationship was removed, add it to the set\n",
        "        relationships_removed_from_confidence_thresh.add(\n",
        "            (relationship_row[\"researcher_id\"], relationship_row[\"developer_account_id\"])\n",
        "        )\n",
        "\n",
        "    # Get the number of authors who were devs on this paper\n",
        "    n_author_devs = 0\n",
        "    n_author_non_devs = 0\n",
        "    for author_id in author_ids:\n",
        "        author_dev_ids = author_dev_links_filtered.loc[\n",
        "            author_dev_links_filtered[\"researcher_id\"] == author_id\n",
        "        ][\"developer_account_id\"].unique()\n",
        "\n",
        "        # Count\n",
        "        if any(author_dev_id in dev_ids for author_dev_id in author_dev_ids):\n",
        "            n_author_devs += 1\n",
        "        else:\n",
        "            n_author_non_devs += 1\n",
        "\n",
        "    # Get the number of devs who aren't authors\n",
        "    n_non_author_devs = 0\n",
        "    for dev_id in dev_ids:\n",
        "        dev_researcher_ids = dev_researcher_links_filtered.loc[\n",
        "            dev_researcher_links_filtered[\"developer_account_id\"] == dev_id\n",
        "        ][\"researcher_id\"].unique()\n",
        "\n",
        "        # No dev should be matched to more than 3 authors\n",
        "        if len(dev_researcher_ids) > 3:\n",
        "            continue\n",
        "\n",
        "        # Count\n",
        "        if not any(dev_researcher_id in author_ids for dev_researcher_id in dev_researcher_ids):\n",
        "            n_non_author_devs += 1\n",
        "\n",
        "    # Append\n",
        "    team_composition_rows.append(\n",
        "        {\n",
        "            \"document_id\": row[\"document_id\"],\n",
        "            \"repository_id\": row[\"repository_id\"],\n",
        "            \"author_ids\": author_ids.tolist(),\n",
        "            \"dev_ids\": dev_ids.tolist(),\n",
        "            \"n_authors\": n_authors,\n",
        "            \"n_author_non_devs\": n_author_non_devs,\n",
        "            \"n_author_devs\": n_author_devs,\n",
        "            \"n_non_author_devs\": n_non_author_devs,\n",
        "            \"n_devs\": n_devs,\n",
        "        }\n",
        "    )\n",
        "\n",
        "# Create dataframe\n",
        "team_composition = pd.DataFrame(team_composition_rows)\n",
        "\n",
        "# Merge with docs_w_1_citation\n",
        "team_composition = team_composition.merge(\n",
        "    docs_w_1_citation,\n",
        "    left_on=[\"document_id\", \"repository_id\"],\n",
        "    right_on=[\"document_id\", \"repository_id\"],\n",
        ")\n",
        "\n",
        "# Filter out papers with less than 3 authors or 1 dev\n",
        "team_composition = team_composition.loc[team_composition[\"n_authors\"] >= 3]\n",
        "team_composition = team_composition.loc[team_composition[\"n_devs\"] >= 1]\n",
        "\n",
        "# Convert datetimes to datetime\n",
        "team_composition[\"publication_date\"] = pd.to_datetime(\n",
        "    team_composition[\"publication_date\"],\n",
        "    utc=True,\n",
        ")\n",
        "team_composition[\"last_pushed_datetime\"] = pd.to_datetime(\n",
        "    team_composition[\"last_pushed_datetime\"],\n",
        "    utc=True,\n",
        ")\n",
        "team_composition[\"creation_datetime\"] = pd.to_datetime(\n",
        "    team_composition[\"creation_datetime\"],\n",
        "    utc=True,\n",
        ")\n",
        "\n",
        "# Calculate years since publication from 2024-11-01\n",
        "team_composition[\"years_since_publication\"] = (\n",
        "    pd.to_datetime(\"2024-11-01\", utc=True) - team_composition[\"publication_date\"]\n",
        ").dt.days / 365.25\n",
        "\n",
        "# Calculate repo creation to last push\n",
        "team_composition[\"repo_commit_duration\"] = (\n",
        "    team_composition[\"last_pushed_datetime\"] - team_composition[\"creation_datetime\"]\n",
        ").dt.days / 365.25\n",
        "\n",
        "# Create a \"days_since_last_push\" column\n",
        "team_composition[\"days_from_publication_to_last_push\"] = (\n",
        "    team_composition[\"last_pushed_datetime\"] - team_composition[\"publication_date\"]\n",
        ").dt.days\n",
        "\n",
        "# Create a \"days_since_last_push\" column\n",
        "team_composition[\"days_from_publication_to_last_push\"] = (\n",
        "    team_composition[\"last_pushed_datetime\"] - team_composition[\"publication_date\"]\n",
        ").dt.days\n",
        "\n",
        "# Must have a push within 90 days of publication\n",
        "team_comp_no_push_after_pub = team_composition.loc[\n",
        "    team_composition[\"days_from_publication_to_last_push\"] <= 90\n",
        "].copy()\n",
        "\n",
        "# Create a \"publication_year\" column\n",
        "team_comp_no_push_after_pub[\"publication_year\"] = team_comp_no_push_after_pub[\n",
        "    \"publication_date\"\n",
        "].dt.year\n",
        "\n",
        "# Drop columns that would conflict with conversion to float\n",
        "team_comp_no_push_after_pub = team_comp_no_push_after_pub.drop(\n",
        "    columns=[\n",
        "        \"publication_date\",\n",
        "        \"last_pushed_datetime\",\n",
        "        \"creation_datetime\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Get the number of authors which would be the 97th percentile\n",
        "n_authors_97th_percentile = team_comp_no_push_after_pub[\"n_authors\"].quantile(0.97)\n",
        "\n",
        "# Remove rows that are greater than 97th percentile for total authors\n",
        "team_comp_no_push_after_pub = team_comp_no_push_after_pub.loc[\n",
        "    team_comp_no_push_after_pub[\"n_authors\"]\n",
        "    <= team_comp_no_push_after_pub[\"n_authors\"].quantile(0.97)\n",
        "]\n",
        "\n",
        "# Get columns needed for counts table\n",
        "team_comp_no_push_after_pub_counts_table = team_comp_no_push_after_pub[\n",
        "    [\n",
        "        \"document_id\",\n",
        "        \"repository_id\",\n",
        "        \"author_ids\",\n",
        "        \"dev_ids\",\n",
        "        \"article_type\",\n",
        "        \"domain\",\n",
        "        \"is_open_access\",\n",
        "    ]\n",
        "].copy()\n",
        "\n",
        "# Drop author_ids and dev_ids from original dataframe\n",
        "team_comp_no_push_after_pub = team_comp_no_push_after_pub.drop(\n",
        "    columns=[\"author_ids\", \"dev_ids\"]\n",
        ")\n",
        "\n",
        "# Add dummies\n",
        "team_comp_no_push_after_pub_dummies = pd.get_dummies(\n",
        "    team_comp_no_push_after_pub,\n",
        "    columns=[\"article_type\", \"domain\"],\n",
        "    drop_first=True,\n",
        ")\n",
        "\n",
        "# Cast all to float\n",
        "team_comp_no_push_after_pub_dummies = team_comp_no_push_after_pub_dummies.astype(float)\n",
        "\n",
        "# Create metrics for overall\n",
        "metric_str_fmt = \"{met_mean:.1f}  {met_std:.1f}\"\n",
        "total_auths = metric_str_fmt.format(\n",
        "    met_mean=team_comp_no_push_after_pub[\"n_authors\"].mean(),\n",
        "    met_std=team_comp_no_push_after_pub[\"n_authors\"].std(),\n",
        ")\n",
        "total_auth_devs = metric_str_fmt.format(\n",
        "    met_mean=team_comp_no_push_after_pub[\"n_author_devs\"].mean(),\n",
        "    met_std=team_comp_no_push_after_pub[\"n_author_devs\"].std(),\n",
        ")\n",
        "total_auth_non_devs = metric_str_fmt.format(\n",
        "    met_mean=team_comp_no_push_after_pub[\"n_author_non_devs\"].mean(),\n",
        "    met_std=team_comp_no_push_after_pub[\"n_author_non_devs\"].std(),\n",
        ")\n",
        "total_non_auth_devs = metric_str_fmt.format(\n",
        "    met_mean=team_comp_no_push_after_pub[\"n_non_author_devs\"].mean(),\n",
        "    met_std=team_comp_no_push_after_pub[\"n_non_author_devs\"].std(),\n",
        ")\n",
        "\n",
        "# Get the number of papers which have great than zero non_author_devs\n",
        "n_papers_w_non_auth_devs = len(team_comp_no_push_after_pub.loc[\n",
        "    team_comp_no_push_after_pub[\"n_non_author_devs\"] > 0\n",
        "])\n",
        "pct_papers_w_non_auth_devs = round(\n",
        "    (n_papers_w_non_auth_devs / len(team_comp_no_push_after_pub)) * 100, 1\n",
        ")\n",
        "median_non_author_devs_for_papers_w_non_auth_devs = team_comp_no_push_after_pub.loc[\n",
        "    team_comp_no_push_after_pub[\"n_non_author_devs\"] > 0\n",
        "][\"n_non_author_devs\"].median()\n",
        "mean_non_author_devs_for_papers_w_non_auth_devs = metric_str_fmt.format(\n",
        "    met_mean=team_comp_no_push_after_pub.loc[\n",
        "        team_comp_no_push_after_pub[\"n_non_author_devs\"] > 0\n",
        "    ][\"n_non_author_devs\"].mean(),\n",
        "    met_std=team_comp_no_push_after_pub.loc[\n",
        "        team_comp_no_push_after_pub[\"n_non_author_devs\"] > 0\n",
        "    ][\"n_non_author_devs\"].std(),\n",
        ")\n",
        "\n",
        "# Merge counts table with dataset sources\n",
        "team_comp_no_push_after_pub_counts_table = team_comp_no_push_after_pub_counts_table.merge(\n",
        "    doc_repo_links[[\n",
        "        \"document_id\",\n",
        "        \"repository_id\",\n",
        "        \"dataset_source_id\",\n",
        "    ]],\n",
        "    on=[\"document_id\", \"repository_id\"],\n",
        "    how=\"left\",\n",
        ").merge(\n",
        "    dataset_sources.rename(\n",
        "        columns={\n",
        "            \"id\": \"dataset_source_id\", \"name\": \"dataset_source_name\"\n",
        "        }\n",
        "    ),\n",
        "    on=\"dataset_source_id\",\n",
        "    how=\"left\",\n",
        ")\n",
        "\n",
        "# Get counts by:\n",
        "# - domain\n",
        "# - article type\n",
        "# - open access status\n",
        "# - dataset source\n",
        "# - total\n",
        "\n",
        "def _get_counts_by_column(\n",
        "    df: pd.DataFrame,\n",
        "    column_name: str,\n",
        "    category: str,\n",
        "    subset_replace: dict | None = None,\n",
        ") -> dict:\n",
        "    counts = []\n",
        "    for value in df[column_name].unique():\n",
        "        subset = df.loc[df[column_name] == value]\n",
        "\n",
        "        # Replace value if subset_replace is provided\n",
        "        if subset_replace is not None and value in subset_replace:\n",
        "            value = subset_replace[value]\n",
        "\n",
        "        # Get counts\n",
        "        author_ids = set(subset[\"author_ids\"].explode().dropna().unique())\n",
        "        dev_ids = set(subset[\"dev_ids\"].explode().dropna().unique())\n",
        "\n",
        "        # Append counts\n",
        "        counts.append({\n",
        "            category: value,\n",
        "            \"n_article_repo_pairs\": len(subset),\n",
        "            \"n_authors\": len(author_ids),\n",
        "            \"n_devs\": len(dev_ids),\n",
        "        })\n",
        "\n",
        "    return counts\n",
        "\n",
        "team_comp_no_push_after_pub_counts_domain = _get_counts_by_column(\n",
        "    team_comp_no_push_after_pub_counts_table,\n",
        "    \"domain\",\n",
        "    \"domain\",\n",
        ")\n",
        "team_comp_no_push_after_pub_counts_article_type = _get_counts_by_column(\n",
        "    team_comp_no_push_after_pub_counts_table,\n",
        "    \"article_type\",\n",
        "    \"doc_type\",\n",
        ")\n",
        "team_comp_no_push_after_pub_counts_open_access = _get_counts_by_column(\n",
        "    team_comp_no_push_after_pub_counts_table,\n",
        "    \"is_open_access\",\n",
        "    \"access_status\",\n",
        "    subset_replace={\n",
        "        0: \"Closed\",\n",
        "        1: \"Open\",\n",
        "    },\n",
        ")\n",
        "team_comp_no_push_after_pub_counts_dataset_source = _get_counts_by_column(\n",
        "    team_comp_no_push_after_pub_counts_table,\n",
        "    \"dataset_source_name\",\n",
        "    \"data_source\",\n",
        ")\n",
        "\n",
        "team_comp_no_push_after_pub_total_pairs = len(\n",
        "    team_comp_no_push_after_pub_counts_table\n",
        ")\n",
        "team_comp_no_push_after_pub_total_authors = team_comp_no_push_after_pub_counts_table[\n",
        "    \"author_ids\"\n",
        "].explode().dropna().nunique()\n",
        "team_comp_no_push_after_pub_total_devs = team_comp_no_push_after_pub_counts_table[\n",
        "    \"dev_ids\"\n",
        "].explode().dropna().nunique()"
      ],
      "id": "f1b935d2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Understanding the composition and dynamics of software development teams provides essential context for analyzing how code contributions relate to scientific recognition and impact. To ensure reliable analysis, we focus on a subset of our article-repository pairs that meet several filtering conditions. First, we require that each article-repository pair have at least one citation, which helps ensure the research has received a basic level of engagement from the scientific community. Next, we require that repository commit activity must stop prior to 90 days past the date of article publication. Disallowing long-term projects ensures we do not include projects that may add additional code contributors later while still allowing a grace period during which developers can update repositories with additional documentation and publication information. We then subset the data to only include article-repository pairs with research teams of typical size by removing those with fewer than three authors and more than `{python} int(n_authors_97th_percentile)` authors, the 97th percentile for research team size. Finally, we filter out any author-developer pairs associated with these projects with predictive model confidence of less than 0.97 to ensure that we only include high-confidence matches[^3]. This filtering process results in a dataset of `{python} len(team_comp_no_push_after_pub)` article-repository pairs. A table with the counts of article-repository pairs, authors, and developers by data sources, domains, document types, and access status for this filtered dataset is shown in @tbl-team-comp-no-push-after-pub-counts.\n",
        "\n",
        "[^3]: @fig-dist-of-author-dev-pred-confidence shows the distribution of predictive model confidence scores for author-developer pairs to justify this threshold. We chose the 0.97 threshold to ensure that we only include high-confidence matches while retaining a large proportion of the data (\\~90,000 author-developer pairs) as less than 3000 author-developer-account pairs have a confidence less than 0.97 in the whole unfiltered dataset.\n",
        "\n",
        "Within this filtered dataset, we categorized individuals into three groups: code-contributing authors (CC-A) who both authored papers and contributed code to associated repositories, non-code-contributing authors (NCC-A) who authored papers but showed no evidence of code contributions, and code-contributing non-authors (CC-NA) who contributed code but received no authorship recognition. This categorization revealed that papers in our dataset typically have `{python} total_auths` total authors, with `{python} total_auth_devs` code-contributing authors and `{python} total_auth_non_devs` non-code-contributing authors. Beyond the author list, papers averaged `{python} total_non_auth_devs` code-contributing non-authors. @tbl-team-composition-counts details these distributions by domain, article type, and open access status.\n",
        "\n",
        "Perhaps most striking is our finding that `{python} n_papers_w_non_auth_devs` papers (`{python} pct_papers_w_non_auth_devs`%) have at least one code contributor who did not receive authorship recognition. Within this substantial subset of papers, we found an average of `{python} mean_non_author_devs_for_papers_w_non_auth_devs` unrecognized code contributors per paper. On average, only one code-contributing author per paper aligns with previous research by @Larivire2020InvestigatingTD, showing that technical tasks like data curation, formal analysis, visualization, and software development typically fall to first authors. However, our finding that over a quarter of papers have unrecognized code contributors suggests a more complex dynamic between software development and authorship recognition.\n"
      ],
      "id": "7939c8cb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a copy of the team_comp_no_push_after_pub dataframe\n",
        "team_comp_no_push_after_pub_counts = team_comp_no_push_after_pub.copy()\n",
        "\n",
        "# Replace binary values in \"is_open_access\" to \"Open Access\" and \"Closed Access\"\n",
        "team_comp_no_push_after_pub_counts[\"is_open_access\"] = (\n",
        "    team_comp_no_push_after_pub_counts.apply(\n",
        "        lambda x: \"Open\" if x[\"is_open_access\"] == 1 else \"Closed\",\n",
        "        axis=1,\n",
        "    )\n",
        ")\n",
        "\n",
        "# Count team composition values and always for each control variable\n",
        "control_vars = {\n",
        "    \"is_open_access\": \"OA Status\",\n",
        "    \"domain\": \"Domain\",\n",
        "    \"article_type\": \"Article Type\",\n",
        "    \"Overall\": \"Overall\",\n",
        "}\n",
        "control_var_tables = {}\n",
        "for control_var, control_display_name in control_vars.items():\n",
        "    if control_var == \"Overall\":\n",
        "        mean_table = (\n",
        "            team_comp_no_push_after_pub_counts[\n",
        "                [\"n_authors\", \"n_author_non_devs\", \"n_author_devs\", \"n_non_author_devs\"]\n",
        "            ]\n",
        "            .mean()\n",
        "            .to_frame()\n",
        "            .T\n",
        "        )\n",
        "        std_table = (\n",
        "            team_comp_no_push_after_pub_counts[\n",
        "                [\"n_authors\", \"n_author_non_devs\", \"n_author_devs\", \"n_non_author_devs\"]\n",
        "            ]\n",
        "            .std()\n",
        "            .to_frame()\n",
        "            .T\n",
        "        )\n",
        "\n",
        "        # Merge and format to string\n",
        "        count_table_rows = []\n",
        "        for _, row in mean_table.iterrows():\n",
        "            std_row = std_table.loc[std_table.index == row.name]\n",
        "            count_table_rows.append(\n",
        "                {\n",
        "                    \"Overall\": \"\",\n",
        "                    \"n_authors\": f\"{row['n_authors']:.1f}  {std_row['n_authors'].iloc[0]:.1f}\",\n",
        "                    \"n_author_non_devs\": f\"{row['n_author_non_devs']:.1f}  {std_row['n_author_non_devs'].iloc[0]:.1f}\",\n",
        "                    \"n_author_devs\": f\"{row['n_author_devs']:.1f}  {std_row['n_author_devs'].iloc[0]:.1f}\",\n",
        "                    \"n_non_author_devs\": f\"{row['n_non_author_devs']:.1f}  {std_row['n_non_author_devs'].iloc[0]:.1f}\",\n",
        "                }\n",
        "            )\n",
        "\n",
        "    else:\n",
        "        # Instead of taking median, let's return the mean and std in the following format\n",
        "        # \"mean  std\"\n",
        "        mean_table = (\n",
        "            team_comp_no_push_after_pub_counts.groupby(control_var)[\n",
        "                [\"n_authors\", \"n_author_non_devs\", \"n_author_devs\", \"n_non_author_devs\"]\n",
        "            ]\n",
        "            .mean()\n",
        "            .reset_index()\n",
        "        )\n",
        "        std_table = (\n",
        "            team_comp_no_push_after_pub_counts.groupby(control_var)[\n",
        "                [\"n_authors\", \"n_author_non_devs\", \"n_author_devs\", \"n_non_author_devs\"]\n",
        "            ]\n",
        "            .std()\n",
        "            .reset_index()\n",
        "        )\n",
        "\n",
        "        # Merge and format to string\n",
        "        count_table_rows = []\n",
        "        for _, row in mean_table.iterrows():\n",
        "            std_row = std_table.loc[std_table[control_var] == row[control_var]]\n",
        "            count_table_rows.append(\n",
        "                {\n",
        "                    control_var: row[control_var],\n",
        "                    \"n_authors\": f\"{row['n_authors']:.1f}  {std_row['n_authors'].iloc[0]:.1f}\",\n",
        "                    \"n_author_non_devs\": f\"{row['n_author_non_devs']:.1f}  {std_row['n_author_non_devs'].iloc[0]:.1f}\",\n",
        "                    \"n_author_devs\": f\"{row['n_author_devs']:.1f}  {std_row['n_author_devs'].iloc[0]:.1f}\",\n",
        "                    \"n_non_author_devs\": f\"{row['n_non_author_devs']:.1f}  {std_row['n_non_author_devs'].iloc[0]:.1f}\",\n",
        "                }\n",
        "            )\n",
        "\n",
        "    # Create dataframe\n",
        "    count_table = pd.DataFrame(count_table_rows)\n",
        "\n",
        "    # Change name of the control_var column to \"Subset\"\n",
        "    count_table = count_table.rename(columns={control_var: \"Subset\"})\n",
        "\n",
        "    # Order columns\n",
        "    count_table = count_table[\n",
        "        [\"Subset\", \"n_authors\", \"n_author_non_devs\", \"n_author_devs\", \"n_non_author_devs\"]\n",
        "    ]\n",
        "    \n",
        "    # Rename columns\n",
        "    count_table = count_table.rename(\n",
        "        columns={\n",
        "            \"n_authors\": \"Total Authors\",\n",
        "            \"n_author_non_devs\": \"NCC-A\",\n",
        "            \"n_author_devs\": \"CC-A\",\n",
        "            \"n_non_author_devs\": \"CC-NA\",\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Order alphabetically\n",
        "    count_table = count_table.sort_values(\"Subset\")\n",
        "    \n",
        "    # Append\n",
        "    control_var_tables[control_display_name] = count_table"
      ],
      "id": "509a6e22",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-format=\"html\"}"
      ],
      "id": "19b5af27"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-team-composition-counts\n",
        "#| tbl-cap: Mean and Standard Deviation of Non-Code-Contributing Authors (NCC-A), Code-Contributing Authors (CC-A), and Code-Contributing Non-Authors (CC-NA) Research Team Members by Domain, Article Type, and Open Access Status. Only includes research teams from article-repository pairs with a most recent commit no later than 90 days after publication and excludes research teams in the top 3% of total author sizes.\n",
        "\n",
        "# Construct multi-row span HTML table\n",
        "# Columns should be: \"Authors\", \"Code Contributing Authors\", \"Non-Author Code Contributors\"\n",
        "# Rows should be:\n",
        "# \"Open Access Status\", \"Domain\", \"Article Type\"\n",
        "\n",
        "# HTML templates\n",
        "count_piece_inital_row_template = \"\"\"\n",
        "<tr>\n",
        "  <td rowspan=\"{n_rows}\">{row_name}</td>\n",
        "  <td>{value_name}</td>\n",
        "  <td>{n_authors}</td>\n",
        "  <td>{n_author_non_devs}</td>\n",
        "  <td>{n_author_devs}</td>\n",
        "  <td>{n_non_author_devs}</td>\n",
        "</tr>\n",
        "\"\"\".strip()\n",
        "\n",
        "count_piece_bolded_row_template = \"\"\"\n",
        "<tr>\n",
        "  <td rowspan=\"{n_rows}\"><b>{row_name}</b></td>\n",
        "  <td><b>{value_name}</b></td>\n",
        "  <td><b>{n_authors}</b></td>\n",
        "  <td><b>{n_author_non_devs}</b></td>\n",
        "  <td><b>{n_author_devs}</b></td>\n",
        "  <td><b>{n_non_author_devs}</b></td>\n",
        "</tr>\n",
        "\"\"\".strip()\n",
        "\n",
        "count_piece_subsequent_row_template = \"\"\"\n",
        "<tr>\n",
        "  <td>{value_name}</td>\n",
        "  <td>{n_authors}</td>\n",
        "  <td>{n_author_non_devs}</td>\n",
        "  <td>{n_author_devs}</td>\n",
        "  <td>{n_non_author_devs}</td>\n",
        "</tr>\n",
        "\"\"\".strip()\n",
        "\n",
        "# Iter over stats portions (and total)\n",
        "count_portions_html = []\n",
        "for key, count_table in control_var_tables.items():\n",
        "    count_portion_html = []\n",
        "    for i, control_value in enumerate(count_table[\"Subset\"].unique()):\n",
        "        if i == 0:\n",
        "            if control_value == \"\":  # empty control value is for the \"Overall\" table\n",
        "                count_portion_html.append(\n",
        "                    count_piece_bolded_row_template.format(\n",
        "                        n_rows=len(count_table),\n",
        "                        row_name=key,\n",
        "                        value_name=control_value,\n",
        "                        n_authors=count_table.loc[\n",
        "                            count_table[\"Subset\"] == control_value,\n",
        "                            \"Total Authors\",\n",
        "                        ].iloc[0],\n",
        "                        n_author_non_devs=count_table.loc[\n",
        "                            count_table[\"Subset\"] == control_value,\n",
        "                            \"NCC-A\",\n",
        "                        ].iloc[0],\n",
        "                        n_author_devs=count_table.loc[\n",
        "                            count_table[\"Subset\"] == control_value,\n",
        "                            \"CC-A\",\n",
        "                        ].iloc[0],\n",
        "                        n_non_author_devs=count_table.loc[\n",
        "                            count_table[\"Subset\"] == control_value,\n",
        "                            \"CC-NA\",\n",
        "                        ].iloc[0],\n",
        "                    )\n",
        "                )\n",
        "            else:\n",
        "                count_portion_html.append(\n",
        "                    count_piece_inital_row_template.format(\n",
        "                        n_rows=len(count_table),\n",
        "                        row_name=key,\n",
        "                        value_name=control_value,\n",
        "                        n_authors=count_table.loc[\n",
        "                            count_table[\"Subset\"] == control_value,\n",
        "                            \"Total Authors\",\n",
        "                        ].iloc[0],\n",
        "                        n_author_non_devs=count_table.loc[\n",
        "                            count_table[\"Subset\"] == control_value,\n",
        "                            \"NCC-A\",\n",
        "                        ].iloc[0],\n",
        "                        n_author_devs=count_table.loc[\n",
        "                            count_table[\"Subset\"] == control_value,\n",
        "                            \"CC-A\",\n",
        "                        ].iloc[0],\n",
        "                        n_non_author_devs=count_table.loc[\n",
        "                            count_table[\"Subset\"] == control_value,\n",
        "                            \"CC-NA\",\n",
        "                        ].iloc[0],\n",
        "                    )\n",
        "                )\n",
        "        else:\n",
        "            count_portion_html.append(\n",
        "                count_piece_subsequent_row_template.format(\n",
        "                    value_name=control_value,\n",
        "                    n_authors=count_table.loc[\n",
        "                        count_table[\"Subset\"] == control_value,\n",
        "                        \"Total Authors\",\n",
        "                    ].iloc[0],\n",
        "                    n_author_non_devs=count_table.loc[\n",
        "                        count_table[\"Subset\"] == control_value,\n",
        "                        \"NCC-A\",\n",
        "                    ].iloc[0],\n",
        "                    n_author_devs=count_table.loc[\n",
        "                        count_table[\"Subset\"] == control_value,\n",
        "                        \"CC-A\",\n",
        "                    ].iloc[0],\n",
        "                    n_non_author_devs=count_table.loc[\n",
        "                        count_table[\"Subset\"] == control_value,\n",
        "                        \"CC-NA\",\n",
        "                    ].iloc[0],\n",
        "                )\n",
        "            )\n",
        "\n",
        "    count_portions_html.append(\"\\n\".join(count_portion_html))\n",
        "\n",
        "# Concat and wrap in table\n",
        "count_table_html = f\"\"\"\n",
        "<table>\n",
        "  <tr>\n",
        "    <th><b>Control</b></th>\n",
        "    <th><b>Subset</b></th>\n",
        "    <th><b>Total Authors</b></th>\n",
        "    <th><b>NCC-A</b></th>\n",
        "    <th><b>CC-A</b></th>\n",
        "    <th><b>CC-NA</b></th>\n",
        "  </tr>\n",
        "  {\" \".join(count_portions_html)}\n",
        "</table>\n",
        "\"\"\".strip()\n",
        "\n",
        "IPython.display.HTML(count_table_html)"
      ],
      "id": "tbl-team-composition-counts",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.content-visible when-format=\"pdf\"}"
      ],
      "id": "4ca05c79"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# LaTeX templates\n",
        "count_piece_initial_row_template = \"\"\"    \\\\multirow{{{n_rows}}}{{*}}{{\\\\textbf{{{row_name}}}}} & \\\\cellcolor{{gray!10}}{value_name} & \\\\cellcolor{{gray!10}}{n_authors} & \\\\cellcolor{{gray!10}}{n_author_non_devs} & \\\\cellcolor{{gray!10}}{n_author_devs} & \\\\cellcolor{{gray!10}}{n_non_author_devs} \\\\\\\\\"\"\"\n",
        "\n",
        "count_piece_bolded_row_template = \"\"\"    \\\\multirow{{{n_rows}}}{{*}}{{\\\\textbf{{{row_name}}}}} & \\\\textbf{{{value_name}}} & \\\\textbf{{{n_authors}}} & \\\\textbf{{{n_author_non_devs}}} & \\\\textbf{{{n_author_devs}}} & \\\\textbf{{{n_non_author_devs}}} \\\\\\\\\"\"\"\n",
        "\n",
        "count_piece_subsequent_row_template = \"\"\"    & {color_cmd}{value_name} & {color_cmd}{n_authors} & {color_cmd}{n_author_non_devs} & {color_cmd}{n_author_devs} & {color_cmd}{n_non_author_devs} \\\\\\\\\"\"\"\n",
        "\n",
        "# Table header and footer templates\n",
        "table_header = \"\"\"\\\\begin{table}\n",
        "\\\\centering\n",
        "\\\\small\n",
        "\\\\caption{Mean and Standard Deviation of Non-Code-Contributing Authors (NCC-A), Code-Contributing Authors (CC-A), and Code-Contributing Non-Authors (CC-NA) Research Team Members by Domain, Article Type, and Open Access Status. Only includes research teams from article-repository pairs with a most recent commit no later than 90 days after publication and excludes research teams in the top $3\\%$ of total author sizes.}\n",
        "\\\\label{tbl-team-composition-counts}\n",
        "\\\\begin{tabular}{llrrrr}\n",
        "\\\\toprule\\\\textbf{Control} & \\\\textbf{Subset} & \\\\textbf{Total Authors} & \\\\textbf{NCC-A} & \\\\textbf{CC-A} & \\\\textbf{CC-NA} \\\\\\\\\n",
        "\\\\midrule\"\"\"\n",
        "\n",
        "table_footer = \"\"\"\\\\bottomrule\n",
        "\\\\end{tabular}\n",
        "\\\\end{table}\"\"\"\n",
        "\n",
        "# Generate table content\n",
        "count_portions_latex = []\n",
        "for key, count_table in control_var_tables.items():\n",
        "    count_portion_latex = []\n",
        "    \n",
        "    # Get unique control values\n",
        "    control_values = count_table[\"Subset\"].unique()\n",
        "\n",
        "    # Handle overall\n",
        "    if key == \"Overall\":\n",
        "        count_portion_latex.append(\n",
        "            count_piece_bolded_row_template.format(\n",
        "                n_rows=1,\n",
        "                row_name=key,\n",
        "                value_name=\"\",\n",
        "                n_authors=total_auths,\n",
        "                n_author_non_devs=total_auth_non_devs,\n",
        "                n_author_devs=total_auth_devs,\n",
        "                n_non_author_devs=total_non_auth_devs,\n",
        "            ).rstrip()\n",
        "        )\n",
        "    \n",
        "    else:\n",
        "        # First row with category label\n",
        "        first_value = control_values[0]\n",
        "        count_portion_latex.append(\n",
        "            count_piece_initial_row_template.format(\n",
        "                n_rows=len(control_values),\n",
        "                row_name=key,\n",
        "                value_name=first_value,\n",
        "                n_authors=count_table.loc[\n",
        "                    count_table[\"Subset\"] == first_value,\n",
        "                    \"Total Authors\",\n",
        "                ].iloc[0],\n",
        "                n_author_non_devs=count_table.loc[\n",
        "                    count_table[\"Subset\"] == first_value,\n",
        "                    \"NCC-A\",\n",
        "                ].iloc[0],\n",
        "                n_author_devs=count_table.loc[\n",
        "                    count_table[\"Subset\"] == first_value,\n",
        "                    \"CC-A\",\n",
        "                ].iloc[0],\n",
        "                n_non_author_devs=count_table.loc[\n",
        "                    count_table[\"Subset\"] == first_value,\n",
        "                    \"CC-NA\",\n",
        "                ].iloc[0],\n",
        "            ).rstrip()\n",
        "        )\n",
        "        \n",
        "        # Subsequent rows with alternating colors\n",
        "        for i, control_value in enumerate(control_values[1:]):\n",
        "            color_cmd = \"\\\\cellcolor{gray!10}\" if i % 2 == 1 else \"\"\n",
        "            count_portion_latex.append(\n",
        "                count_piece_subsequent_row_template.format(\n",
        "                    color_cmd=color_cmd,\n",
        "                    value_name=control_value,\n",
        "                    n_authors=count_table.loc[\n",
        "                        count_table[\"Subset\"] == control_value,\n",
        "                        \"Total Authors\",\n",
        "                    ].iloc[0],\n",
        "                    n_author_non_devs=count_table.loc[\n",
        "                        count_table[\"Subset\"] == control_value,\n",
        "                        \"NCC-A\",\n",
        "                    ].iloc[0],\n",
        "                    n_author_devs=count_table.loc[\n",
        "                        count_table[\"Subset\"] == control_value,\n",
        "                        \"CC-A\",\n",
        "                    ].iloc[0],\n",
        "                    n_non_author_devs=count_table.loc[\n",
        "                        count_table[\"Subset\"] == control_value,\n",
        "                        \"CC-NA\",\n",
        "                    ].iloc[0],\n",
        "                ).rstrip()\n",
        "            )\n",
        "\n",
        "    section_latex = \"\\n\".join(count_portion_latex)\n",
        "    \n",
        "    if key != list(control_var_tables.keys())[-1]:  # Don't add midrule after last section\n",
        "        section_latex += \"\\\\midrule\"\n",
        "    count_portions_latex.append(section_latex)\n",
        "\n",
        "# Combine all parts\n",
        "count_table_latex = f\"\"\"{table_header}\n",
        "{\"\\n\".join(count_portions_latex)}\n",
        "{table_footer}\"\"\"\n",
        "\n",
        "IPython.display.Latex(count_table_latex)"
      ],
      "id": "541986d9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "When examining these patterns over time and across different team sizes (@fig-contributor-type-by-time-and-size), we found that the number of code-contributing authors and unrecognized contributors has remained relatively stable. This stability over time suggests that while the exclusion of code contributors from authorship is not worsening, it represents a persistent feature of scientific software development rather than a historical artifact or transition period in research practices. Similarly, the number of code-contributing non-authors remains constant even as team size grows, indicating that larger research teams do not necessarily adopt more inclusive authorship practices for code contributors, despite representing broader collaborative efforts.\n"
      ],
      "id": "543af217"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-contributor-type-by-time-and-size\n",
        "#| fig-cap: Average number of contributors per article, by contribution type along with A) the year the article was published, and B) the total number of authors included on the article. Only includes research teams from article-repository pairs with a most recent commit no later than 90 days after publication and excludes research teams in the top 3% of total author sizes for publication years with 50 or more articles. Shaded areas show the 95% confidence interval for the mean.\n",
        "\n",
        "# Make a copy for team size comparison\n",
        "team_comp_by_year_and_size = team_comp_no_push_after_pub.copy()\n",
        "\n",
        "# Rename columns\n",
        "team_comp_by_year_and_size = team_comp_by_year_and_size.rename(\n",
        "    columns={\n",
        "        \"n_authors\": \"Total Authors\",\n",
        "        \"publication_year\": \"Publication Year\",\n",
        "        \"n_author_devs\": \"Code-Contrib. Authors\",\n",
        "        \"n_author_non_devs\": \"Non-Code-Contrib. Authors\",\n",
        "        \"n_non_author_devs\": \"Code-Contrib. Non-Authors\",\n",
        "    }\n",
        ")\n",
        "\n",
        "# Get value count of publication year\n",
        "year_counts = team_comp_no_push_after_pub[\"publication_year\"].value_counts()\n",
        "\n",
        "# Drop years with less than 50 publications\n",
        "team_comp_by_year_and_size = team_comp_by_year_and_size.loc[\n",
        "    team_comp_no_push_after_pub[\"publication_year\"].isin(\n",
        "        year_counts.loc[lambda x: x >= 50].index\n",
        "    )\n",
        "]\n",
        "\n",
        "# First separate out the year and authors\n",
        "team_comp_by_year_and_size = team_comp_by_year_and_size.melt(\n",
        "    id_vars=[\n",
        "        \"document_id\",\n",
        "        \"Non-Code-Contrib. Authors\",\n",
        "        \"Code-Contrib. Authors\",\n",
        "        \"Code-Contrib. Non-Authors\",\n",
        "    ],\n",
        "    value_vars=[\n",
        "        \"Publication Year\",\n",
        "        \"Total Authors\",\n",
        "    ],\n",
        "    var_name=\"comparison_var\",\n",
        "    value_name=\"value\",\n",
        ")\n",
        "\n",
        "# Melt again to get the contributor type\n",
        "team_comp_by_year_and_size = team_comp_by_year_and_size.melt(\n",
        "    id_vars=[\n",
        "        \"document_id\",\n",
        "        \"comparison_var\",\n",
        "        \"value\",\n",
        "    ],\n",
        "    value_vars=[\n",
        "        \"Non-Code-Contrib. Authors\",\n",
        "        \"Code-Contrib. Authors\",\n",
        "        \"Code-Contrib. Non-Authors\",\n",
        "    ],\n",
        "    var_name=\"Contributor Type\",\n",
        "    value_name=\"Count\",\n",
        ")\n",
        "\n",
        "# Plot\n",
        "g = sns.relplot(\n",
        "    data=team_comp_by_year_and_size,\n",
        "    x=\"value\",\n",
        "    y=\"Count\",\n",
        "    col=\"comparison_var\",\n",
        "    hue=\"Contributor Type\",\n",
        "    hue_order=[\n",
        "        \"Non-Code-Contrib. Authors\",\n",
        "        \"Code-Contrib. Authors\",\n",
        "        \"Code-Contrib. Non-Authors\",\n",
        "    ],\n",
        "    kind=\"line\",\n",
        "    estimator=\"mean\",\n",
        "    legend=True,\n",
        "    facet_kws={\"sharex\": False, \"sharey\": False},\n",
        "    height=4,\n",
        ")\n",
        "\n",
        "# Iter over axes and set x labels\n",
        "for i, ax in enumerate(g.axes.flat):\n",
        "    if i == 0:\n",
        "        ax.set_title(\"A\")\n",
        "        ax.set_xlabel(\"Publication Year\")\n",
        "    else:\n",
        "        ax.set_title(\"B\")\n",
        "        ax.set_xlabel(\"Total Authors\")\n",
        "\n",
        "# Move legend to below the plot\n",
        "sns.move_legend(\n",
        "    g,\n",
        "    bbox_to_anchor=(0.4, -0.0001),\n",
        "    loc=\"upper center\",\n",
        "    ncol=3,\n",
        "    fontsize=12,\n",
        "    title_fontsize=14,\n",
        ")"
      ],
      "id": "fig-contributor-type-by-time-and-size",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modeling Article Citations\n",
        "\n",
        "Building upon previous work examining the effects of team size and team diversity on scientific impact and software quality (see @sec-background), we investigate how the number of code contributors within a research team may be associated with an article's research impact. We hypothesized that more code contributors might signal greater technical complexity in research, which may be associated with higher citation counts as the community builds upon more technically sophisticated works.\n",
        "\n",
        "Using our filtered dataset of article-repository pairs (@tbl-team-comp-no-push-after-pub-counts), we conducted multiple regression analyses to examine these relationships while controlling for various factors. Without controlling for domain, open access, or article type differences (@tbl-article-composition-overall), our analysis revealed a modest positive association between the number of code contributing authors and article citations, with each code-contributing author associated with a 5.1% increase in article citations (p \\< 0.001).\n",
        "\n",
        "When controlling for article type (@tbl-article-composition-type), we observed divergent patterns between preprints and research articles. For preprints, each code-contributing non-author was associated with a statistically significant 3.2% decrease in citations (p \\< 0.005). In contrast, research articles showed more positive associations: we found a significant positive relationship between code-contributing authors and citations (p \\< 0.001), though we cannot estimate the precise magnitude due to the non-significant main effect in the model. Additionally, each code-contributing non-author was associated with a 0.1% increase in expected citations for research articles (p \\< 0.001).\n",
        "\n",
        "Based on these findings, we ***partially accept*** our hypothesis (*H1*) that \"the number of individuals contributing code to a publication's associated repository positively correlates with the article's citation count.\" Several important nuances qualify this acceptance: the relationship is statistically significant but modest in magnitude and differs substantially between research articles (positive association) and preprints (negative association for non-author code contributors). These variations suggest that the relationship between code contributions and citation impact is context-dependent and more complex than initially hypothesized.\n"
      ],
      "id": "145fd2aa"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def compute_article_level_models(\n",
        "    y_col: str,\n",
        "    data: pd.DataFrame,\n",
        "    glm_family: sm.families.Family,\n",
        ") -> dict[str, sm.GLM]:\n",
        "    # Remove outliers\n",
        "    no_outliers = data[\n",
        "        data[y_col].between(\n",
        "            data[y_col].quantile(0.03),\n",
        "            data[y_col].quantile(0.97),\n",
        "        )\n",
        "    ].copy()\n",
        "\n",
        "    # Remove nans\n",
        "    no_outliers = no_outliers.dropna(subset=[y_col])\n",
        "\n",
        "    # Replace names\n",
        "    no_outliers = no_outliers.rename(\n",
        "        columns={\n",
        "            \"n_authors\": \"Total Authors\",\n",
        "            # \"n_author_non_devs\": \"Non-Code-Contrib. Authors\",\n",
        "            \"n_author_devs\": \"Code-Contrib. Authors\",\n",
        "            \"n_non_author_devs\": \"Code-Contrib. Non-Authors\",\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Replace other names (except const.) by removing `_` and title casing\n",
        "    no_outliers = no_outliers.rename(\n",
        "        columns=lambda x: x.replace(\"_\", \" \").title() if x not in [\"const\", y_col] else x\n",
        "    )\n",
        "\n",
        "    # Common features to use in all models\n",
        "    required_features = [\n",
        "        y_col,\n",
        "        \"Total Authors\",\n",
        "        # \"n_author_non_devs\",\n",
        "        \"Code-Contrib. Authors\",\n",
        "        \"Code-Contrib. Non-Authors\",\n",
        "        \"Years Since Publication\",\n",
        "    ]\n",
        "\n",
        "    \n",
        "\n",
        "    # Iter over different control variables and create models for each\n",
        "    models = {}\n",
        "    for control_var in [\n",
        "        \"no-control\",\n",
        "        \"Article Type\",\n",
        "        \"Domain\",\n",
        "        \"Is Open Access\",\n",
        "    ]:\n",
        "        if control_var != \"no-control\":\n",
        "            # Get control variable list\n",
        "            control_variables = [\n",
        "                col for col in no_outliers.columns if col.startswith(control_var)\n",
        "            ]\n",
        "\n",
        "            # Create control variable subset of the data\n",
        "            control_var_subset = no_outliers[required_features + control_variables].copy()\n",
        "\n",
        "            # Create interactions\n",
        "            for coding_status_col in [\"Code-Contrib. Authors\", \"Code-Contrib. Non-Authors\"]:\n",
        "                for control_col in control_variables:\n",
        "                    control_var_subset[f\"{coding_status_col}  {control_col}\"] = (\n",
        "                        control_var_subset[coding_status_col]\n",
        "                        * control_var_subset[control_col]\n",
        "                    )\n",
        "        else:\n",
        "            control_var_subset = no_outliers[required_features].copy()\n",
        "\n",
        "        # Drop inf and nan\n",
        "        control_var_subset = control_var_subset.replace(\n",
        "            [float(\"inf\"), -float(\"inf\")], float(\"nan\")\n",
        "        ).dropna()\n",
        "\n",
        "        # Create x and y\n",
        "        y = control_var_subset[y_col]\n",
        "        x = control_var_subset.drop(columns=[y_col])\n",
        "        x = sm.add_constant(x)\n",
        "\n",
        "        # Fit model\n",
        "        model = sm.GLM(y, x, family=glm_family).fit()\n",
        "        models[control_var] = model\n",
        "\n",
        "    return models"
      ],
      "id": "e4d826a2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# LaTeX templates\n",
        "model_results_row_template = \"\"\"    {bold_str}\\\\cellcolor{{{color}}}{var_name}{stars} & {bold_str}\\\\cellcolor{{{color}}}{coef:.2f} & {bold_str}\\\\cellcolor{{{color}}}{p:.2f} & {bold_str}\\\\cellcolor{{{color}}}{ci_low:.2f} & {bold_str}\\\\cellcolor{{{color}}}{ci_high:.2f} \\\\\\\\\"\"\"\n",
        "\n",
        "# Table header and footer templates\n",
        "model_results_table_header = \"\"\"\\\\begin{tabular}{l*{6}{r}}\n",
        "\\\\toprule\n",
        "\\\\textbf{Variable} & \\\\textbf{coef} & \\\\textbf{P>|z|} & \\\\multicolumn{2}{c}{\\\\textbf{[0.025 0.975]}} \\\\\\\\\n",
        "\\\\midrule\"\"\"\n",
        "\n",
        "model_results_table_footer = \"\"\"\\\\bottomrule\n",
        "\\\\end{tabular}\"\"\"\n",
        "\n",
        "def convert_model_results_to_printable_pdf_ready(\n",
        "    model: sm.GLM,\n",
        "    tbl_cap: str,\n",
        "    tbl_label: str,\n",
        ") -> tuple[str, pd.DataFrame]:\n",
        "    # Get only the dataframe\n",
        "    summary_simple_tab = model.summary().tables[1]\n",
        "    model_results_df = pd.read_html(StringIO(summary_simple_tab.as_html()), header=0, index_col=0)[0]\n",
        "\n",
        "    # Add exponentiated coef\n",
        "    model_results_df[\"exp(coef)\"] = np.exp(model_results_df[\"coef\"])\n",
        "\n",
        "    # Keep only the specified columns\n",
        "    model_results_df = model_results_df[\n",
        "        [\"exp(coef)\", \"coef\", \"z\", \"P>|z|\", \"[0.025\", \"0.975]\"]\n",
        "    ]\n",
        "\n",
        "    # Set index name to \"variable\"\n",
        "    model_results_df.index.name = \"variable\"\n",
        "    model_results_df = model_results_df.reset_index()\n",
        "\n",
        "    rows_latex = []\n",
        "    for i, result_row in model_results_df.iterrows():\n",
        "        # Determine if row should be bold and if stars should be added\n",
        "        is_significant = result_row[\"P>|z|\"] < 0.05\n",
        "        stars = \"\"\n",
        "        if result_row[\"P>|z|\"] < 0.001:\n",
        "            stars = \"~$^{***}$\"\n",
        "        elif result_row[\"P>|z|\"] < 0.01:\n",
        "            stars = \"~$^{**}$\"\n",
        "        elif result_row[\"P>|z|\"] < 0.05:\n",
        "            stars = \"~$^{*}$\"\n",
        "        \n",
        "        \n",
        "        \n",
        "        # Handle bolding - only for significant rows\n",
        "        if is_significant:\n",
        "            bold_str = \"\\\\bfseries\"\n",
        "        else:\n",
        "            bold_str = \"\"\n",
        "        \n",
        "        # Set background color for alternating rows\n",
        "        color = \"gray!10\" if i % 2 == 1 else \"white\"\n",
        "        \n",
        "        # Format the row\n",
        "        rows_latex.append(\n",
        "            model_results_row_template.format(\n",
        "                bold_str=bold_str,\n",
        "                var_name=result_row[\"variable\"].replace(\"_\", \" \"),\n",
        "                # exp_coef=result_row[\"exp(coef)\"],\n",
        "                coef=result_row[\"coef\"],\n",
        "                # z=result_row[\"z\"],\n",
        "                p=result_row[\"P>|z|\"],\n",
        "                ci_low=result_row[\"[0.025\"],\n",
        "                ci_high=result_row[\"0.975]\"],\n",
        "                stars=stars,\n",
        "                color=color\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Combine all parts\n",
        "    regression_table_latex = f\"\"\"\\\\begin{{table}}\n",
        "\\\\centering\n",
        "\\\\caption{{{tbl_cap}}}\n",
        "\\\\label{{{tbl_label}}}\n",
        "\\\\label{{tbl-reg-results}}\n",
        "{model_results_table_header}\n",
        "{chr(10).join(rows_latex)}\n",
        "{model_results_table_footer}\n",
        "\\\\end{{table}}\"\"\"\n",
        "\n",
        "    return regression_table_latex, model_results_df"
      ],
      "id": "ec7bfaeb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create models for cited_by_count\n",
        "article_cited_by_count_models = compute_article_level_models(\n",
        "    \"cited_by_count\",\n",
        "    team_comp_no_push_after_pub_dummies,\n",
        "    glm_family=sm.families.NegativeBinomial(),\n",
        ")"
      ],
      "id": "82f575ec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Characteristics of Scientific Code Contributors\n",
        "\n",
        "### Author Positions of Code Contributing Authors\n"
      ],
      "id": "84c84d61"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def _run_posthocs(\n",
        "    data: pd.DataFrame,\n",
        "    split_var: str,\n",
        ") -> list[dict[str, str]]:\n",
        "    # Conduct pairwise posthocs\n",
        "    binom_results = []\n",
        "    for split_val in data[split_var].unique():\n",
        "        split_val_subset = data.loc[\n",
        "            data[split_var] == split_val\n",
        "        ]\n",
        "\n",
        "        # Run test\n",
        "        results = binomtest(\n",
        "            sum(split_val_subset[\"is_code_contributor\"]),\n",
        "            len(split_val_subset),\n",
        "            0.5,\n",
        "        )\n",
        "        \n",
        "        # Conduct binomial test\n",
        "        binom_results.append(\n",
        "            {\n",
        "                split_var: split_val,\n",
        "                \"p\": results.pvalue,\n",
        "                \"statistic\": results.statistic,\n",
        "                \"n\": len(split_val_subset),\n",
        "                \"n_code_contributors\": sum(split_val_subset[\"is_code_contributor\"]),\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # P adjust\n",
        "        _, p_values, _, _ = multipletests(\n",
        "            [result[\"p\"] for result in binom_results],\n",
        "            method=\"bonferroni\",\n",
        "        )\n",
        "\n",
        "        # Replace p values\n",
        "        for i, result in enumerate(binom_results):\n",
        "            result[\"p\"] = p_values[i]\n",
        "        \n",
        "    # Print results\n",
        "    formatted_binom_results = []\n",
        "    for result in binom_results:\n",
        "        formatted_binom_results.append({\n",
        "            split_var.replace(\"_\", \" \").title(): result[split_var].title(),\n",
        "            \"Coding\": result[\"n_code_contributors\"],\n",
        "            \"Total\": result[\"n\"],\n",
        "            \"p\": result[\"p\"],\n",
        "        })\n",
        "\n",
        "    return formatted_binom_results\n",
        "    \n",
        "\n",
        "def _run_chi2_and_posthocs(\n",
        "    data: pd.DataFrame,\n",
        "    control_var: str | None,\n",
        "    split_var: str,\n",
        ") -> list[dict[str, str | float | int]]:\n",
        "    chi2_result_str_template = \"2={stat:.1f}, p={p:.3f}, n={n}\"\n",
        "    \n",
        "    return_vals = []\n",
        "\n",
        "    if control_var is None:\n",
        "        xtabs = pd.crosstab(\n",
        "            data[split_var],\n",
        "            data[\"is_code_contributor\"],\n",
        "        )\n",
        "        chi2_result = chi2_contingency(xtabs)\n",
        "        if chi2_result.pvalue < 0.05:\n",
        "            posthoc_results = _run_posthocs(\n",
        "                data,\n",
        "                split_var=split_var,\n",
        "            )\n",
        "\n",
        "            for posthoc_result in posthoc_results:\n",
        "                return_vals.append({\n",
        "                    \"Control\": \"Overall\",\n",
        "                    \"Subset\": \"Overall\",\n",
        "                    **posthoc_result,\n",
        "                })\n",
        "\n",
        "        else:\n",
        "            pass\n",
        "    else:\n",
        "        for control_val in data[control_var].unique():\n",
        "            # Get data with control_val subset\n",
        "            control_val_subset = data.loc[\n",
        "                data[control_var] == control_val\n",
        "            ]\n",
        "            xtabs = pd.crosstab(\n",
        "                control_val_subset[split_var],\n",
        "                control_val_subset[\"is_code_contributor\"],\n",
        "            )\n",
        "            chi2_result = chi2_contingency(xtabs)\n",
        "            posthoc_results = _run_posthocs(\n",
        "                control_val_subset,\n",
        "                split_var=split_var,\n",
        "            )\n",
        "\n",
        "            for posthoc_result in posthoc_results:\n",
        "                return_vals.append({\n",
        "                    \"Control\": control_var.replace(\"_\", \" \").title(),\n",
        "                    \"Subset\": control_val.replace(\"_\", \" \").title(),\n",
        "                    **posthoc_result,\n",
        "                })\n",
        "\n",
        "    return return_vals"
      ],
      "id": "85a89a93",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "doc_contribs_for_code_char = document_contributors.loc[\n",
        "    document_contributors[\"document_id\"].isin(team_comp_no_push_after_pub[\"document_id\"])\n",
        "].merge(\n",
        "    team_comp_no_push_after_pub[[\n",
        "        \"document_id\",\n",
        "        \"repository_id\",\n",
        "        \"is_open_access\",\n",
        "        \"domain\",\n",
        "        \"article_type\",\n",
        "    ]]\n",
        ")\n",
        "\n",
        "# Iter over rows and check if the author is a dev on the same article-repo pair\n",
        "doc_contribs_for_code_char_rows = []\n",
        "for i, row in doc_contribs_for_code_char.iterrows():\n",
        "    # Get devs\n",
        "    this_repo_devs = repository_contributors.loc[\n",
        "        repository_contributors[\"repository_id\"] == row[\"repository_id\"]\n",
        "    ]\n",
        "\n",
        "    # Get the set of researcher_dev_links for the authors\n",
        "    author_dev_links = researcher_dev_links.loc[\n",
        "        researcher_dev_links[\"researcher_id\"] == row.researcher_id\n",
        "    ].sort_values(\"predictive_model_confidence\", ascending=False)\n",
        "\n",
        "    # Drop duplicates by developer_account_id (keeping first)\n",
        "    # as we may have accidently matched the same dev to the multiple authors\n",
        "    author_dev_links = author_dev_links.drop_duplicates(\n",
        "        subset=[\"developer_account_id\"],\n",
        "        keep=\"first\",\n",
        "    )\n",
        "\n",
        "    # Drop any author dev links that have less than 97% confidence\n",
        "    author_dev_links = author_dev_links.loc[\n",
        "        author_dev_links[\"predictive_model_confidence\"] >= 0.97\n",
        "    ]\n",
        "\n",
        "    # If no author dev links return same rows and \"is_code_contributor\" as False\n",
        "    if len(author_dev_links) == 0:\n",
        "        doc_contribs_for_code_char_rows.append(\n",
        "            {\n",
        "                **row.to_dict(),\n",
        "                \"is_code_contributor\": False,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    # If there are more than three author dev links, ignore\n",
        "    elif len(author_dev_links) > 3:\n",
        "        continue\n",
        "    \n",
        "    else:\n",
        "        # Add same rows and add \"is_code_contributor\" based on if developer_account_id is in this_repo_devs\n",
        "        doc_contribs_for_code_char_rows.append(\n",
        "            {\n",
        "                **row.to_dict(),\n",
        "                \"is_code_contributor\": any(\n",
        "                    author_dev_link[\"developer_account_id\"]\n",
        "                    in this_repo_devs[\"developer_account_id\"].unique()\n",
        "                    for _, author_dev_link in author_dev_links.iterrows()\n",
        "                ),\n",
        "            }\n",
        "        )\n",
        "\n",
        "# Create dataframe\n",
        "doc_contribs_for_code_char = pd.DataFrame(doc_contribs_for_code_char_rows)\n",
        "\n",
        "# Replace binary values in \"is_open_access\" to \"Open Access\" and \"Closed Access\"\n",
        "doc_contribs_for_code_char[\"is_open_access\"] = (\n",
        "    doc_contribs_for_code_char.apply(\n",
        "        lambda x: \"Open Access\" if x[\"is_open_access\"] == 1 else \"Closed Access\",\n",
        "        axis=1,\n",
        "    )\n",
        ")\n",
        "\n",
        "# Replace binary values in \"is_corresponding\" to \"Corresponding\" and \"Not Corresponding\"\n",
        "doc_contribs_for_code_char[\"is_corresponding\"] = (\n",
        "    doc_contribs_for_code_char.apply(\n",
        "        lambda x: \"Corresponding\" if x[\"is_corresponding\"] == 1 else \"Not Corresponding\",\n",
        "        axis=1,\n",
        "    )\n",
        ")\n",
        "\n",
        "# Calculate the percent of first authors who are code contributors\n",
        "first_author_code_contributors = doc_contribs_for_code_char.loc[\n",
        "    doc_contribs_for_code_char[\"position\"] == \"first\"\n",
        "][\"is_code_contributor\"].sum()\n",
        "percent_first_authors_code_contributors = (\n",
        "    first_author_code_contributors\n",
        "    / len(\n",
        "        doc_contribs_for_code_char.loc[\n",
        "            doc_contribs_for_code_char[\"position\"] == \"first\"\n",
        "        ]\n",
        "    )\n",
        ")\n",
        "percent_first_authors_code_contributors_str = f\"{percent_first_authors_code_contributors:.1%}\"\n",
        "\n",
        "middle_author_code_contributors = doc_contribs_for_code_char.loc[\n",
        "    doc_contribs_for_code_char[\"position\"] == \"middle\"\n",
        "][\"is_code_contributor\"].sum()\n",
        "percent_middle_authors_code_contributors = (\n",
        "    middle_author_code_contributors\n",
        "    / len(\n",
        "        doc_contribs_for_code_char.loc[\n",
        "            doc_contribs_for_code_char[\"position\"] == \"middle\"\n",
        "        ]\n",
        "    )\n",
        ")\n",
        "percent_middle_authors_code_contributors_str = f\"{percent_middle_authors_code_contributors:.1%}\"\n",
        "\n",
        "last_author_code_contributors = doc_contribs_for_code_char.loc[\n",
        "    doc_contribs_for_code_char[\"position\"] == \"last\"\n",
        "][\"is_code_contributor\"].sum()\n",
        "percent_last_authors_code_contributors = (\n",
        "    last_author_code_contributors\n",
        "    / len(\n",
        "        doc_contribs_for_code_char.loc[\n",
        "            doc_contribs_for_code_char[\"position\"] == \"last\"\n",
        "        ]\n",
        "    )\n",
        ")\n",
        "percent_last_authors_code_contributors_str = f\"{percent_last_authors_code_contributors:.1%}\"\n",
        "\n",
        "domain_code_char = pd.DataFrame(\n",
        "    _run_chi2_and_posthocs(\n",
        "        doc_contribs_for_code_char,\n",
        "        control_var=\"domain\",\n",
        "        split_var=\"position\",\n",
        "    )\n",
        ")\n",
        "position_sorter = [\"First\", \"Middle\", \"Last\"]\n",
        "domain_code_char[\"Position\"] = pd.Categorical(\n",
        "    domain_code_char[\"Position\"],\n",
        "    categories=position_sorter,\n",
        "    ordered=True,\n",
        ")\n",
        "domain_code_char = domain_code_char.sort_values([\"Subset\", \"Position\"])\n",
        "\n",
        "article_type_code_char = pd.DataFrame(\n",
        "    _run_chi2_and_posthocs(\n",
        "        doc_contribs_for_code_char,\n",
        "        control_var=\"article_type\",\n",
        "        split_var=\"position\",\n",
        "    )\n",
        ")\n",
        "article_type_code_char[\"Position\"] = pd.Categorical(\n",
        "    article_type_code_char[\"Position\"],\n",
        "    categories=position_sorter,\n",
        "    ordered=True,\n",
        ")\n",
        "article_type_code_char = article_type_code_char.sort_values([\"Subset\", \"Position\"])\n",
        "\n",
        "open_access_status_code_char = pd.DataFrame(\n",
        "    _run_chi2_and_posthocs(\n",
        "        doc_contribs_for_code_char,\n",
        "        control_var=\"is_open_access\",\n",
        "        split_var=\"position\",\n",
        "    )\n",
        ")\n",
        "open_access_status_code_char[\"Position\"] = pd.Categorical(\n",
        "    open_access_status_code_char[\"Position\"],\n",
        "    categories=position_sorter,\n",
        "    ordered=True,\n",
        ")\n",
        "open_access_status_code_char = open_access_status_code_char.sort_values([\"Subset\", \"Position\"])\n",
        "\n",
        "# Run chi2 and posthocs for overall author position\n",
        "overall_code_char = pd.DataFrame(\n",
        "    _run_chi2_and_posthocs(\n",
        "        doc_contribs_for_code_char,\n",
        "        control_var=None,\n",
        "        split_var=\"position\",\n",
        "    )\n",
        ")\n",
        "overall_code_char[\"Position\"] = pd.Categorical(\n",
        "    overall_code_char[\"Position\"],\n",
        "    categories=position_sorter,\n",
        "    ordered=True,\n",
        ")\n",
        "overall_code_char = overall_code_char.sort_values([\"Subset\", \"Position\"])"
      ],
      "id": "42d9feed",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Building upon previous work examining the relationship between authorship position and research contributions, we investigate how author position may relate to code contribution patterns. We hypothesized that first authors, traditionally contributing the bulk of intellectual and experimental work, are most likely to contribute code to a project. In contrast, middle and last authors often provide oversight and guidance and would be less likely to contribute code.\n",
        "\n",
        "To analyze these patterns within our previously filtered dataset of article-repository pairs (@tbl-team-comp-no-push-after-pub-counts), we conducted Chi-square tests of independence between author position and code contribution status. These tests revealed significant associations between author position and likelihood of code contribution overall and when controlling for research domain, article type, and open access status (all p \\< 0.01), indicating that the proportion of authors contributing code differs significantly based on author position. Following these significant associations, we examined the specific proportions across positions (@tbl-post-hoc-tests-on-author-positions): `{python} percent_first_authors_code_contributors_str` of first authors contributed code to their projects, compared to only `{python} percent_middle_authors_code_contributors_str` of middle authors and `{python} percent_last_authors_code_contributors_str` of last authors. The differences in these proportions remained statistically significant across all tested scenarios, regardless of research domain, article type, or open access status.\n",
        "\n",
        "Based on these findings, we ***accept*** our hypothesis (*H2a*) that \"first authors have higher code contribution rates than authors in other positions.\" The data demonstrates that the proportion of first authors who contribute code (`{python} percent_first_authors_code_contributors_str`) is significantly higher than the proportion of both middle authors (`{python} percent_middle_authors_code_contributors_str`) and last authors (`{python} percent_last_authors_code_contributors_str`). This relationship remains robust and statistically significant across all tested conditions, including variations in research domain, article type, and open access status, indicating a fundamental connection between authorship position and technical contribution in scientific research.\n",
        "\n",
        "### Corresponding Status of Code Contributing Authors\n"
      ],
      "id": "e2540c04"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Run chi2 and posthocs for overall corresponding status\n",
        "domain_code_corresponding = pd.DataFrame(\n",
        "    _run_chi2_and_posthocs(\n",
        "        doc_contribs_for_code_char,\n",
        "        control_var=\"domain\",\n",
        "        split_var=\"is_corresponding\",\n",
        "    )\n",
        ").sort_values([\"Subset\", \"Is Corresponding\"])\n",
        "\n",
        "article_type_code_corresponding = pd.DataFrame(\n",
        "    _run_chi2_and_posthocs(\n",
        "        doc_contribs_for_code_char,\n",
        "        control_var=\"article_type\",\n",
        "        split_var=\"is_corresponding\",\n",
        "    )\n",
        ").sort_values([\"Subset\", \"Is Corresponding\"])\n",
        "\n",
        "open_access_status_code_corresponding = pd.DataFrame(\n",
        "    _run_chi2_and_posthocs(\n",
        "        doc_contribs_for_code_char,\n",
        "        control_var=\"is_open_access\",\n",
        "        split_var=\"is_corresponding\",\n",
        "    )\n",
        ").sort_values([\"Subset\", \"Is Corresponding\"])\n",
        "\n",
        "overall_code_corresponding = pd.DataFrame(\n",
        "    _run_chi2_and_posthocs(\n",
        "        doc_contribs_for_code_char,\n",
        "        control_var=None,\n",
        "        split_var=\"is_corresponding\",\n",
        "    )\n",
        ").sort_values([\"Subset\", \"Is Corresponding\"])\n",
        "\n",
        "# Calculate the percent of corresponding authors who are code contributors\n",
        "corres_author_code_contributors = doc_contribs_for_code_char.loc[\n",
        "    doc_contribs_for_code_char[\"is_corresponding\"] == \"Corresponding\"\n",
        "][\"is_code_contributor\"].sum()\n",
        "percent_corresponding_authors_code_contributors = (\n",
        "    corres_author_code_contributors\n",
        "    / len(\n",
        "        doc_contribs_for_code_char.loc[\n",
        "            doc_contribs_for_code_char[\"is_corresponding\"] == \"Corresponding\"\n",
        "        ]\n",
        "    )\n",
        ")\n",
        "percent_corresponding_authors_code_contributors_str = f\"{percent_corresponding_authors_code_contributors:.1%}\"\n",
        "\n",
        "non_corres_author_code_contributors = doc_contribs_for_code_char.loc[\n",
        "    doc_contribs_for_code_char[\"is_corresponding\"] == \"Not Corresponding\"\n",
        "][\"is_code_contributor\"].sum()\n",
        "percent_non_corresponding_authors_code_contributors = (\n",
        "    non_corres_author_code_contributors\n",
        "    / len(\n",
        "        doc_contribs_for_code_char.loc[\n",
        "            doc_contribs_for_code_char[\"is_corresponding\"] == \"Not Corresponding\"\n",
        "        ]\n",
        "    )\n",
        ")\n",
        "percent_non_corresponding_authors_code_contributors_str = f\"{percent_non_corresponding_authors_code_contributors:.1%}\""
      ],
      "id": "df025fcb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Building upon our analysis of author position, we next examine how corresponding author status relates to code contribution patterns. We hypothesized that corresponding authors, who traditionally maintain research artifacts and serve as primary points of contact, would be more likely to contribute code compared to non-corresponding authors, as this role often involves responsibility for project resources and materials.\n",
        "\n",
        "To analyze these relationships within our filtered dataset of article-repository pairs, we conducted Chi-square tests of independence between corresponding author status and code contribution status. Our analysis revealed patterns contrary to our initial hypothesis. The proportion of code contributors was low among both groups, with only `{python} percent_corresponding_authors_code_contributors_str` of corresponding authors and `{python} percent_non_corresponding_authors_code_contributors_str` of non-corresponding authors contributing code to their projects. Further examination (@tbl-post-hoc-tests-on-corresponding-status) showed that this pattern holds across nearly all conditions, with only a single exception: corresponding authors in closed-access publications showed no significant difference in their proportion of code contributors. However, this was tested with a sample of less than 200 authors.\n",
        "\n",
        "Based on these findings, we ***reject*** our hypothesis (*H2b*) that \"corresponding authors have higher code contribution rates than non-corresponding authors.\" Contrary to our expectations, our analysis revealed that the proportion of code contributors among corresponding authors (`{python} percent_corresponding_authors_code_contributors_str`) did not significantly differ from the proportion among non-corresponding authors (`{python} percent_non_corresponding_authors_code_contributors_str`). This pattern of similar proportions remained consistent across most studied conditions, with a single, small sample size exception in closed-access publications.\n",
        "\n",
        "### Modeling Author H-Index\n"
      ],
      "id": "aa2ee59b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# First, get the set of researchers who have at least 3 documents\n",
        "researchers_w_3_docs = researchers.loc[\n",
        "    researchers[\"id\"].isin(\n",
        "        document_contributors[\"researcher_id\"]\n",
        "        .value_counts()\n",
        "        .loc[lambda x: x >= 3]\n",
        "        .index\n",
        "    )\n",
        "]\n",
        "\n",
        "# Use sample?\n",
        "if USE_SAMPLE:\n",
        "    researchers_w_3_docs = researchers_w_3_docs.sample(frac=0.02, random_state=12)\n",
        "\n",
        "# Next, for each researcher, get the set of documents they have contributed to\n",
        "researchers_w_3_docs = document_contributors.loc[\n",
        "    document_contributors[\"researcher_id\"].isin(researchers_w_3_docs[\"id\"])\n",
        "].merge(\n",
        "    researchers_w_3_docs,\n",
        "    left_on=\"researcher_id\",\n",
        "    right_on=\"id\",\n",
        ")\n",
        "\n",
        "# Attach document for publication date\n",
        "researchers_w_3_docs = researchers_w_3_docs.merge(\n",
        "    documents[[\"id\", \"publication_date\"]],\n",
        "    left_on=\"document_id\",\n",
        "    right_on=\"id\",\n",
        ").drop(\n",
        "    columns=[\"id\"],\n",
        ")\n",
        "\n",
        "# Keep only certain columns\n",
        "researchers_w_3_docs = researchers_w_3_docs[\n",
        "    [\n",
        "        \"researcher_id\",\n",
        "        \"document_id\",\n",
        "        \"publication_date\",\n",
        "        \"position\",\n",
        "        \"is_corresponding\",\n",
        "        \"works_count\",\n",
        "        \"cited_by_count\",\n",
        "        \"h_index\",\n",
        "        \"i10_index\",\n",
        "        \"two_year_mean_citedness\",\n",
        "    ]\n",
        "]\n",
        "\n",
        "# Next, for each researcher_doc, attach the document details (domain, reduced_doc_type)\n",
        "researchers_w_3_docs = (\n",
        "    researchers_w_3_docs.merge(\n",
        "        document_topics[[\"document_id\", \"topic_id\"]],\n",
        "        left_on=\"document_id\",\n",
        "        right_on=\"document_id\",\n",
        "    )\n",
        "    .merge(\n",
        "        topics[[\"id\", \"domain_name\"]],\n",
        "        left_on=\"topic_id\",\n",
        "        right_on=\"id\",\n",
        "    )\n",
        "    .drop(\n",
        "        columns=[\"id\", \"topic_id\"],\n",
        "    )\n",
        "    .merge(\n",
        "        reduced_doc_types,\n",
        "        left_on=\"document_id\",\n",
        "        right_on=\"document_id\",\n",
        "    )\n",
        ")\n",
        "\n",
        "# Now for each of these, we want to see if they have coded on the document\n",
        "researchers_coded = []\n",
        "individuals_removed_from_confidence_thresh = 0\n",
        "for _, row in researchers_w_3_docs.iterrows():\n",
        "    # Check for dev account\n",
        "    dev_links = researcher_dev_links.loc[\n",
        "        researcher_dev_links[\"researcher_id\"] == row[\"researcher_id\"]\n",
        "    ]\n",
        "\n",
        "    # Remove any links with less than 97% confidence\n",
        "    before_drop_count = len(dev_links)\n",
        "    dev_links = dev_links.loc[\n",
        "        dev_links[\"predictive_model_confidence\"] >= 0.97\n",
        "    ]\n",
        "    after_drop_count = len(dev_links)\n",
        "    individuals_removed_from_confidence_thresh += (\n",
        "        before_drop_count - after_drop_count\n",
        "    )\n",
        "\n",
        "    # Fast exit\n",
        "    if len(dev_links) == 0:\n",
        "        researchers_coded.append(\n",
        "            {\n",
        "                \"researcher_id\": row[\"researcher_id\"],\n",
        "                \"document_id\": row[\"document_id\"],\n",
        "                \"coded_on_article\": 0,\n",
        "            }\n",
        "        )\n",
        "        continue\n",
        "\n",
        "    # Skip this person if they have more than 3 links\n",
        "    # Likely something went wrong\n",
        "    if len(dev_links) > 3:\n",
        "        continue\n",
        "\n",
        "    # Get repos associated with document\n",
        "    repo_links_for_doc = doc_repo_links.loc[\n",
        "        doc_repo_links[\"document_id\"] == row[\"document_id\"]\n",
        "    ]\n",
        "\n",
        "    # Skip if there is more than 1 repo associated with the document\n",
        "    # We just don't know how to handle these cases right now\n",
        "    if len(repo_links_for_doc) > 1:\n",
        "        continue\n",
        "\n",
        "    # Also skip if 0\n",
        "    if len(repo_links_for_doc) == 0:\n",
        "        continue\n",
        "\n",
        "    # Get the repo_id for the single repo\n",
        "    repo_id = repo_links_for_doc[\"repository_id\"].iloc[0]\n",
        "\n",
        "    # Get the repo_contributors for this repository\n",
        "    repo_contributors = repository_contributors.loc[\n",
        "        repository_contributors[\"repository_id\"] == repo_id\n",
        "    ]\n",
        "\n",
        "    # Check if any of the dev accounts are in the repo contribs\n",
        "    researcher_coded = (\n",
        "        len(\n",
        "            set(repo_contributors[\"developer_account_id\"].unique()).intersection(\n",
        "                set(dev_links[\"developer_account_id\"].unique()),\n",
        "            )\n",
        "        )\n",
        "        > 0\n",
        "    )\n",
        "\n",
        "    # Finally assert any of the repo_contributors are the dev account\n",
        "    # associated with the researcher\n",
        "    researchers_coded.append(\n",
        "        {\n",
        "            \"researcher_id\": row[\"researcher_id\"],\n",
        "            \"document_id\": row[\"document_id\"],\n",
        "            \"coded_on_article\": int(researcher_coded),\n",
        "        }\n",
        "    )\n",
        "\n",
        "# Create dataframe\n",
        "researchers_coded_df = pd.DataFrame(researchers_coded)\n",
        "\n",
        "# Merge with researchers_w_3_docs\n",
        "researchers_w_3_docs_and_coded = researchers_coded_df.merge(\n",
        "    researchers_w_3_docs,\n",
        "    left_on=[\"researcher_id\", \"document_id\"],\n",
        "    right_on=[\"researcher_id\", \"document_id\"],\n",
        ")\n",
        "\n",
        "def _mode_or_recent_reduce(group: pd.DataFrame, col: str) -> str:\n",
        "    # Get the mode\n",
        "    mode = group[col].mode().tolist()\n",
        "    if len(mode) == 1:\n",
        "        return mode[0]\n",
        "\n",
        "    # Otherwise, iter over most recent publications until value in mode is found\n",
        "    group_ordered_by_date = group.sort_values(\"publication_date\", ascending=False)\n",
        "    for _, row in group_ordered_by_date.iterrows():\n",
        "        if row[col] in mode:\n",
        "            return row[col]\n",
        "\n",
        "\n",
        "def _agg_apply(group: pd.DataFrame) -> dict:\n",
        "    return {\n",
        "        \"n_documents\": group[\"document_id\"].nunique(),\n",
        "        \"n_coded\": group[\"coded_on_article\"].sum(),\n",
        "        \"works_count\": group[\"works_count\"].iloc[0],\n",
        "        \"cited_by_count\": group[\"cited_by_count\"].iloc[0],\n",
        "        \"h_index\": group[\"h_index\"].iloc[0],\n",
        "        \"i10_index\": group[\"i10_index\"].iloc[0],\n",
        "        \"two_year_mean_citedness\": group[\"two_year_mean_citedness\"].iloc[0],\n",
        "        \"position\": _mode_or_recent_reduce(group, \"position\"),\n",
        "        \"domain_name\": _mode_or_recent_reduce(group, \"domain_name\"),\n",
        "        \"reduced_doc_type\": _mode_or_recent_reduce(group, \"reduced_doc_type\"),\n",
        "    }\n",
        "\n",
        "\n",
        "researchers_w_3_docs_and_coded_agg = (\n",
        "    researchers_w_3_docs_and_coded.groupby(\"researcher_id\")[\n",
        "        [col for col in researchers_w_3_docs_and_coded if col != \"researcher_id\"]\n",
        "    ]\n",
        "    .apply(_agg_apply)\n",
        "    .reset_index(name=\"dicts\")\n",
        ")\n",
        "researchers_w_3_docs_and_coded_agg = researchers_w_3_docs_and_coded_agg.join(\n",
        "    pd.json_normalize(researchers_w_3_docs_and_coded_agg[\"dicts\"])\n",
        ").drop(columns=[\"dicts\"])\n",
        "\n",
        "# Create three features for coding status\n",
        "# \"any\" coding status\n",
        "# \"majority\" coding status\n",
        "# \"always\" coding status\n",
        "# determine type by taking the percentage of documents coded on\n",
        "# and determining if it is greater than 0, greater than 0.5, or 1\n",
        "researchers_w_3_docs_and_coded_agg[\"coding_pct\"] = (\n",
        "    researchers_w_3_docs_and_coded_agg[\"n_coded\"]\n",
        "    / researchers_w_3_docs_and_coded_agg[\"n_documents\"]\n",
        ")\n",
        "\n",
        "researchers_w_3_docs_and_coded_agg[\"any_coding\"] = (\n",
        "    (researchers_w_3_docs_and_coded_agg[\"coding_pct\"] > 0)\n",
        "    & (researchers_w_3_docs_and_coded_agg[\"coding_pct\"] <= 0.5)\n",
        ").astype(int)\n",
        "researchers_w_3_docs_and_coded_agg[\"majority_coding\"] = (\n",
        "    (researchers_w_3_docs_and_coded_agg[\"coding_pct\"] > 0.5)\n",
        "    & (researchers_w_3_docs_and_coded_agg[\"coding_pct\"] < 1)\n",
        ").astype(int)\n",
        "researchers_w_3_docs_and_coded_agg[\"always_coding\"] = (\n",
        "    researchers_w_3_docs_and_coded_agg[\"coding_pct\"] == 1\n",
        ").astype(int)\n",
        "\n",
        "# Drop n_documents, n_coded and\n",
        "# rename \"position\" to \"common_author_position\",\n",
        "# \"domain_name\" to \"common_domain\",\n",
        "# and \"reduced_doc_type\" to \"common_article_type\"\n",
        "researchers_w_3_docs_and_coded_agg = researchers_w_3_docs_and_coded_agg.drop(\n",
        "    columns=[\"n_documents\", \"n_coded\"]\n",
        ")\n",
        "researchers_w_3_docs_and_coded_agg = researchers_w_3_docs_and_coded_agg.rename(\n",
        "    columns={\n",
        "        \"position\": \"common_author_position\",\n",
        "        \"domain_name\": \"common_domain\",\n",
        "        \"reduced_doc_type\": \"common_article_type\",\n",
        "    }\n",
        ")\n",
        "\n",
        "# Get dummies for categorical variables\n",
        "researchers_w_3_docs_and_coded_agg_dummies = pd.get_dummies(\n",
        "    researchers_w_3_docs_and_coded_agg,\n",
        "    columns=[\"common_author_position\", \"common_domain\", \"common_article_type\"],\n",
        "    drop_first=True,\n",
        ")\n",
        "\n",
        "# Cast all to float\n",
        "researchers_w_3_docs_and_coded_agg_dummies = (\n",
        "    researchers_w_3_docs_and_coded_agg_dummies.astype(float)\n",
        ")"
      ],
      "id": "b1e448f0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Building upon previous work examining career implications for researchers who prioritize software development (see @sec-background), we investigated how varying levels of code contribution relate to scholarly impact through h-index metrics. To ensure a robust analysis, we applied several key data filtering steps. We only included researchers with at least three publications in our dataset, removed those with more than three developer account associations, and used each researcher's most common domain, article type, and author position, with ties broken by the most recent occurrence. We removed h-index outliers by excluding researchers below the bottom 3rd and above the top 97th percentiles. Finally, we removed any author-developer-account pairs with a predictive model confidence of less than 0.97. @tbl-h-index-counts summarizes the number of researchers in each coding frequency group, categorized by author position, publication type, and research domain.\n",
        "\n",
        "We categorized researchers' coding contributions into mutually exclusive groups: non-coders (no code contributions), any coding (code contribution in less than half of article-repository pairs), majority coding (code contribution in at least half, but not all, article-repository pairs), and always coding (code contribution in every article-repository pair).\n",
        "\n",
        "@fig-author-h-index-by-coding-status shows the distribution of author h-indices across these coding frequency groups, grouped by author position, publication type, and research domain.\n"
      ],
      "id": "7b1b3f0d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-author-h-index-by-coding-status\n",
        "#| fig-cap: 'Distribution of author h-index by coding frequency across three key publication factors. Results are grouped by each author''s most frequent: (1) position in publication bylines (first, middle, or last), (2) publication type (preprint, research article, or software article), and (3) research domain (Social Sciences, Physical Sciences, Health Sciences, or Life Sciences). Within each subplot, h-indices are divided by the author''s coding frequency: ''none'' (no coding in any of their publications), ''any'' (coding in at least one but fewer than half of their publications), ''majority'' (coding in at least half but not all of their publications), and ''always'' (coding in each of their publications). Authors are only included if they have three or more publications within our dataset and are associated with no more than three developer accounts, with each association having a predicted model confidence of at least 97%.'\n",
        "\n",
        "def _get_coding_status_string(row: pd.Series) -> str:\n",
        "    if row[\"any_coding\"] == 1:\n",
        "        return \"any\"\n",
        "    elif row[\"majority_coding\"] == 1:\n",
        "        return \"majority\"\n",
        "    elif row[\"always_coding\"] == 1:\n",
        "        return \"always\"\n",
        "    else:\n",
        "        return \"none\"\n",
        "\n",
        "researchers_w_3_docs_and_coded_agg[\"coding_status\"] = researchers_w_3_docs_and_coded_agg.apply(\n",
        "    _get_coding_status_string,\n",
        "    axis=1,\n",
        ")\n",
        "\n",
        "# Select columns of interest\n",
        "researchers_w_3_docs_and_coded_agg_subset_for_vis = researchers_w_3_docs_and_coded_agg[\n",
        "    [\n",
        "        \"researcher_id\",\n",
        "        \"h_index\",\n",
        "        \"coding_status\",\n",
        "        \"common_author_position\",\n",
        "        \"common_domain\",\n",
        "        \"common_article_type\",\n",
        "    ]\n",
        "]\n",
        "\n",
        "# Melt control variables\n",
        "researchers_w_3_docs_and_coded_agg_vis_melted = researchers_w_3_docs_and_coded_agg_subset_for_vis.melt(\n",
        "    id_vars=[\"researcher_id\", \"h_index\", \"coding_status\"],\n",
        "    value_vars=[\n",
        "        \"common_author_position\",\n",
        "        \"common_domain\",\n",
        "        \"common_article_type\",\n",
        "    ],\n",
        "    var_name=\"control_var\",\n",
        "    value_name=\"control_value\",\n",
        ")\n",
        "\n",
        "# Define visualization order and control variables\n",
        "vis_order = [\"none\", \"any\", \"majority\", \"always\"]\n",
        "control_vars = OrderedDict([\n",
        "    (\"common_author_position\", [\"first\", \"middle\", \"last\"]),\n",
        "    (\"common_article_type\", [\"preprint\", \"research article\", \"software article\"]),\n",
        "    (\"common_domain\", [\"Social Sciences\", \"Physical Sciences\", \"Health Sciences\", \"Life Sciences\"]),\n",
        "])\n",
        "\n",
        "# Set up the figure with appropriate size\n",
        "fig = plt.figure(figsize=(8, 9))\n",
        "\n",
        "# Calculate grid dimensions based on control variables\n",
        "num_controls = len(control_vars)\n",
        "max_subsets = max(len(values) for values in control_vars.values())\n",
        "grid_height = num_controls * 3  # 3 rows per control variable\n",
        "\n",
        "# Iterate through control variables to create subplots\n",
        "for control_idx, (control_var, control_values) in enumerate(control_vars.items()):\n",
        "    # Determine row position for this control variable\n",
        "    row_pos = control_idx * 3\n",
        "    \n",
        "    # Create subplots for each control value\n",
        "    for col_idx, control_value in enumerate(control_values):\n",
        "        # Create the subplot\n",
        "        ax = plt.subplot2grid(\n",
        "            (grid_height, max_subsets),\n",
        "            (row_pos, col_idx),\n",
        "            rowspan=2,\n",
        "            colspan=1\n",
        "        )\n",
        "        \n",
        "        # Filter data for this control variable and value\n",
        "        subset_data = researchers_w_3_docs_and_coded_agg_vis_melted[\n",
        "            (researchers_w_3_docs_and_coded_agg_vis_melted[\"control_var\"] == control_var) & \n",
        "            (researchers_w_3_docs_and_coded_agg_vis_melted[\"control_value\"] == control_value)\n",
        "        ]\n",
        "        \n",
        "        # Create boxplot with specified configuration\n",
        "        sns.boxplot(\n",
        "            data=subset_data,\n",
        "            x=\"coding_status\",\n",
        "            y=\"h_index\",\n",
        "            hue=\"coding_status\",\n",
        "            hue_order=vis_order,\n",
        "            order=vis_order,\n",
        "            ax=ax,\n",
        "            showfliers=False,\n",
        "            legend=False,\n",
        "        )\n",
        "\n",
        "        # Set ylim to row specific values\n",
        "        ax.set_ylim([-1, 115])\n",
        "        \n",
        "        # Set titles and labels\n",
        "        ax.set_title(f\"{control_value.title()}\", fontsize=14)\n",
        "        \n",
        "        if col_idx == 0:\n",
        "            ax.set_ylabel(\"h-index\", fontsize=14)\n",
        "        else:\n",
        "            ax.set_ylabel(\"\")\n",
        "            \n",
        "        # Never show xlabel as we use legend\n",
        "        # if col_idx == 0:\n",
        "        #     ax.set_xlabel(\"Coding Frequency\", loc=\"left\", fontproperties={\"size\": 16, \"weight\": \"bold\"})\n",
        "        # else:\n",
        "        ax.set_xlabel(\"\")\n",
        "            \n",
        "        # Rotate x-tick labels for better readability\n",
        "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
        "\n",
        "    # Add row header for this control variable\n",
        "    fig.text(\n",
        "        0.13, \n",
        "        0.95 - (control_idx * 0.299), \n",
        "        f\"{control_var.replace('_', ' ').title().replace(\"Common\", \"\").strip()}\", \n",
        "        fontsize=16, \n",
        "        ha=\"left\", \n",
        "        va=\"center\", \n",
        "        fontweight=\"bold\"\n",
        "    )\n",
        "\n",
        "\n",
        "# # Add a legend outside the plots\n",
        "# handles = [plt.Rectangle((0,0), 1, 1, color=tuple(cmaps.bold[2:]._colors.tolist()[i])) for i in range(len(vis_order))]\n",
        "# legend = fig.legend(\n",
        "#     handles, \n",
        "#     vis_order, \n",
        "#     bbox_to_anchor=(0.92, 0.52),  # Position at bottom center\n",
        "#     fontsize=16,\n",
        "#     title=\"Author Coding Frequency\",\n",
        "#     title_fontproperties={\"size\": 16, \"weight\": \"bold\"},\n",
        "#     alignment=\"left\",\n",
        "# )\n",
        "\n",
        "# Adjust layout\n",
        "plt.subplots_adjust(hspace=0.2, wspace=0.2, bottom=0.1)\n",
        "plt.tight_layout(rect=[0.03, 0.06, 0.97, 0.95])\n",
        "\n",
        "# Draw a horizontal lines at those coordinates\n",
        "for y in [0.69, 0.39]:\n",
        "    line = plt.Line2D([0.066, 0.944], [y,y], transform=fig.transFigure, color=\"black\", linestyle=\"--\")\n",
        "    fig.add_artist(line)\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "id": "fig-author-h-index-by-coding-status",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our analysis revealed a consistent and statistically significant negative relationship between code contribution frequency and h-index across multiple analytical controls. Our initial uncontrolled analysis (@tbl-researcher-coding-status-no-control) indicates increasingly adverse h-index effects as researcher coding frequency increases. Compared to non-coding authors, researchers were associated with progressively lower h-indices: occasional code contributors showed a \\~27.3% lower h-index (p \\< 0.001), majority code contributors demonstrated a \\~53.5% lower h-index (p \\< 0.001), and always coding authors exhibited a \\~62.1% lower h-index (p \\< 0.001).\n",
        "\n",
        "When controlling for author position (@tbl-researcher-coding-status-author-position), we found a general pattern of reduced h-indices with increased code contribution, with one notable exception. Occasional coding first authors were associated with a \\~14.9% higher h-index (p \\< 0.001), while always coding first authors saw a \\~21.6% reduction compared to non-coding first authors (p \\< 0.001). For middle and last authors, the pattern was more consistently negative. Middle authors who occasionally coded showed a \\~26.6% lower h-index (p \\< 0.001), and those always coding demonstrated a \\~52.9% lower h-index (p \\< 0.001). Similarly, last authors who occasionally coded experienced a \\~13.1% lower h-index (p \\< 0.001), with always coding authors showing a \\~45.7% lower h-index (p \\< 0.001).\n",
        "\n",
        "When controlling for research domain (@tbl-researcher-coding-status-domain), majority coding scientists showed significant h-index reductions across all domains. Health sciences researchers saw the most dramatic reduction at \\~76.5% (p \\< 0.001), followed by physical sciences at \\~52.6% (p \\< 0.001), social sciences at \\~51.4% (p \\< 0.001), and life sciences at \\~47.1% (p \\< 0.001).\n",
        "\n",
        "Analyzing by common article type (@tbl-researcher-coding-status-article-type) revealed similar patterns. For authors primarily publishing preprints, the h-index reductions were substantial: \\~25.6% for occasional coding, \\~53.5% for majority coding, and \\~62.9% for always coding authors. Authors primarily publishing software articles showed slightly better but still significant reductions: \\~33.1% for majority coding and \\~33.0% for always coding authors.\n",
        "\n",
        "Based on these findings, we ***accept*** our hypothesis (*H3*) that \"the frequency with which individual researchers contribute code to their research projects is negatively correlated with their h-index.\" Our analysis demonstrates a clear and statistically significant negative relationship between coding frequency and scholarly impact as measured by the researcher's h-index. This relationship was robust across multiple analytical controls, including author position, research domain, and article type. These results are particularly striking because each of our models includes publication count as an input feature, suggesting that these h-index reductions persist even when accounting for total research output.\n",
        "\n",
        "# Discussion\n",
        "\n",
        "Our analysis reveals significant disparities in the recognition of software contributions to scientific research, with nearly 30% of articles having non-author code-contributors who received no formal authorship credit. This persistent pattern suggests a systemic disconnect between software development and scientific recognition systems, reflecting challenges in how scientific contributions are valued and credited. This exclusion reflects what @shapin1989invisible observed about scientific authoritythe selective attribution of technical work as either genuine knowledge or mere skill significantly impacts who receives formal recognition. These findings further support previous research by @olivier_philippe_2019_2585783 and @Carver2022ASO documenting the frequent relegation of software contributors to either acknowledgment sections or receiving no credit at all, rather than authorship positions, despite the increasingly central role of software in scientific inquiry. The stability of this pattern over time indicates that this phenomenon has embedded itself in scientific software development rather than representing a transitional phase, raising questions about scientific labor and how reward structures integrate technical contributions.\n",
        "\n",
        "Our finding that, on average, article-repository pairs have only a single code contributor mirrors prior work from @farber2020analyzingGithubPapers. Further, the distribution of code contributions across author positions provides context to the hierarchical organization of scientific work. First authors emerge as significantly more likely to contribute code with `{python} percent_first_authors_code_contributors_str` of all first authors in our dataset contributing code. Middle and last authors, meanwhile, were statistically significantly less likely to contribute code, with only `{python} percent_middle_authors_code_contributors_str` of middle authors and `{python} percent_last_authors_code_contributors_str` of last authors acting as code-contributing members of the research team. Corresponding authors were similarly less likely than expected to be code contributors, as we found that within our dataset, corresponding authors were code contributors `{python} percent_corresponding_authors_code_contributors_str` of the time. These patterns align with traditional scientific labor distribution where first authors typically handle technical aspects of research while middle and last authors are likely specialist contributors or provide guidance and oversight [@Larivire2020InvestigatingTD; @sauermann2017authorship]. However, our data did not support our initial hypothesis that corresponding authors would also be more likely to contribute code due to their shared responsibility for the long-term maintenance of research artifacts. This finding suggests a potential strict division between project management responsibilities and direct technical engagement with software development.\n",
        "\n",
        "The modest citation advantage associated with code-contributing authors (5.1% increase in citations per code-contributing author) stands in contrast with the significant negative relationship between coding frequency and an individual's scholarly impact (h-index). This misalignment between technical contributions and scientific recognition creates an asymmetrical relationship in which software development may enhance research impact but potentially penalizes individual careers. The progressive reduction in h-index as coding frequency increases indicates a cumulative disadvantage for frequent code contributors. This pattern persists even when controlling for publication count, suggesting issues in how software contributions are valued relative to other scientific outputs. These findings echo concerns raised by @muna2016astropyproblem about the sustainability of research software development and highlight how current reward structures may discourage talented developers from pursuing scientific careers.\n",
        "\n",
        "Software development represents a form of scholarly labor that has become increasingly essential to modern research yet remains incompletely integrated into formal recognition systems. Similar to the high proportion of articles with authors who made data curation contributions towards research observed by @Larivire2020InvestigatingTD, our finding that a quarter of papers have unacknowledged code contributors highlights a labor role that is simultaneously common and undervalued. The prevalence of code contributions across domains demonstrates the importance of this work to contemporary research. However, the persistent exclusion of contributors from authorship suggests that researchers continue to classify software development as technical support rather than intellectual contribution. This classification may reflect disciplinary traditions that privilege certain forms of scholarly production despite the growing recognition that software represents a legitimate research output [@Katz2020RecognizingTV]. The tension between software's importance and contributors' recognition status raises questions about how we define, value, and reward different forms of scientific labor in an increasingly computational research landscape.\n",
        "\n",
        "## Limitations\n",
        "\n",
        "Our data collection approach introduces several methodological constraints that should be considered when interpreting these results. By focusing exclusively on GitHub repositories, we likely miss contributions stored on alternative platforms such as GitLab, Bitbucket, or institutional repositories, potentially skewing our understanding of contribution patterns. As @trujillo2022penumbra, @Cao2023TheRO, and @escamilla2022riseofgithub have all noted, while GitHub is the predominate host of scientific software, significant portions of research code exist on other platforms. Additionally, our reliance on public repositories means we cannot account for private repositories or code that were never publicly shared, potentially underrepresenting sensitive research areas or proprietary methods.\n",
        "\n",
        "Our predictive modeling approach for matching authors with developer accounts presents additional limitations. The model's performance can be affected by shorter names where less textual information is available for matching, potentially creating biases against researchers from cultures with shorter naming conventions. Organization accounts used for project management pose particular challenges for accurate matching, and while we implemented filtering mechanisms to minimize their impact, some misclassifications may persist. Furthermore, our approach may not capture all code contributors if multiple individuals developed code. However, only one uploaded it to a repository, creating attribution artifacts that may systematically underrepresent specific contributors, particularly junior researchers or technical staff who may not have direct repository access.\n",
        "\n",
        "Our analytical approach required substantial data filtering to ensure reliable results, introducing potential selection biases in our sample. By focusing on article-repository pairs with commit activity no later than 90 days past the date of article publication and at least three authors and less than `{python} int(n_authors_97th_percentile)` authors, we may have systematically excluded certain types of research projects, particularly those with extended development timelines or extensive collaborations. Our categorization of coding status (non-coder, any coding, majority coding, always coding) necessarily simplifies complex contribution patterns. It does not account for code contributions' quality, complexity, or significance. Additionally, our reliance on OpenAlex metadata introduces certain limitations to our analysis. While OpenAlex provides good overall coverage, it lags behind proprietary databases in indexing references and citations. The lag in OpenAlex data may affect our citation-based analyses and the completeness of author metadata used in our study [@alperin2024analysis].\n",
        "\n",
        "## Future Work\n",
        "\n",
        "Future technical improvements may enhance our understanding of the relationship between software development and scientific recognition systems. Expanding analysis beyond GitHub to include other code hosting platforms would provide a more comprehensive understanding of scientific software development practices across domains and institutional contexts. More sophisticated entity-matching techniques could improve author-developer account identification, particularly for cases with limited information or common names. Developing more nuanced measures and classifications of code contribution type, quality, and significance beyond binary contribution identification would better capture the true impact of technical contributions to research. These methodological advances would enable more precise tracking of how code contributions translateor fail to translateinto formal scientific recognition, providing clearer evidence for policy interventions.\n",
        "\n",
        "Our findings point to several directions for future research on the changing nature of scientific labor and recognition. Longitudinal studies tracking how code contribution patterns affect career trajectories would provide valuable insights into the long-term impacts of the observed h-index disparities and whether these effects vary across career stages. Comparative analyses across different scientific domains could reveal discipline-specific norms and practices around software recognition, potentially identifying models that more equitably credit technical contributions. Qualitative studies examining how research teams make authorship decisions regarding code contributors would complement our quantitative findings by illuminating the social and organizational factors influencing recognition practices. Additionally, to better understand corresponding authors' role in maintaining research artifacts, future work could remove the 90-day post-publication commit activity filter to examine long-term sustainability actions. However, this approach would need to address the introduction of contributors unrelated to the original paper.\n",
        "\n",
        "Despite their growing importance, the persistent underrecognition of software contributions suggests a need for structural interventions in how we conceptualize and reward scientific work. Building upon efforts like CRediT [@brand2015beyond], future work should investigate potential policy changes to better align institutional incentives with the diverse spectrum of contributions that drive modern scientific progress. However, as the example of CRediT demonstrates, even well-intentioned taxonomies may reproduce existing hierarchies or create new forms of inequality if they fail to address underlying power dynamics in scientific communities. The challenge is not merely technical but social: creating recognition systems that simultaneously support innovation, ensure appropriate credit, maintain research integrity, and foster equitable participation in an increasingly computational scientific enterprise.\n",
        "\n",
        "# References\n",
        "\n",
        "::: {#refs}\n",
        ":::\n",
        "\n",
        "# Appendix\n",
        "\n",
        "## Extended Data and Methods\n",
        "\n",
        "### Building a Dataset of Linked Scientific Articles and Code Repositories\n",
        "\n",
        "The increasing emphasis on research transparency has led many journals and platforms to require or recommend code and data sharing [@stodden2013toward; @sharma2024analytical], creating traceable links between publications and code. These explicit links enable systematic study of both article-repository and author-developer account relationships [@Hata2021ScienceSoftwareLT; @Kelley2021AFF; @Stankovski2024RepoFromPaperAA; @milewicz2019characterizing].\n",
        "\n",
        "Our dataset collection process leveraged four sources of linked scientific articles and code repositories, each with specific mechanisms for establishing these connections:\n",
        "\n",
        "1.  **Public Library of Science (PLOS)**: We extracted repository links from PLOS articles' mandatory data and code availability statements.\n",
        "2.  **Journal of Open Source Software (JOSS)**: JOSS requires explicit code repository submission and review as a core part of its publication process.\n",
        "3.  **SoftwareX**: Similar to JOSS, SoftwareX mandates code repositories as a publication requirement.\n",
        "4.  **Papers with Code**: This platform directly connects machine learning preprints with their implementations. We focus solely on the \"official\" article-repository relationships rather than the \"unverified\" or \"unofficial\" links.\n",
        "\n",
        "To create a comprehensive and analyzable dataset, we enriched these article-repository pairs with metadata from multiple sources. We utilized the Semantic Scholar API for DOI resolution to ensure we found the latest version of each article. This resolution step was particularly important when working with preprints, as journals may have published these papers since their inclusion in the Papers with Code dataset. Using Semantic Scholar, we successfully resolved `{python} round(overall_doi_resolution * 100, 1)`% of all DOIs within our dataset[^4].\n",
        "\n",
        "[^4]: Broken out by dataset source, we resolved `{python} round(per_dataset_doi_resolution[\"plos\"] * 100, 1)`% of all PLOS DOIs, `{python} round(per_dataset_doi_resolution[\"joss\"] * 100, 1)`% of all JOSS DOIs, `{python} round(per_dataset_doi_resolution[\"softwarex\"] * 100, 1)`% of all SoftwareX DOIs, and `{python} round(per_dataset_doi_resolution[\"pwc\"] * 100, 1)`% of all Papers with Code (arXiv) DOIs.\n",
        "\n",
        "We then utilized the OpenAlex API to gather detailed publication metadata, including:\n",
        "\n",
        "-   Publication characteristics (open access status, domain, publication date)\n",
        "-   Author details (name, author position, corresponding author status)\n",
        "-   Article- and individual-level metrics (citation counts, FWCI, h-index)\n",
        "\n",
        "Similarly, the GitHub API provided comprehensive information for source code repositories:\n",
        "\n",
        "-   Repository metadata (name, description, programming languages, creation date)\n",
        "-   Contributor details (username, display name, email)\n",
        "-   Repository-level metrics (star count, fork count, issue count)\n",
        "\n",
        "### Developing a Predictive Model for Author-Developer Account Matching\n",
        "\n",
        "#### Annotated Dataset Creation\n"
      ],
      "id": "4e477bc1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "\n",
        "# We run this in it's own cell so that output from data download doesn't show\n",
        "# Load annotated dataset\n",
        "annotated_dataset = sci_soft_models_data.load_annotated_dev_author_em_dataset()\n",
        "\n",
        "# Get counts of positive and negative examples\n",
        "n_positive_examples = len(annotated_dataset.loc[annotated_dataset[\"match\"]])\n",
        "n_negative_examples = len(annotated_dataset.loc[~annotated_dataset[\"match\"]])\n",
        "pct_positive_examples = round(n_positive_examples / len(annotated_dataset), 3) * 100\n",
        "pct_negative_examples = round(n_negative_examples / len(annotated_dataset), 3) * 100\n",
        "\n",
        "# Create function to split row details\n",
        "def split_dev_details(dev_details: str) -> dict:\n",
        "    details_list = dev_details.split(\";\\\\n\")\n",
        "    details_dict = {}\n",
        "    for detail in details_list:\n",
        "        try:\n",
        "            key, value = detail.split(\": \", 1)\n",
        "        except Exception:\n",
        "            print(detail)\n",
        "            raise\n",
        "\n",
        "        # Only keep username, name, and email\n",
        "        if key in [\"username\", \"name\", \"email\"]:\n",
        "            # If value is \"None\" replace with None\n",
        "            if value == \"None\":\n",
        "                value = None\n",
        "\n",
        "            # Store to details dict\n",
        "            details_dict[key] = value\n",
        "    \n",
        "    return details_dict\n",
        "\n",
        "# Split dev details\n",
        "all_dev_details = annotated_dataset[\"dev_details\"].apply(split_dev_details)\n",
        "\n",
        "# Convert to dataframe\n",
        "all_dev_details_df = pd.DataFrame(all_dev_details.tolist())\n",
        "\n",
        "# Drop duplicates on username\n",
        "all_dev_details_df = all_dev_details_df.drop_duplicates(subset=[\"username\"])\n",
        "\n",
        "# Get counts\n",
        "n_devs = len(all_dev_details_df)\n",
        "n_devs_with_name = all_dev_details_df[\"name\"].notnull().sum()\n",
        "n_devs_with_email = all_dev_details_df[\"email\"].notnull().sum()\n",
        "\n",
        "pct_devs_with_name = round(n_devs_with_name / n_devs, 3) * 100\n",
        "pct_devs_with_email = round(n_devs_with_email / n_devs, 3) * 100\n",
        "\n",
        "# Get unique authors from original frame from unique semantic scholar id\n",
        "n_authors = annotated_dataset[\"semantic_scholar_id\"].nunique()"
      ],
      "id": "f75734ef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating an accurate author-developer account matching model required quality-labeled training data that reflects real-world identity matching challenges. Exact-matching on names or emails proved insufficient due to variations in formatting (e.g., \"J. Doe\" vs. \"Jane Doe\"), use of institutional versus personal email addresses, and incomplete information. However, author and developer account information often contains sufficient similarities for probabilistic matching, such as when author \"Jane Doe\" corresponds to username \"jdoe\" or \"janedoe123\".\n",
        "\n",
        "To efficiently build our training and evaluation dataset, we used JOSS articles as we believed they typically feature higher author-developer-account overlap, increasing positive match density. Our dataset creation process followed these steps:\n",
        "\n",
        "1.  We generated semantic embeddings for each developer account and author name using the [multi-qa-MiniLM-L6-cos-v1](https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1) model from the Sentence Transformers Python library [@reimers-2019-sentence-bert].\n",
        "2.  We calculated cosine similarity between all potential author-developer-account pairs for each article-repository pair.\n",
        "3.  For annotation efficiency, we selected the three most similar authors for each developer account.\n",
        "\n",
        "From these generated author-developer-account pairs, we randomly selected 3,000 for classification by two independent annotators as either matches or non-matches, resolving disagreements through discussion and verification. The resulting dataset contains `{python} n_positive_examples` (`{python} pct_positive_examples`%) positive matches and `{python} n_negative_examples` (`{python} pct_negative_examples`%) negative matches, comprising `{python} n_authors` unique authors and `{python} n_devs` unique developer accounts.\n",
        "\n",
        "Our collected data for annotation confirmed that exact matching would be insufficientonly `{python} int(n_devs_with_name)` (`{python} float(pct_devs_with_name)`%) of developer accounts had associated display names and just `{python} int(n_devs_with_email)` (`{python} float(pct_devs_with_email)`%) had associated email addresses.\n",
        "\n",
        "#### Training and Evaluation {#sec-appendix-model-training-eval}\n"
      ],
      "id": "71cad333"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "training_split_counts = sci_soft_models_data.load_final_model_training_split_details()\n",
        "\n",
        "# Take sum on rows\n",
        "training_split_counts = training_split_counts.set_index(\"split\").sum(axis=1)\n",
        "training_split_pct = ((training_split_counts / training_split_counts.sum()) * 100).round(1)"
      ],
      "id": "31a72e51",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our training and evaluation methodology began with careful dataset preparation to prevent data leakage between training and test sets. To ensure complete separation of authors and developers, we randomly selected 10% of unique authors and 10% of unique developers, designating any pairs containing these selected entities for the test set. This entity-based splitting strategy resulted in `{python} int(training_split_counts[\"train\"])` (`{python} float(training_split_pct[\"train\"])`%) pairs for training and `{python} int(training_split_counts[\"test\"])` (`{python} float(training_split_pct[\"test\"])`%) pairs for testing.\n",
        "\n",
        "For our predictive model, we evaluated three transformer-based architectures that have demonstrated strong performance in entity matching tasks:\n",
        "\n",
        "-   DeBERTa-v3-base [@he2021debertav3; @he2021deberta]\n",
        "-   mBERT (bert-base-multilingual-cased) [@bert2018]\n",
        "-   DistilBERT [@Sanh2019DistilBERTAD]\n",
        "\n",
        "We systematically evaluated these base models across different combinations of developer-account features, ranging from using only the username to incorporating complete profile information (username, display name, and email address). We fine-tuned all models using the Adam optimizer with a linear learning rate of 1e-05 for training and a batch size of 8 for training and evaluation. Given the size of our dataset and the binary nature of our classification task, models were trained for a single epoch to prevent overfitting.\n",
        "\n",
        "We evaluated model performance using standard binary classification metrics: precision, recall, and F1-score. This evaluation framework allowed us to directly compare model architectures and feature combinations while accounting for the balance between precision and recall in identifying correct matches.\n",
        "\n",
        "Our comprehensive model evaluation revealed that fine-tuning DeBERTa-v3-base [@he2021debertav3] with developer username and display name as input features produces optimal performance for author-developer matching. This model configuration achieved a binary F1 score of `{python} float(best_model_f1)`, with an accuracy of `{python} float(best_model_acc)`, precision of `{python} float(best_model_prec)`, and recall of `{python} float(best_model_rec)`. @tbl-em-model-comparison presents a complete comparison of model architectures and feature combinations.\n",
        "\n",
        "::: {.content-visible when-format=\"html\"}"
      ],
      "id": "5be24974"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-em-model-comparison\n",
        "#| tbl-cap: Comparison of Models for Author-Developer-Account Matching\n",
        "\n",
        "exp_results"
      ],
      "id": "tbl-em-model-comparison",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.content-visible when-format=\"pdf\"}"
      ],
      "id": "4344dbeb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def df_to_latex_table(\n",
        "    df: pd.DataFrame,\n",
        "    caption: str,\n",
        "    label: str,\n",
        "    alignment: str = None,\n",
        "    float_format: str = \".3f\"\n",
        ") -> str:\n",
        "    # Generate column alignment if not provided\n",
        "    if alignment is None:\n",
        "        # Use 'l' for string/object columns, 'r' for numeric columns\n",
        "        alignment = \"\".join(\n",
        "            [\"l\" if dtype.kind in \"OUS\" else \"r\" for dtype in df.dtypes]\n",
        "        )\n",
        "    else:\n",
        "        # Ensure provided alignment matches number of columns\n",
        "        if len(alignment) != len(df.columns):\n",
        "            raise ValueError(f\"Alignment string length ({len(alignment)}) must match number of columns ({len(df.columns)})\")\n",
        "    \n",
        "    # Create header row\n",
        "    header_row = \" & \".join([f\"\\\\textbf{{{col}}}\" for col in df.columns]) + \" \\\\\\\\\"\n",
        "    \n",
        "    # Create data rows with alternating colors\n",
        "    data_rows = []\n",
        "    for i, row in enumerate(df.itertuples(index=False)):\n",
        "        color = \"gray!10\" if i % 2 == 1 else \"white\"\n",
        "        \n",
        "        # Format each cell value\n",
        "        cells = []\n",
        "        for val, dtype in zip(row, df.dtypes):\n",
        "            if pd.isna(val):\n",
        "                cells.append(\"\")\n",
        "            elif dtype.kind in \"fc\":  # Float or complex\n",
        "                try:\n",
        "                    cells.append(f\"{float(val):{float_format}}\")\n",
        "                except (ValueError, TypeError):\n",
        "                    cells.append(str(val))\n",
        "            else:\n",
        "                cells.append(str(val))\n",
        "        \n",
        "        # Create the row with consistent cell coloring\n",
        "        colored_cells = [f\"\\\\cellcolor{{{color}}}{cell}\" for cell in cells]\n",
        "        row_str = \" & \".join(colored_cells) + \" \\\\\\\\\"\n",
        "        data_rows.append(f\"    {row_str}\")\n",
        "    \n",
        "    # Combine into final table\n",
        "    table_template = f\"\"\"\\\\begin{{table}}\n",
        "\\\\centering\n",
        "\\\\caption{{{caption}}}\n",
        "\\\\label{{{label}}}\n",
        "\\\\begin{{tabular}}{{{alignment}}}\n",
        "\\\\toprule\n",
        "{header_row}\n",
        "\\\\midrule\n",
        "{chr(10).join(data_rows)}\n",
        "\\\\bottomrule\n",
        "\\\\end{{tabular}}\n",
        "\\\\end{{table}}\"\"\"\n",
        "\n",
        "    return table_template"
      ],
      "id": "4244330b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Convert to LaTeX table\n",
        "model_comparison_latex = df_to_latex_table(\n",
        "    exp_results,\n",
        "    caption=\"Comparison of Models for Author-Developer-Account Matching\",\n",
        "    label=\"tbl-em-model-comparison\",\n",
        "    alignment=\"llrrrr\",\n",
        ")\n",
        "\n",
        "IPython.display.Latex(model_comparison_latex)"
      ],
      "id": "292bfa02",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "Analysis of each model's performance revealed that including developer display names had the largest positive impact on model performance compared to username alone. We also observed that mBERT's performance was comparable to DeBERTa's while using the developer email address as an additional input feature. However, we selected the DeBERTa configuration as it had consistent strong performance across various feature combinations.\n",
        "\n",
        "To facilitate the reuse of our work, we have made our trained model and supporting code publicly available. Complete fine-tuning, evaluation, and inference code is available as the Python package: [sci-soft-models](https://github.com/evamaxfield/sci-soft-models), and the fine-tuned model has been released on HuggingFace ([evamxb/dev-author-em-clf](https://huggingface.co/evamxb/dev-author-em-clf)).\n",
        "\n",
        "#### Model Limitations\n",
        "\n",
        "While our model demonstrates strong performance, we acknowledge certain limitations in our approach:\n",
        "\n",
        "1.  **Short name sensitivity**: Shorter names (both usernames and display names) can affect the model's performance, as less textual information is available for matching.\n",
        "2.  **Organization accounts**: Research lab accounts used for project management present a potential challenge for accurate matching, as they don't correspond to individual authors. However, our filtering mechanisms applied before analysis help minimize their impact in modeling.\n",
        "\n",
        "### Dataset Characteristics and Repository Types\n",
        "\n",
        "Our compiled dataset appears to contain a mix of repository types, varying from analysis script repositories to software tools and likely some \"code dumps\" (where code is copied to a new repository immediately before publication). This diversity is reflected in the commit duration patterns across different publication types. The median commit duration for repositories in our analysis is:\n",
        "\n",
        "-   47 days for preprints\n",
        "-   104 days for research articles\n",
        "-   247 days for software articles\n",
        "\n",
        "Complete statistics on commit durations, including count, mean, and quantile details, are available in @tbl-commit-duration-distributions.\n"
      ],
      "id": "f71df7cf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-commit-duration-distributions\n",
        "#| tbl-cap: Commit duration (in days) distributions for different publication types. Only includes article-repository pairs with a most recent commit no later than 90 days after publication and excludes publications from research teams in the top 3% of total author sizes.\n",
        "\n",
        "commit_dist_plotting_data = team_comp_no_push_after_pub.copy()\n",
        "commit_dist_plotting_data = commit_dist_plotting_data.sort_values(\n",
        "    \"article_type\",\n",
        ")\n",
        "commit_dist_plotting_data[\"repo_commit_duration\"] = commit_dist_plotting_data[\"repo_commit_duration\"] * 365.25\n",
        "commit_dist_plotting_data.groupby([\"article_type\"])[\"repo_commit_duration\"].describe(percentiles=[0.1, 0.25, 0.5, 0.75, 0.9])"
      ],
      "id": "tbl-commit-duration-distributions",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Distributions of Author-Developer-Account Prediction Confidence\n"
      ],
      "id": "0588d8b0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-dist-of-author-dev-pred-confidence\n",
        "#| fig-cap: Distribution of author-developer-account prediction confidence scores. The left plot shows the distribution of all prediction confidence scores, while the right plot shows the distribution of prediction confidence scores for author-developer-account pairs with a confidence score greater than or equal to 0.97.\n",
        "\n",
        "# Create a reduced copy of researcher_dev_links\n",
        "reduced_researcher_dev_links = (\n",
        "    researcher_dev_links.copy().sort_values(\n",
        "        by=\"predictive_model_confidence\",\n",
        "        ascending=False,\n",
        "    )\n",
        "    .drop_duplicates(\n",
        "        subset=[\"developer_account_id\"],\n",
        "        keep=\"first\",\n",
        "    )\n",
        "    .drop_duplicates(\n",
        "        subset=[\"researcher_id\"],\n",
        "        keep=\"first\",\n",
        "    )\n",
        ")\n",
        "\n",
        "# Select columns of interest, change column names, and merge\n",
        "merged_reduced_researcher_dev_links = (\n",
        "    reduced_researcher_dev_links[\n",
        "        [\n",
        "            \"researcher_id\",\n",
        "            \"developer_account_id\",\n",
        "            \"predictive_model_confidence\",\n",
        "        ]\n",
        "    ]\n",
        "    .rename(\n",
        "        columns={\n",
        "            \"predictive_model_confidence\": \"confidence\",\n",
        "        },\n",
        "    )\n",
        "    .merge(\n",
        "        researchers.copy()[[\"id\", \"name\"]].rename(\n",
        "            columns={\n",
        "                \"id\": \"researcher_id\",\n",
        "                \"name\": \"researcher_name\",\n",
        "            },\n",
        "        ),\n",
        "        on=\"researcher_id\",\n",
        "    )\n",
        "    .merge(\n",
        "        devs.copy()[[\"id\", \"username\", \"name\"]].rename(\n",
        "            columns={\n",
        "                \"id\": \"developer_account_id\",\n",
        "                \"username\": \"dev_username\",\n",
        "                \"name\": \"dev_provided_name\",\n",
        "            },\n",
        "        ),\n",
        "        on=\"developer_account_id\",\n",
        "    )\n",
        ")[[\n",
        "    \"researcher_name\",\n",
        "    \"dev_username\",\n",
        "    \"dev_provided_name\",\n",
        "    \"confidence\",\n",
        "]]\n",
        "\n",
        "thresholded_reduced_researcher_dev_links = (\n",
        "    merged_reduced_researcher_dev_links.copy().loc[\n",
        "        merged_reduced_researcher_dev_links[\"confidence\"] >= 0.97\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Caluclate how much data was removed\n",
        "count_removed_from_conf_thresh = len(\n",
        "    merged_reduced_researcher_dev_links\n",
        ") - len(thresholded_reduced_researcher_dev_links)\n",
        "pct_removed_from_conf_thresh = round(\n",
        "    (\n",
        "        1 - (len(thresholded_reduced_researcher_dev_links) / len(merged_reduced_researcher_dev_links))\n",
        "    ) * 100,\n",
        "    1,\n",
        ")\n",
        "\n",
        "# Add new column to \"merged_reduced_researcher_dev_links\" df that is \"set\" and has \"full\" as value\n",
        "merged_reduced_researcher_dev_links[\"set\"] = \"full\"\n",
        "# Add new column to \"thresholded_reduced_researcher_dev_links\" df that is \"set\" and has \"thresholded\" as value\n",
        "thresholded_reduced_researcher_dev_links[\"set\"] = \"threshold\"\n",
        "\n",
        "# Concatenate the two dataframes\n",
        "full_and_thresh = pd.concat(\n",
        "    [\n",
        "        merged_reduced_researcher_dev_links,\n",
        "        thresholded_reduced_researcher_dev_links,\n",
        "    ],\n",
        "    ignore_index=True,\n",
        ").reset_index(drop=True)\n",
        "\n",
        "# Create plot\n",
        "g = sns.displot(\n",
        "    full_and_thresh,\n",
        "    x=\"confidence\",\n",
        "    kind=\"ecdf\",\n",
        "    col=\"set\",\n",
        "    hue=\"set\",\n",
        "    facet_kws={\"sharex\": False},\n",
        "    legend=False,\n",
        ")\n",
        "\n",
        "# Set titles\n",
        "g.set_titles(col_template=\"{col_name} set\")\n",
        "g.fig.tight_layout()"
      ],
      "id": "fig-dist-of-author-dev-pred-confidence",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Thresholding the predictive model confidence at 0.97 resulted in a `{python} pct_removed_from_conf_thresh`% (`{python} count_removed_from_conf_thresh`) reduction in the number of author-developer-account pairs (from an unfiltered total of `{python} len(merged_reduced_researcher_dev_links)` author-developer-account pairs). This threshold was chosen to ensure a high level of confidence in the matches while retaining a large number of pairs for analysis.\n",
        "\n",
        "## Filtered Dataset Description for Article-Citation, Author-Position, and Author-Correspondence Analysis\n",
        "\n",
        "::: {.content-visible when-format=\"html\"}"
      ],
      "id": "381a0908"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-team-comp-no-push-after-pub-counts\n",
        "#| tbl-cap: Counts of article-repository pairs, authors, and developers for research teams. Only includes research teams from article-repository pairs with a most recent commit no later than 90 days after publication and excludes research teams in the top 3% of total author sizes.\n",
        "\n",
        "team_comp_no_push_after_pub_counts_html = _get_html_for_counts_table(\n",
        "    domain_stats=team_comp_no_push_after_pub_counts_domain,\n",
        "    doc_type_stats=team_comp_no_push_after_pub_counts_article_type,\n",
        "    access_stats=team_comp_no_push_after_pub_counts_open_access,\n",
        "    data_source_stats=team_comp_no_push_after_pub_counts_dataset_source,\n",
        "    total_article_repo_pairs=team_comp_no_push_after_pub_total_pairs,\n",
        "    total_authors=team_comp_no_push_after_pub_total_authors,\n",
        "    total_devs=team_comp_no_push_after_pub_total_devs,\n",
        ")\n",
        "\n",
        "IPython.display.HTML(team_comp_no_push_after_pub_counts_html)"
      ],
      "id": "tbl-team-comp-no-push-after-pub-counts",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.content-visible when-format=\"pdf\"}"
      ],
      "id": "33511b80"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "team_comp_no_push_after_pub_counts_latex = _get_latex_for_counts_table(\n",
        "    caption=(\n",
        "        \"Counts of article-repository pairs, authors, and developers for research teams. \"\n",
        "        \"Only includes research teams from article-repository pairs with a \"\n",
        "        \"most recent commit no later than 90 days after publication and excludes \"\n",
        "        \"research teams in the top 3\\% of total author sizes.\"\n",
        "    ),\n",
        "    label=\"tbl-team-comp-no-push-after-pub-counts\",\n",
        "    domain_stats=team_comp_no_push_after_pub_counts_domain,\n",
        "    doc_type_stats=team_comp_no_push_after_pub_counts_article_type,\n",
        "    access_stats=team_comp_no_push_after_pub_counts_open_access,\n",
        "    data_source_stats=team_comp_no_push_after_pub_counts_dataset_source,\n",
        "    total_article_repo_pairs=team_comp_no_push_after_pub_total_pairs,\n",
        "    total_authors=team_comp_no_push_after_pub_total_authors,\n",
        "    total_devs=team_comp_no_push_after_pub_total_devs,\n",
        ")\n",
        "\n",
        "IPython.display.Latex(team_comp_no_push_after_pub_counts_latex)"
      ],
      "id": "12b65e4a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "## Article Citation Linear Model Results\n",
        "\n",
        "::: {.content-visible when-format=\"html\"}"
      ],
      "id": "5645e3d8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-article-composition-overall\n",
        "#| tbl-cap: Article citations by code contributorship of research team. Generalized linear model fit with negative binomial distribution and log link function.\n",
        "\n",
        "article_cited_by_count_models[\"no-control\"].summary()"
      ],
      "id": "tbl-article-composition-overall",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.content-visible when-format=\"pdf\"}"
      ],
      "id": "66b09be7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_results_latex, model_results_df = convert_model_results_to_printable_pdf_ready(\n",
        "    article_cited_by_count_models[\"no-control\"],\n",
        "    tbl_cap=(\n",
        "        \"Article citations by code contributorship of research team. \"\n",
        "        \"Generalized linear model fit with negative binomial distribution \"\n",
        "        \"and log link function. Significant p-values are indicated with asterisks: \"\n",
        "        \"p < 0.05 (*), p < 0.01 (**), p < 0.001 (***)\"\n",
        "    ),\n",
        "    tbl_label=\"tbl-article-composition-overall\",\n",
        ")\n",
        "\n",
        "IPython.display.Latex(model_results_latex)"
      ],
      "id": "369efc77",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.content-visible when-format=\"html\"}"
      ],
      "id": "4642b8a5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-article-composition-oa-status\n",
        "#| tbl-cap: Article citations by code contributorship of research team controlled by open access status. Generalized linear model fit with negative binomial distribution and log link function.\n",
        "\n",
        "article_cited_by_count_models[\"Is Open Access\"].summary()"
      ],
      "id": "tbl-article-composition-oa-status",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.content-visible when-format=\"pdf\"}"
      ],
      "id": "ef0cd5ea"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_results_latex, model_results_df = convert_model_results_to_printable_pdf_ready(\n",
        "    article_cited_by_count_models[\"Is Open Access\"],\n",
        "    tbl_cap=(\n",
        "        \"Article citations by code contributorship of research team \"\n",
        "        \"controlled by open access status. \"\n",
        "        \"Generalized linear model fit with negative binomial distribution \"\n",
        "        \"and log link function. Significant p-values are indicated with asterisks: \"\n",
        "        \"p < 0.05 (*), p < 0.01 (**), p < 0.001 (***)\"\n",
        "    ),\n",
        "    tbl_label=\"tbl-article-composition-oa-status\",\n",
        ")\n",
        "\n",
        "IPython.display.Latex(model_results_latex)"
      ],
      "id": "2af3b564",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.content-visible when-format=\"html\"}"
      ],
      "id": "961397ba"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-article-composition-domain\n",
        "#| tbl-cap: Article citations by code contributorship of research team controlled by domain. Generalized linear model fit with negative binomial distribution and log link function.\n",
        "\n",
        "article_cited_by_count_models[\"Domain\"].summary()"
      ],
      "id": "tbl-article-composition-domain",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.content-visible when-format=\"pdf\"}"
      ],
      "id": "7e712626"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_results_latex, model_results_df = convert_model_results_to_printable_pdf_ready(\n",
        "    article_cited_by_count_models[\"Domain\"],\n",
        "    tbl_cap=(\n",
        "        \"Article citations by code contributorship of research team \"\n",
        "        \"controlled by domain. \"\n",
        "        \"Generalized linear model fit with negative binomial distribution \"\n",
        "        \"and log link function. Significant p-values are indicated with asterisks: \"\n",
        "        \"p < 0.05 (*), p < 0.01 (**), p < 0.001 (***)\"\n",
        "    ),\n",
        "    tbl_label=\"tbl-article-composition-domain\",\n",
        ")\n",
        "\n",
        "IPython.display.Latex(model_results_latex)"
      ],
      "id": "3ef4ca51",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.content-visible when-format=\"html\"}"
      ],
      "id": "07c530f8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-article-composition-type\n",
        "#| tbl-cap: Article citations by code contributorship of research team controlled by article type. Generalized linear model fit with negative binomial distribution and log link function.\n",
        "\n",
        "article_cited_by_count_models[\"Article Type\"].summary()"
      ],
      "id": "tbl-article-composition-type",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.content-visible when-format=\"pdf\"}"
      ],
      "id": "d76d1c77"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_results_latex, model_results_df = convert_model_results_to_printable_pdf_ready(\n",
        "    article_cited_by_count_models[\"Article Type\"],\n",
        "    tbl_cap=(\n",
        "        \"Article citations by code contributorship of research team \"\n",
        "        \"controlled by article type. \"\n",
        "        \"Generalized linear model fit with negative binomial distribution \"\n",
        "        \"and log link function. Significant p-values are indicated with asterisks: \"\n",
        "        \"p < 0.05 (*), p < 0.01 (**), p < 0.001 (***)\"\n",
        "    ),\n",
        "    tbl_label=\"tbl-article-composition-type\",\n",
        ")\n",
        "\n",
        "IPython.display.Latex(model_results_latex)"
      ],
      "id": "b38921b2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "## Post-Hoc Tests for Coding vs Non-Coding Authors by Position\n",
        "\n",
        "::: {.content-visible when-format=\"html\"}"
      ],
      "id": "197b3c10"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-post-hoc-tests-on-author-positions\n",
        "#| tbl-cap: 'Counts of Code-Contributing Authors (''Coding'') as well as Total Authors by Position and Bonferroni Corrected p-values from Post-Hoc Binomial Tests. Significant p-values are indicated with asterisks: p < 0.05 (`*`), p < 0.01 (`**`), p < 0.001 (`***`).'\n",
        "\n",
        "def create_multirow_html_table(\n",
        "    dataframes_dict: dict[str, pd.DataFrame],\n",
        "    split_var: str,\n",
        "):\n",
        "    # HTML templates for rows\n",
        "    initial_row_for_control_template = \"\"\"\n",
        "    <tr>\n",
        "        <td rowspan=\"{n_control_rows}\"><b>{control}</b></td>\n",
        "        <td rowspan=\"{n_subset_rows}\"><b>{subset}</b></td>\n",
        "        <td>{split_val}</td>\n",
        "        <td>{coding}</td>\n",
        "        <td>{total}</td>\n",
        "        <td>{p_value}</td>\n",
        "    </tr>\n",
        "    \"\"\".strip()\n",
        "\n",
        "    initial_row_for_subset_template = \"\"\"\n",
        "    <tr>\n",
        "        <td rowspan=\"{n_subset_rows}\"><b>{subset}</b></td>\n",
        "        <td>{split_val}</td>\n",
        "        <td>{coding}</td>\n",
        "        <td>{total}</td>\n",
        "        <td>{p_value}</td>\n",
        "    </tr>\n",
        "    \"\"\".strip()\n",
        "\n",
        "    subsequent_row_template = \"\"\"\n",
        "    <tr>\n",
        "        <td>{split_val}</td>\n",
        "        <td>{coding}</td>\n",
        "        <td>{total}</td>\n",
        "        <td>{p_value}</td>\n",
        "    </tr>\n",
        "    \"\"\".strip()\n",
        "\n",
        "    # Generate table rows\n",
        "    all_rows = []\n",
        "    \n",
        "    for control, df in dataframes_dict.items():\n",
        "        rows_for_control = []\n",
        "        # Group by subset to handle the spans\n",
        "        for subset_name, subset_group in df.groupby(\"Subset\"):\n",
        "            subset_rows = []\n",
        "            for idx, row in subset_group.iterrows():\n",
        "                # Format p-value\n",
        "                p_value = f\"{row['p']:.3f}\" if row['p'] >= 0.001 else \"0.000\"\n",
        "                if row[\"p\"] < 0.001:\n",
        "                    p_value = f\"{p_value}***\"\n",
        "                elif row[\"p\"] < 0.01:\n",
        "                    p_value = f\"{p_value}**\"\n",
        "                elif row[\"p\"] < 0.05:\n",
        "                    p_value = f\"{p_value}*\"\n",
        "                \n",
        "                row_data = {\n",
        "                    \"control\": control,\n",
        "                    \"subset\": subset_name,\n",
        "                    \"split_val\": row[split_var],\n",
        "                    \"coding\": row[\"Coding\"],\n",
        "                    \"total\": row[\"Total\"],\n",
        "                    \"p_value\": p_value,\n",
        "                    \"n_subset_rows\": len(subset_group),\n",
        "                    \"n_control_rows\": len(df),\n",
        "                }\n",
        "                \n",
        "                if len(rows_for_control) == 0 and len(subset_rows) == 0:\n",
        "                    subset_rows.append(initial_row_for_control_template.format(**row_data))\n",
        "                elif len(subset_rows) == 0:\n",
        "                    subset_rows.append(initial_row_for_subset_template.format(**row_data))\n",
        "                else:\n",
        "                    subset_rows.append(subsequent_row_template.format(**row_data))\n",
        "            \n",
        "            rows_for_control.extend(subset_rows)\n",
        "\n",
        "        all_rows.extend(rows_for_control)\n",
        "\n",
        "    # Create the final HTML table\n",
        "    table_html = f\"\"\"\n",
        "    <table>\n",
        "        <tr>\n",
        "            <th><b>Control</b></th>\n",
        "            <th><b>Subset</b></th>\n",
        "            <th><b>{split_var.replace('_', ' ').title()}</b></th>\n",
        "            <th><b>Coding</b></th>\n",
        "            <th><b>Total</b></th>\n",
        "            <th><b>p</b></th>\n",
        "        </tr>\n",
        "        {'\\n'.join(all_rows)}\n",
        "    </table>\n",
        "    \"\"\".strip()\n",
        "    \n",
        "    return table_html\n",
        "\n",
        "IPython.display.HTML(\n",
        "    create_multirow_html_table(\n",
        "        {\n",
        "            \"Domain\": domain_code_char,\n",
        "            \"Article Type\": article_type_code_char,\n",
        "            \"Open Access Status\": open_access_status_code_char,\n",
        "            \"Overall\": overall_code_char,\n",
        "        },\n",
        "        split_var=\"Position\",\n",
        "    )\n",
        ")"
      ],
      "id": "tbl-post-hoc-tests-on-author-positions",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.content-visible when-format=\"pdf\"}"
      ],
      "id": "fc7cd448"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def create_multirow_latex_table(\n",
        "    dataframes_dict: dict[str, pd.DataFrame],\n",
        "    caption: str,\n",
        "    label: str,\n",
        "    split_var: str,\n",
        "):\n",
        "    # LaTeX templates for rows\n",
        "    initial_row_for_control_template = \"\"\"    \\\\multirow{{{n_control_rows}}}{{*}}{{\\\\textbf{{{control}}}}} & \\\\multirow{{{n_subset_rows}}}{{*}}{{\\\\textbf{{{subset}}}}} & {color}{split_val} & {color}{coding} & {color}{total} & {color}{p_value} \\\\\\\\\"\"\"\n",
        "\n",
        "    initial_row_for_subset_template = \"\"\"    & \\\\multirow{{{n_subset_rows}}}{{*}}{{\\\\textbf{{{subset}}}}} & {color}{split_val} & {color}{coding} & {color}{total} & {color}{p_value} \\\\\\\\\"\"\"\n",
        "\n",
        "    subsequent_row_template = \"\"\"    & & {color}{split_val} & {color}{coding} & {color}{total} & {color}{p_value} \\\\\\\\\"\"\"\n",
        "\n",
        "    # Table header and footer templates\n",
        "    table_header = \"\"\"\\\\begin{table}\n",
        "\\\\centering\n",
        "\\\\small\n",
        "\\\\caption{%s}\n",
        "\\\\label{%s}\n",
        "\\\\begin{tabular}{llllrr}\n",
        "\\\\toprule\n",
        "\\\\textbf{Control} & \\\\textbf{Subset} & \\\\textbf{%s} & \\\\textbf{Coding} & \\\\textbf{Total} & \\\\textbf{p} \\\\\\\\\n",
        "\\\\midrule\"\"\" % (caption, label, split_var.replace('_', ' ').title())\n",
        "\n",
        "    table_footer = \"\"\"\\\\bottomrule\n",
        "\\\\end{tabular}\n",
        "\\\\end{table}\"\"\"\n",
        "\n",
        "    # Generate table rows\n",
        "    all_rows = []\n",
        "    overall_row_count = 0  # Counter for all rows to handle coloring\n",
        "    \n",
        "    for control, df in dataframes_dict.items():\n",
        "        rows_for_control = []\n",
        "        # Group by subset to handle the spans\n",
        "        for subset_name, subset_group in df.groupby(\"Subset\"):\n",
        "            subset_rows = []\n",
        "            for idx, row in subset_group.iterrows():\n",
        "                # Format p-value with significance stars\n",
        "                p_value = f\"{row['p']:.3f}\" if row['p'] >= 0.001 else \"0.000\"\n",
        "                if row[\"p\"] < 0.001:\n",
        "                    p_value = f\"{p_value}$^{{***}}$\"\n",
        "                elif row[\"p\"] < 0.01:\n",
        "                    p_value = f\"{p_value}$^{{**}}$\"\n",
        "                elif row[\"p\"] < 0.05:\n",
        "                    p_value = f\"{p_value}$^{{*}}$\"\n",
        "                \n",
        "                # Add cell color for odd-numbered rows\n",
        "                color = \"\\\\cellcolor{gray!10}\" if overall_row_count % 2 == 1 else \"\"\n",
        "                \n",
        "                row_data = {\n",
        "                    \"control\": control,\n",
        "                    \"subset\": subset_name,\n",
        "                    \"split_val\": row[split_var],\n",
        "                    \"coding\": row[\"Coding\"],\n",
        "                    \"total\": row[\"Total\"],\n",
        "                    \"p_value\": p_value,\n",
        "                    \"n_subset_rows\": len(subset_group),\n",
        "                    \"n_control_rows\": len(df),\n",
        "                    \"color\": color\n",
        "                }\n",
        "                \n",
        "                if len(rows_for_control) == 0 and len(subset_rows) == 0:\n",
        "                    subset_rows.append(initial_row_for_control_template.format(**row_data))\n",
        "                elif len(subset_rows) == 0:\n",
        "                    subset_rows.append(initial_row_for_subset_template.format(**row_data))\n",
        "                else:\n",
        "                    subset_rows.append(subsequent_row_template.format(**row_data))\n",
        "                \n",
        "                overall_row_count += 1\n",
        "            \n",
        "            rows_for_control.extend(subset_rows)\n",
        "            \n",
        "        # Add midrule between controls (except after the last control)\n",
        "        if control != list(dataframes_dict.keys())[-1]:\n",
        "            rows_for_control.append(\"    \\\\midrule\")\n",
        "            \n",
        "        all_rows.extend(rows_for_control)\n",
        "\n",
        "    # Combine all parts\n",
        "    latex_table = f\"\"\"{table_header}\n",
        "{chr(10).join(all_rows)}\n",
        "{table_footer}\"\"\"\n",
        "    \n",
        "    return latex_table\n",
        "\n",
        "IPython.display.Latex(\n",
        "    create_multirow_latex_table(\n",
        "        {\n",
        "            \"Domain\": domain_code_char,\n",
        "            \"Article Type\": article_type_code_char,\n",
        "            \"Open Access Status\": open_access_status_code_char,\n",
        "            \"Overall\": overall_code_char,\n",
        "        },\n",
        "        caption=\"Counts of Code-Contributing Authors ('Coding') as well as Total Authors by Position and Bonferroni Corrected p-values from Post-Hoc Binomial Tests. Significant p-values are indicated with asterisks: p < 0.05 (*), p < 0.01 (**), p < 0.001 (***).\",\n",
        "        label=\"tbl-post-hoc-tests-on-author-positions\",\n",
        "        split_var=\"Position\",\n",
        "    )\n",
        ")"
      ],
      "id": "d8e6a09c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "Counts of authors in @tbl-post-hoc-tests-on-author-positions may differ slightly from counts in @tbl-team-comp-no-push-after-pub-counts. @tbl-team-comp-no-push-after-pub-counts counts unique authors, while @tbl-post-hoc-tests-on-author-positions counts unique author-document pairs (i.e., the same author may appear in multiple documents).\n",
        "\n",
        "## Post-Hoc Tests for Coding vs Non-Coding Authors by Corresponding Status\n",
        "\n",
        "::: {.content-visible when-format=\"html\"}"
      ],
      "id": "46545abc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-post-hoc-tests-on-corresponding-status\n",
        "#| tbl-cap: 'Counts of Code-Contributing Authors (''Coding'') as well as Total Authors by Corresponding Status and Bonferroni Corrected p-values from Post-Hoc Binomial Tests. Significant p-values are indicated with asterisks: p < 0.05 (`*`), p < 0.01 (`**`), p < 0.001 (`***`).'\n",
        "\n",
        "IPython.display.HTML(\n",
        "    create_multirow_html_table(\n",
        "        {\n",
        "            \"Domain\": domain_code_corresponding,\n",
        "            \"Article Type\": article_type_code_corresponding,\n",
        "            \"Open Access Status\": open_access_status_code_corresponding,\n",
        "            \"Overall\": overall_code_corresponding,\n",
        "        },\n",
        "        split_var=\"Is Corresponding\",\n",
        "    )\n",
        ")"
      ],
      "id": "tbl-post-hoc-tests-on-corresponding-status",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.content-visible when-format=\"pdf\"}"
      ],
      "id": "dba7fa42"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "IPython.display.Latex(\n",
        "    create_multirow_latex_table(\n",
        "        {\n",
        "            \"Domain\": domain_code_corresponding,\n",
        "            \"Article Type\": article_type_code_corresponding,\n",
        "            \"Open Access Status\": open_access_status_code_corresponding,\n",
        "            \"Overall\": overall_code_corresponding,\n",
        "        },\n",
        "        caption=\"Counts of Code-Contributing Authors ('Coding') as well as Total Authors by Corresponding Status and Bonferroni Corrected p-values from Post-Hoc Binomial Tests. Significant p-values are indicated with asterisks: p < 0.05 (*), p < 0.01 (**), p < 0.001 (***).\",\n",
        "        label=\"tbl-post-hoc-tests-on-corresponding-status\",\n",
        "        split_var=\"Is Corresponding\",\n",
        "    )\n",
        ")"
      ],
      "id": "d09fab5e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "Counts of authors in @tbl-post-hoc-tests-on-corresponding-status may differ slightly from counts in @tbl-team-comp-no-push-after-pub-counts. @tbl-team-comp-no-push-after-pub-counts counts unique authors, while @tbl-post-hoc-tests-on-corresponding-status counts unique author-document pairs (i.e., the same author may appear in multiple documents).\n",
        "\n",
        "## Filtered Dataset Description for h-Index Analysis\n"
      ],
      "id": "a5e8b3ab"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def _get_counts_by_column_for_h_index_data(\n",
        "    df: pd.DataFrame,\n",
        "    column_name: str,\n",
        ") -> list[dict[str, int]]:\n",
        "    counts = []\n",
        "    for value in df[column_name].unique():\n",
        "        subset = df.loc[df[column_name] == value]\n",
        "\n",
        "        # Append counts\n",
        "        counts.append({\n",
        "            column_name: value,\n",
        "            \"n_authors\": subset[\"researcher_id\"].nunique(),\n",
        "            \"n_any_coding\": int(subset[\"any_coding\"].sum()),\n",
        "            \"n_majority_coding\": int(subset[\"majority_coding\"].sum()),\n",
        "            \"n_always_coding\": int(subset[\"always_coding\"].sum()),\n",
        "        })\n",
        "\n",
        "    return counts\n",
        "\n",
        "h_index_domain_stats = _get_counts_by_column_for_h_index_data(\n",
        "    researchers_w_3_docs_and_coded_agg,\n",
        "    \"common_domain\",\n",
        ")\n",
        "h_index_article_type_stats = _get_counts_by_column_for_h_index_data(\n",
        "    researchers_w_3_docs_and_coded_agg,\n",
        "    \"common_article_type\",\n",
        ")\n",
        "h_index_author_position_stats = _get_counts_by_column_for_h_index_data(\n",
        "    researchers_w_3_docs_and_coded_agg,\n",
        "    \"common_author_position\",\n",
        ")\n",
        "\n",
        "h_index_total_authors = researchers_w_3_docs_and_coded_agg[\"researcher_id\"].nunique()\n",
        "h_index_n_any_coding = int(researchers_w_3_docs_and_coded_agg[\"any_coding\"].sum())\n",
        "h_index_n_majority_coding = int(researchers_w_3_docs_and_coded_agg[\"majority_coding\"].sum())\n",
        "h_index_n_always_coding = int(researchers_w_3_docs_and_coded_agg[\"always_coding\"].sum())"
      ],
      "id": "17d74b62",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-format=\"html\"}"
      ],
      "id": "c89e07ec"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-h-index-counts\n",
        "#| tbl-cap: Counts of Total Authors, n Any Coding Authors, n Majority Coding Authors, and n Always Coding Authors by Common Domain, Document Type, and Author Position. Authors are only included if they have three or more publications within our dataset and are associated with no more than three developer accounts, with each association having a predicted model confidence of at least 97%.\n",
        "\n",
        "# Construct multi-row span HTML table\n",
        "# Columns should be: \"n_authors\", \"n_any_coding\", \"n_majority_coding\", \"n_always_coding\"\n",
        "# Rows should be:\n",
        "# \"By Common Domain\", \"By Common Document Type\", \"By Common Author Position\" and \"Total\"\n",
        "\n",
        "# HTML templates\n",
        "stats_piece_inital_row_template = \"\"\"\n",
        "<tr>\n",
        "<td rowspan=\"{n_rows}\">{row_name}</td>\n",
        "<td>{value_name}</td>\n",
        "<td>{n_authors}</td>\n",
        "<td>{n_any_coding}</td>\n",
        "<td>{n_majority_coding}</td>\n",
        "<td>{n_always_coding}</td>\n",
        "</tr>\n",
        "\"\"\".strip()\n",
        "\n",
        "stats_piece_subsequent_row_template = \"\"\"\n",
        "<tr>\n",
        "<td>{value_name}</td>\n",
        "<td>{n_authors}</td>\n",
        "<td>{n_any_coding}</td>\n",
        "<td>{n_majority_coding}</td>\n",
        "<td>{n_always_coding}</td>\n",
        "</tr>\n",
        "\"\"\".strip()\n",
        "\n",
        "# Iter over stats portions (and total)\n",
        "stats_portions_html = []\n",
        "for stats_portion, stats_name, value_key in [\n",
        "    (h_index_domain_stats, \"<b>By Commmon Domain</b>\", \"common_domain\"),\n",
        "    (h_index_article_type_stats, \"<b>By Document Type</b>\", \"common_article_type\"),\n",
        "    (h_index_author_position_stats, \"<b>By Author Position</b>\", \"common_author_position\"),\n",
        "    (\n",
        "        [\n",
        "            {\n",
        "                \"empty\": \"\",\n",
        "                \"n_authors\": f\"<b>{h_index_total_authors}</b>\",\n",
        "                \"n_any_coding\": f\"<b>{h_index_n_any_coding}</b>\",\n",
        "                \"n_majority_coding\": f\"<b>{h_index_n_majority_coding}</b>\",\n",
        "                \"n_always_coding\": f\"<b>{h_index_n_always_coding}</b>\",\n",
        "            }\n",
        "        ],\n",
        "        \"<b>Total</b>\",\n",
        "        \"empty\",\n",
        "    ),\n",
        "]:\n",
        "    # Order by value_key\n",
        "    if value_key != \"empty\":\n",
        "        stats_portion = sorted(\n",
        "            stats_portion, key=lambda x: x[value_key]\n",
        "        )\n",
        "\n",
        "    stats_portion_html = []\n",
        "    for i, stats_piece in enumerate(stats_portion):\n",
        "        if i == 0:\n",
        "            stats_portion_html.append(\n",
        "                stats_piece_inital_row_template.format(\n",
        "                    n_rows=len(stats_portion),\n",
        "                    row_name=stats_name,\n",
        "                    value_name=stats_piece[value_key].title(),\n",
        "                    n_authors=stats_piece[\"n_authors\"],\n",
        "                    n_any_coding=stats_piece[\"n_any_coding\"],\n",
        "                    n_majority_coding=stats_piece[\"n_majority_coding\"],\n",
        "                    n_always_coding=stats_piece[\"n_always_coding\"],\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            stats_portion_html.append(\n",
        "                stats_piece_subsequent_row_template.format(\n",
        "                    value_name=stats_piece[value_key].title(),\n",
        "                    n_authors=stats_piece[\"n_authors\"],\n",
        "                    n_any_coding=stats_piece[\"n_any_coding\"],\n",
        "                    n_majority_coding=stats_piece[\"n_majority_coding\"],\n",
        "                    n_always_coding=stats_piece[\"n_always_coding\"],\n",
        "                )\n",
        "            )\n",
        "\n",
        "    stats_portions_html.append(\"\\n\".join(stats_portion_html))\n",
        "\n",
        "# Concat and wrap in table\n",
        "stats_table_html = f\"\"\"\n",
        "<table>\n",
        "<tr>\n",
        "    <th><b>Category</b></th>\n",
        "    <th><b>Subset</b></th>\n",
        "    <th><b>Total Authors</b></th>\n",
        "    <th><b>Any Code</b></th>\n",
        "    <th><b>Majority Code</b></th>\n",
        "    <th><b>Always Code</b></th>\n",
        "</tr>\n",
        "{\" \".join(stats_portions_html)}\n",
        "</table>\n",
        "\"\"\".strip()\n",
        "\n",
        "IPython.display.HTML(stats_table_html)"
      ],
      "id": "tbl-h-index-counts",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.content-visible when-format=\"pdf\"}"
      ],
      "id": "f9a461f7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# LaTeX templates\n",
        "stats_piece_initial_row_template = \"\"\"    \\\\multirow{{{n_rows}}}{{*}}{{\\\\textbf{{{row_name}}}}} & \\\\cellcolor{{gray!10}}{value_name} & \\\\cellcolor{{gray!10}}{n_authors} & \\\\cellcolor{{gray!10}}{n_any_coding} & \\\\cellcolor{{gray!10}}{n_majority_coding} & \\\\cellcolor{{gray!10}}{n_always_coding} \\\\\\\\\"\"\"\n",
        "\n",
        "stats_piece_subsequent_row_template = \"\"\"    & {value_name} & {n_authors} & {n_any_coding} & {n_majority_coding} & {n_always_coding} \\\\\\\\\"\"\"\n",
        "\n",
        "caption = (\n",
        "    \"Counts of Total Authors, n Any Coding Authors, n Majority Coding Authors, \"\n",
        "    \"and n Always Coding Authors by Common Domain, Document Type, \"\n",
        "    \"and Author Position. Authors are only included if they have three \"\n",
        "    \"or more publications within our dataset and are associated with \"\n",
        "    \"no more than three developer accounts, with each association \"\n",
        "    \"having a predicted model confidence of at least 97\\%.\"\n",
        ")\n",
        "label = \"tbl-h-index-counts\"\n",
        "\n",
        "# Table header and footer templates\n",
        "table_header_start = \"\"\"\\\\begin{{table}}\n",
        "\\\\centering\n",
        "\\\\small\n",
        "\\\\caption{{{caption}}}\n",
        "\\\\label{{{label}}}\n",
        "\\\\begin{{tabular}}{{llrrrr}}\n",
        "\\\\toprule\n",
        "\\\\textbf{{Category}} & \\\\textbf{{Subset}} & \\\\textbf{{Total Authors}} & \\\\textbf{{Any Code}} & \\\\textbf{{Majority Code}} & \\\\textbf{{Always Code}} \\\\\\\\\n",
        "\\\\midrule\"\"\".format(\n",
        "    caption=caption,\n",
        "    label=label,\n",
        ")\n",
        "\n",
        "table_footer = \"\"\"\\\\bottomrule\n",
        "\\\\end{tabular}\n",
        "\\\\end{table}\"\"\"\n",
        "\n",
        "# Generate table content\n",
        "stats_portions_latex = []\n",
        "for stats_portion, stats_name, value_key in [\n",
        "    (h_index_domain_stats, \"By Commmon Domain\", \"common_domain\"),\n",
        "    (h_index_article_type_stats, \"By Document Type\", \"common_article_type\"),\n",
        "    (h_index_author_position_stats, \"By Author Position\", \"common_author_position\"),\n",
        "    (\n",
        "        [\n",
        "            {\n",
        "                \"empty\": \"\",\n",
        "                \"n_authors\": h_index_total_authors,\n",
        "                \"n_any_coding\": h_index_n_any_coding,\n",
        "                \"n_majority_coding\": h_index_n_majority_coding,\n",
        "                \"n_always_coding\": h_index_n_always_coding,\n",
        "            }\n",
        "        ],\n",
        "        \"Total\",\n",
        "        \"empty\",\n",
        "    ),\n",
        "]:\n",
        "    # Order by value_key\n",
        "    if value_key != \"empty\":\n",
        "        stats_portion = sorted(\n",
        "            stats_portion, key=lambda x: x[value_key]\n",
        "        )\n",
        "\n",
        "    stats_portion_latex = []\n",
        "    if stats_name == \"Total\":\n",
        "        stats_portion_latex.append(\n",
        "            f\"    \\\\textbf{{Total}} & & \\\\textbf{{{stats_portion[0]['n_authors']}}} & \\\\textbf{{{stats_portion[0]['n_any_coding']}}} & \\\\textbf{{{stats_portion[0]['n_majority_coding']}}} & \\\\textbf{{{stats_portion[0]['n_always_coding']}}} \\\\\\\\\"\n",
        "        )\n",
        "    else:\n",
        "        # Add the first row with the category label and colored cells\n",
        "        first_piece = stats_portion[0]\n",
        "        stats_portion_latex.append(\n",
        "            stats_piece_initial_row_template.format(\n",
        "                n_rows=len(stats_portion),\n",
        "                row_name=stats_name,\n",
        "                value_name=first_piece[value_key].title(),\n",
        "                n_authors=first_piece[\"n_authors\"],\n",
        "                n_any_coding=first_piece[\"n_any_coding\"],\n",
        "                n_majority_coding=first_piece[\"n_majority_coding\"],\n",
        "                n_always_coding=first_piece[\"n_always_coding\"],\n",
        "            ).rstrip()\n",
        "        )\n",
        "        \n",
        "        # Add subsequent rows with alternating colors\n",
        "        for i, stats_piece in enumerate(stats_portion[1:]):\n",
        "            color_cmd = \"\\\\cellcolor{gray!10}\" if i % 2 == 1 else \"\"\n",
        "            stats_portion_latex.append(\n",
        "                f\"    & {color_cmd}{stats_piece[value_key]} & {color_cmd}{stats_piece['n_authors']} & {color_cmd}{stats_piece['n_any_coding']} & {color_cmd}{stats_piece['n_majority_coding']} & {color_cmd}{stats_piece['n_always_coding']} \\\\\\\\\"\n",
        "            )\n",
        "\n",
        "    section_latex = \"\\n\".join(stats_portion_latex)\n",
        "    if stats_name != \"Total\":\n",
        "        section_latex += \"\\\\midrule\"\n",
        "    stats_portions_latex.append(section_latex)\n",
        "\n",
        "# Combine all parts\n",
        "stats_table_latex = f\"\"\"{table_header_start}\n",
        "{\"\\n\".join(stats_portions_latex)}\n",
        "{table_footer}\"\"\"\n",
        "\n",
        "IPython.display.Latex(stats_table_latex)"
      ],
      "id": "208faf44",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "## h-Index Linear Model Results\n"
      ],
      "id": "0be3a9f5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def compute_researcher_level_models(\n",
        "    y_col: str,\n",
        "    data: pd.DataFrame,\n",
        "    glm_family: sm.families.Family,\n",
        ") -> dict[str, sm.GLM]:\n",
        "    # Remove all \"zero\" y_col authors\n",
        "    no_outliers = data[data[y_col] > 0].copy()\n",
        "\n",
        "    # Remove outliers\n",
        "    no_outliers = no_outliers[\n",
        "        no_outliers[y_col].between(\n",
        "            no_outliers[y_col].quantile(0.03),\n",
        "            no_outliers[y_col].quantile(0.97),\n",
        "        )\n",
        "    ].copy()\n",
        "\n",
        "    # Replace names\n",
        "    no_outliers = no_outliers.rename(\n",
        "        columns={\n",
        "            \"works_count\": \"Works Count\",\n",
        "            \"any_coding\": \"Any Coding\",\n",
        "            \"majority_coding\": \"Majority Coding\",\n",
        "            \"always_coding\": \"Always Coding\",\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Replace other names (except const.) by removing `_` and title casing\n",
        "    no_outliers = no_outliers.rename(\n",
        "        columns=lambda x: x.replace(\"_\", \" \").title() if x not in [\"const\", y_col] else x\n",
        "    )\n",
        "\n",
        "    # Common features to use in all models\n",
        "    required_features = [\n",
        "        y_col,\n",
        "        \"Works Count\",\n",
        "        \"Any Coding\",\n",
        "        \"Majority Coding\",\n",
        "        \"Always Coding\",\n",
        "    ]\n",
        "\n",
        "    # Iter over different control variables and create models for each\n",
        "    models = {}\n",
        "    for control_var in [\n",
        "        \"no-control\",\n",
        "        \"Common Author Position\",\n",
        "        \"Common Article Type\",\n",
        "        \"Common Domain\",\n",
        "    ]:\n",
        "        if control_var != \"no-control\":\n",
        "            # Get control variable list\n",
        "            control_variables = [\n",
        "                col for col in no_outliers.columns if col.startswith(control_var)\n",
        "            ]\n",
        "\n",
        "            # Create control variable subset of the data\n",
        "            control_var_subset = no_outliers[required_features + control_variables].copy()\n",
        "\n",
        "            # Create interactions\n",
        "            for coding_status_col in [\"Any Coding\", \"Majority Coding\", \"Always Coding\"]:\n",
        "                for control_col in control_variables:\n",
        "                    control_var_subset[f\"{coding_status_col}  {control_col}\"] = (\n",
        "                        control_var_subset[coding_status_col]\n",
        "                        * control_var_subset[control_col]\n",
        "                    )\n",
        "        else:\n",
        "            control_var_subset = no_outliers[required_features].copy()\n",
        "\n",
        "        # Drop inf and nan\n",
        "        control_var_subset = control_var_subset.replace(\n",
        "            [float(\"inf\"), -float(\"inf\")], float(\"nan\")\n",
        "        ).dropna()\n",
        "\n",
        "        # Create x and y\n",
        "        y = control_var_subset[y_col]\n",
        "        x = control_var_subset.drop(columns=[y_col])\n",
        "        x = sm.add_constant(x)\n",
        "\n",
        "        # Fit model\n",
        "        model = sm.GLM(y, x, family=glm_family).fit()\n",
        "        models[control_var] = model\n",
        "\n",
        "    return models\n",
        "\n",
        "# Create models for h_index\n",
        "author_h_index_models = compute_researcher_level_models(\n",
        "    \"h_index\",\n",
        "    researchers_w_3_docs_and_coded_agg_dummies,\n",
        "    glm_family=sm.families.Gaussian(sm.families.links.Log()),\n",
        ")"
      ],
      "id": "187ce3b7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-format=\"html\"}"
      ],
      "id": "5322d4f2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-researcher-coding-status-no-control\n",
        "#| tbl-cap: Code-contributing authors h-index by coding status. Generalized linear model fit with Gaussian distribution and log link function.\n",
        "\n",
        "author_h_index_models[\"no-control\"].summary()"
      ],
      "id": "tbl-researcher-coding-status-no-control",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.content-visible when-format=\"pdf\"}"
      ],
      "id": "41bcef0d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_results_latex, model_results_df = convert_model_results_to_printable_pdf_ready(\n",
        "    author_h_index_models[\"no-control\"],\n",
        "    tbl_cap=(\n",
        "        \"Code-contributing authors h-index by coding status. \"\n",
        "        \"Generalized linear model fit with Gaussian distribution \"\n",
        "        \"and log link function. Significant p-values are indicated \"\n",
        "        \"with asterisks: p < 0.05 (*), p < 0.01 (**), p < 0.001 (***).\"\n",
        "    ),\n",
        "    tbl_label=\"tbl-researcher-coding-status-no-control\",\n",
        ")\n",
        "\n",
        "IPython.display.Latex(model_results_latex)"
      ],
      "id": "33d66552",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.content-visible when-format=\"html\"}"
      ],
      "id": "3cd6cd06"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-researcher-coding-status-author-position\n",
        "#| tbl-cap: Code-contributing authors h-index by coding status controlled by most freq. author position. Generalized linear model fit with Gaussian distribution and log link function.\n",
        "\n",
        "author_h_index_models[\"Common Author Position\"].summary()"
      ],
      "id": "tbl-researcher-coding-status-author-position",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.content-visible when-format=\"pdf\"}"
      ],
      "id": "32b7f2b1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_results_latex, model_results_df = convert_model_results_to_printable_pdf_ready(\n",
        "    author_h_index_models[\"Common Author Position\"],\n",
        "    tbl_cap=(\n",
        "        \"Code-contributing authors h-index by coding status \"\n",
        "        \"controlled by most freq. author position. \"\n",
        "        \"Generalized linear model fit with Gaussian distribution and log link function.\"\n",
        "        \"Significant p-values are indicated \"\n",
        "        \"with asterisks: p < 0.05 (*), p < 0.01 (**), p < 0.001 (***).\"\n",
        "    ),\n",
        "    tbl_label=\"tbl-researcher-coding-status-author-position\",\n",
        ")\n",
        "\n",
        "IPython.display.Latex(model_results_latex)"
      ],
      "id": "c927058c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.content-visible when-format=\"html\"}"
      ],
      "id": "c73aea26"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-researcher-coding-status-domain\n",
        "#| tbl-cap: Code-contributing authors h-index by coding status controlled by most freq. domain. Generalized linear model fit with Gaussian distribution and log link function.\n",
        "\n",
        "author_h_index_models[\"Common Domain\"].summary()"
      ],
      "id": "tbl-researcher-coding-status-domain",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.content-visible when-format=\"pdf\"}"
      ],
      "id": "a2469da8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_results_latex, model_results_df = convert_model_results_to_printable_pdf_ready(\n",
        "    author_h_index_models[\"Common Domain\"],\n",
        "    tbl_cap=(\n",
        "        \"Code-contributing authors h-index by coding status \"\n",
        "        \"controlled by most freq. domain. \"\n",
        "        \"Generalized linear model fit with Gaussian distribution and log link function.\"\n",
        "        \"Significant p-values are indicated \"\n",
        "        \"with asterisks: p < 0.05 (*), p < 0.01 (**), p < 0.001 (***).\"\n",
        "    ),\n",
        "    tbl_label=\"tbl-researcher-coding-status-domain\",\n",
        ")\n",
        "\n",
        "IPython.display.Latex(model_results_latex)"
      ],
      "id": "9bda69e4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.content-visible when-format=\"html\"}"
      ],
      "id": "7f51c447"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-researcher-coding-status-article-type\n",
        "#| tbl-cap: Code-contributing authors h-index by coding status controlled by most freq. article type. Generalized linear model fit with Gaussian distribution and log link function.\n",
        "\n",
        "author_h_index_models[\"Common Article Type\"].summary()"
      ],
      "id": "tbl-researcher-coding-status-article-type",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.content-visible when-format=\"pdf\"}"
      ],
      "id": "4a33b83e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_results_latex, model_results_df = convert_model_results_to_printable_pdf_ready(\n",
        "    author_h_index_models[\"Common Article Type\"],\n",
        "    tbl_cap=(\n",
        "        \"Code-contributing authors h-index by coding status \"\n",
        "        \"controlled by most freq. article type. \"\n",
        "        \"Generalized linear model fit with Gaussian distribution and log link function.\"\n",
        "        \"Significant p-values are indicated \"\n",
        "        \"with asterisks: p < 0.05 (*), p < 0.01 (**), p < 0.001 (***).\"\n",
        "    ),\n",
        "    tbl_label=\"tbl-researcher-coding-status-article-type\",\n",
        ")\n",
        "\n",
        "IPython.display.Latex(model_results_latex)"
      ],
      "id": "5f536199",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "## Analysis of Project Duration and Percentage Code-Contributors Who Are Authors\n",
        "\n",
        "In our pre-registered analysis plan (<https://osf.io/fc74m>), we originally hypothesized about the relationship between project duration and authorship recognition. Specifically, we posited that sustained technical engagement and scientific recognition might be meaningfully related, with longer project durations potentially leading to higher rates of code-contributor authorship. We saw repository histories as providing a unique opportunity to examine this relationship, leading us to hypothesize that projects with longer commit durations would be associated with higher percentages of developers receiving authorship recognition (pre-registered as H2).\n"
      ],
      "id": "1d83ab60"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "team_comp_no_push_after_pub_for_duration = team_comp_no_push_after_pub.copy()\n",
        "\n",
        "# Calculate ratio\n",
        "team_comp_no_push_after_pub_for_duration[\"pct_devs_authors\"] = (\n",
        "    team_comp_no_push_after_pub_for_duration[\"n_author_devs\"]\n",
        "    / (\n",
        "        team_comp_no_push_after_pub_for_duration[\"n_author_devs\"]\n",
        "        + team_comp_no_push_after_pub_for_duration[\"n_non_author_devs\"]\n",
        "    )\n",
        ")\n",
        "\n",
        "# Drop inf and nan\n",
        "team_comp_no_push_after_pub_for_duration = (\n",
        "    team_comp_no_push_after_pub_for_duration.replace(\n",
        "        [float(\"inf\"), -float(\"inf\")], float(\"nan\")\n",
        "    ).dropna()\n",
        ")\n",
        "\n",
        "# Remove negative \"repo_commit_duration\" repos\n",
        "team_comp_no_push_after_pub_for_duration = team_comp_no_push_after_pub_for_duration[\n",
        "    team_comp_no_push_after_pub_for_duration[\"repo_commit_duration\"] >= 0\n",
        "]\n",
        "\n",
        "# Run pearson correlation\n",
        "non_author_to_author_dev_pearson_results = pearsonr(\n",
        "    team_comp_no_push_after_pub_for_duration[\"pct_devs_authors\"],\n",
        "    team_comp_no_push_after_pub_for_duration[\"repo_commit_duration\"],\n",
        ")\n",
        "\n",
        "pearson_rho = f\"{non_author_to_author_dev_pearson_results[0]:.2f}\""
      ],
      "id": "5ed1cac5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, our analysis found no evidence to support this hypothesis. When examining the relationship between a repository's commit duration and the percentage of developers who receive authorship recognition, we found no significant correlation (r = `{python} pearson_rho`, p = n.s.). This suggests that the length of time a project has been in development has no meaningful relationship with the proportion of developers who are recognized as authors.\n",
        "\n",
        "We ultimately decided to move this analysis to the appendix for two key methodological reasons. First, our approach of using repository-level commit duration as a proxy for individual contribution patterns proved too coarse-grained. A more precise analysis would need to examine individual-level contribution durations and patterns rather than overall project length. Second, our method did not account for the varying levels of contribution that different developers make to a repository. Simply correlating overall project duration with authorship rates fails to capture the nuanced ways that sustained, meaningful technical contributions might influence authorship decisions.\n",
        "\n",
        "These limitations suggest potential directions for future work that could more rigorously examine the relationship between long-term technical engagement and scientific recognition. Such work might benefit from more granular analysis of individual contribution patterns, perhaps incorporating measures of contribution significance and sustainability rather than just temporal duration."
      ],
      "id": "ce1ee4d6"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/nmweber/Library/Python/3.11/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}