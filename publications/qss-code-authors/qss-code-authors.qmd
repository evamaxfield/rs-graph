---
title: "Code Contribution and Authorship"
author:
  - name: "Eva Maxfield Brown"
    email: evamxb@uw.edu
    orcid: 0000-0003-2564-0373
    affliation:
      name: University of Washington Information School
      city: Seattle
      state: Washington
      country: USA
  - name: "Nicholas Weber"
    email: nmweber@uw.edu
    orcid: 0000-0002-6008-3763
    affliation:
      name: University of Washington Information School
      city: Seattle
      state: Washington
      country: USA

abstract: |
  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur eget porta erat. Morbi consectetur est vel gravida pretium. Suspendisse ut dui eu ante cursus gravida non sed sem. Nullam sapien tellus, commodo id velit id, eleifend volutpat quam. Phasellus mauris velit, dapibus finibus elementum vel, pulvinar non tellus. Nunc pellentesque pretium diam, quis maximus dolor faucibus id. Nunc convallis sodales ante, ut ullamcorper est egestas vitae. Nam sit amet enim ultrices, ultrices elit pulvinar, volutpat risus.

## Basics
bibliography: main.bib

## Number sections (required for section cross ref)
number-sections: true

## Citation Style Language
# See https://github.com/citation-style-language/styles for more options
# We default to PNAS (Proceedings of the National Academy of Sciences)
# csl: support/acm-proceedings.csl

## Specific for target format
format:
  html:
    code-tools: true
    code-fold: true
    code-summary: "Show the code"
    standalone: true
    embed-resources: true
    toc: true
    toc-location: left
    reference-location: margin
    citation-location: margin

  pdf:
    toc: false
    execute:
      echo: false
    include-in-header:  
      - text: |
          \usepackage{booktabs}    % For professional-looking tables
          \usepackage{multirow}    % For merged cells
          \usepackage{siunitx}     % For number alignment
          \usepackage[table]{xcolor}  % The 'table' option is crucial for \rowcolors to work

---

# Introduction

```{python}
from datetime import datetime
from io import StringIO
import os
from pathlib import Path

import colormaps as cmaps
import IPython.display
import matplotlib.pyplot as plt
import numpy as np
import opinionated  # noqa
import pandas as pd
import seaborn as sns
import statsmodels.api as sm
from sci_soft_models.dev_author_em import data as sci_soft_models_data
from scipy.stats import (
    binomtest,
    chi2_contingency,
    false_discovery_control,
    pearsonr,
)
from sqlalchemy import text

from rs_graph.db import models as db_models
from rs_graph.db import utils as db_utils

# Get db engine for production database
db_conn = db_utils.get_engine(use_prod=True)

# Set seaborn style
plt.style.use("opinionated_rc")
sns.set_palette(
    cmaps.bold[2:]._colors.tolist(),
)

# Get or set USE_SAMPLE
if "QSS_CODE_AUTHORSHIP_USE_SAMPLE" in os.environ:
    USE_SAMPLE = bool(int(os.environ["QSS_CODE_AUTHORSHIP_USE_SAMPLE"]))
else:
    USE_SAMPLE = True
```

- Contemporary scientific research fundamentally depends on specialized software tools and computational methods.
    - define scientific software (analysis scripts, research tools, computational infrastructure)
    - software enables reproducible research and large-scale experiments
    - code serves as a detailed log of research methodology
    - due to all of the above, code is increasingly being shared alongside research articles

- The development and maintenance of scientific software requires substantial contribution, yet faces persistent challenges in receiving academic recognition.
    - software contributions often receive only acknowledgments rather than authorship
    - lack of formal credit affects career advancement in academia
    - other general discussion of software citations and credit systems

- Recent initiatives to expand academic credit systems, while promising, have not fully addressed the challenges of recognizing software contributions.
    - describe the Contributor Roles Taxonomy (CRediT)
    - previous research using CRediT to understand research labor distribution
    - CRediT research is still centered on traditional author lists
    - problem of self-reporting without verification

- Our novel predictive model addresses these challenges by enabling systematic matching between scientific article authors and source code developer accounts.
    - we use predictive modeling due to the lack of standardized identifiers (i.e. ORCID) for developers
    - further, lack consistency in naming and email overlap
    - semantic models handle subtle variations in identity information (general entity matching has moved to transformers and semantic embeddings)

- By applying our model across a corpus of XYZ paired research articles and repositories, we provide unique insight into the dynamics of code contribution within research teams, the impact of code contribution on research outcomes, and an understanding of the authors who are and who aren’t code contributors.
    - move from self-reporting to verifiable source code repository commit histories
    - provide preliminary quantitative evidence of exclusion of code contributors from academic authorship
    - model article level impact metrics as a function of software development dynamics to show the benefit code contributors have on research
    - find that first authors are more likely to be code contributors than not
    - find that code-contributing authors have reduced individual level impact metrics compared to their non-coding counterparts

- These findings not only illuminate the relationship between code contribution and scientific impact but also provide an empirical foundation for reforming academic credit systems to better recognize software development contributions in research.

# Background

- The relationship between scientific software development and academic credit systems represents a complex intersection of traditional academic practices and modern research requirements.
    - academic credit traditionally focuses on analytical, theoretical, and experimental contributions
    - software development historically viewed as technical rather than scholarly work
    - growing recognition that research software development requires deep domain expertise
    - increased emphasis on large scale (big data) projects has resulted in larger need for software development
    - understanding this relationship requires examining both team-level dynamics and individual contributions

- (H1) Modern research increasingly depends on collaborative software development, yet we lack systematic evidence of how code contribution patterns affect research outcomes.
    - existing research focuses primarily on general team size and diversity
    - software engineering literature shows correlation between team size and code quality (many eyes make all bugs shallow)
    - limited understanding of how code contribution is associated to research impact
    - need to understand relationship between code contributors and citation metrics to understand the value of these technical, potentially uncredited, contributions
    - we believe that more code contributors may signal a more technical research project and that technical complexity may be rewarded with more citations

- (H2) Despite formal taxonomies like CRediT attempting to standardize contribution recognition, the criteria for granting authorship to technical contributors remain inconsistent and poorly understood across research communities.
    - existing contribution frameworks provide definitions for software development roles
    - however, these frameworks may not capture the full spectrum of technical contributions
    - repository histories allow us to examine how sustained technical engagement relates to authorship status
    - we believe that longer project involvement increases likelihood of authorship recognition
    - specifically, we hypothesize that projects with longer durations will show higher proportions of author-developers compared to non-author developers

- (H3 and H4) Academic authorship conventions signal both intellectual contribution and project responsibilities, yet their relationship to software development remains poorly understood.
    - first authors traditionally responsible for primary intellectual and experimental contributions
    - corresponding authors serve as primary points of contact and often maintain research artifacts
    - varying expectations across academic disciplines regarding technical contributions
    - limited research examining how these authorship roles relate to code contributions
    - potential insights into how software development responsibilities are distributed within research teams
    - we believe that first authors and corresponding authors will have higher proportions of code contribution than not.
    - conversely, middle and last authors and non-corresponding authors will have lower proportions of code contribution than not.

- (H5) Academic career advancement has historically depended on traditional impact metrics, creating potential tension for researchers who dedicate significant time to software development.
    - time invested in code development may reduce traditional scholarly output
    - potential career implications for researchers who prioritize coding
    - need to understand relationship between code contributions and academic impact
    - we believe that code contributing researchers will have lower individual level impact metrics than non-coding researchers, likely due to a combination of lack of recognition and authorship credit as well as reduced time for traditional scholarly output

- Understanding these relationships is crucial for developing equitable academic credit systems that recognize the full spectrum of research contributions.
    - findings will inform policy making around academic credit
    - importance of large-scale quantitative evidence for understanding current credit systems
    - implications for academic hiring and promotion decisions
    - potential to develop new impact metrics that capture software contributions

# Data and Methods

## Linking Scientific Articles and Source Code Repositories

```{python}
def read_table(table: str) -> pd.DataFrame:
    return pd.read_sql(text(f"SELECT * FROM {table}"), db_conn)


# Read all data from database
doc_repo_links = read_table(db_models.DocumentRepositoryLink.__tablename__)
researchers = read_table(db_models.Researcher.__tablename__)
devs = read_table(db_models.DeveloperAccount.__tablename__)
documents = read_table(db_models.Document.__tablename__)
document_contributors = read_table(db_models.DocumentContributor.__tablename__)
repositories = read_table(db_models.Repository.__tablename__)
repository_contributors = read_table(db_models.RepositoryContributor.__tablename__)
topics = read_table(db_models.Topic.__tablename__)
document_topics = read_table(db_models.DocumentTopic.__tablename__)
dataset_sources = read_table(db_models.DatasetSource.__tablename__)
researcher_dev_links = read_table(
    db_models.ResearcherDeveloperAccountLink.__tablename__
)

# Drop all "updated_datetime" and "created_datetime" columns
for df in [
    doc_repo_links,
    researchers,
    devs,
    documents,
    document_contributors,
    repositories,
    repository_contributors,
    topics,
    document_topics,
    dataset_sources,
    researcher_dev_links,
]:
    df.drop(columns=["updated_datetime", "created_datetime"], inplace=True)

# Specifically drop doc_repo_links "id" column
# It isn't used and will get in the way later when we do a lot of joins
doc_repo_links.drop(columns=["id"], inplace=True)

# Construct reduced doc_repo_links
original_doc_repo_links_len = len(doc_repo_links)
doc_repo_links = doc_repo_links.drop_duplicates(subset=["document_id"], keep=False)
doc_repo_links = doc_repo_links.drop_duplicates(subset=["repository_id"], keep=False)

# Reduce other tables to only documents / repositories in the updated doc_repo_links
documents = documents[documents["id"].isin(doc_repo_links["document_id"])]
repositories = repositories[repositories["id"].isin(doc_repo_links["repository_id"])]
document_contributors = document_contributors[
    document_contributors["document_id"].isin(documents["id"])
]
repository_contributors = repository_contributors[
    repository_contributors["repository_id"].isin(repositories["id"])
]
document_topics = document_topics[document_topics["document_id"].isin(documents["id"])]

# Reduce researchers and devs to only those in the
# updated document_contributors and repository_contributors
researchers = researchers[
    researchers["id"].isin(document_contributors["researcher_id"])
]
devs = devs[devs["id"].isin(repository_contributors["developer_account_id"])]
researcher_dev_links = researcher_dev_links[
    (
        researcher_dev_links["researcher_id"].isin(researchers["id"])
        & researcher_dev_links["developer_account_id"].isin(devs["id"])
    )
]

# Sort document topics and keep first
document_topics = document_topics.sort_values("score", ascending=False)
document_topics = document_topics.drop_duplicates(subset=["document_id"], keep="first")

# Create document, document topic merged table
merged_document_topics = pd.merge(
    document_topics, topics, left_on="topic_id", right_on="id"
)

# Create basic merged tables
merged_document_contributor_doc_repo_links = pd.merge(
    document_contributors, doc_repo_links, left_on="document_id", right_on="document_id"
)
merged_repository_contributor_doc_repo_links = pd.merge(
    repository_contributors,
    doc_repo_links,
    left_on="repository_id",
    right_on="repository_id",
)

# Compute stats for data sources
data_source_stats = []
for _, data_source in dataset_sources.iterrows():
    # Get total article-repo pairs
    data_source_stats.append(
        {
            "data_source": data_source["name"],
            "n_article_repo_pairs": len(
                doc_repo_links[doc_repo_links["dataset_source_id"] == data_source["id"]]
            ),
            "n_authors": merged_document_contributor_doc_repo_links.loc[
                merged_document_contributor_doc_repo_links["dataset_source_id"]
                == data_source["id"]
            ]["researcher_id"].nunique(),
            "n_devs": merged_repository_contributor_doc_repo_links.loc[
                merged_repository_contributor_doc_repo_links["dataset_source_id"]
                == data_source["id"]
            ]["developer_account_id"].nunique(),
        }
    )

# Create topic merged tables
merged_doc_repo_links_topics = pd.merge(
    doc_repo_links, document_topics, left_on="document_id", right_on="document_id"
).merge(topics, left_on="topic_id", right_on="id")
merged_doc_repo_links_topics_document_contributors = pd.merge(
    merged_doc_repo_links_topics,
    document_contributors,
    left_on="document_id",
    right_on="document_id",
)
merged_doc_repo_links_topics_repository_contributors = pd.merge(
    merged_doc_repo_links_topics,
    repository_contributors,
    left_on="repository_id",
    right_on="repository_id",
)

# Compute stats for domains
domain_stats = []
for domain in merged_doc_repo_links_topics.domain_name.unique():
    # Get total article-repo pairs
    domain_stats.append(
        {
            "domain": domain,
            "n_article_repo_pairs": len(
                merged_doc_repo_links_topics[
                    merged_doc_repo_links_topics["domain_name"] == domain
                ]
            ),
            "n_authors": merged_doc_repo_links_topics_document_contributors.loc[
                merged_doc_repo_links_topics_document_contributors["domain_name"]
                == domain
            ]["researcher_id"].nunique(),
            "n_devs": merged_doc_repo_links_topics_repository_contributors.loc[
                merged_doc_repo_links_topics_repository_contributors["domain_name"]
                == domain
            ]["developer_account_id"].nunique(),
        }
    )

# Create document merged tables
merged_doc_repo_links_documents = pd.merge(
    doc_repo_links, documents, left_on="document_id", right_on="id"
)
merged_doc_repo_links_documents_document_contributors = pd.merge(
    merged_doc_repo_links_documents,
    document_contributors,
    left_on="document_id",
    right_on="document_id",
)
merged_doc_repo_links_documents_repository_contributors = pd.merge(
    merged_doc_repo_links_documents,
    repository_contributors,
    left_on="repository_id",
    right_on="repository_id",
)

# Compute stats for document types
# This isn't a standard data pull
# In short:
# - pairs from PLOS are "research articles"
# - pairs from JOSS are "software articles"
# - pairs from SoftwareX are "software articles"
# - pairs from Papers with Code / ArXiv are "pre-prints"
#   UNLESS they have been published in a journal
# All of those should be easy to assert / apply a label to with the exception
# of Papers with Code / ArXiv pre-prints that have been published in a journal
# In that case, we need to look at the existing document type in the database
# If the document type is "preprint" use preprint, otherwise, if it's anything else,
# use "research article"

# Create a "reduced_doc_types" dataframe with document_id and "reduced_doc_type"
# columns
reduced_doc_types_rows = []
# We can use the "reduced_doc_types" dataframe to calculate the stats

# Iter over data sources even though we are looking for doc types
for _, data_source in dataset_sources.iterrows():
    # Get total article-repo pairs
    doc_type = None
    if data_source["name"] in ["plos", "joss", "softwarex"]:
        if data_source["name"] == "plos":
            doc_type = "research article"
        else:
            doc_type = "software article"

        # Add all document_ids to reduced_doc_types_rows
        reduced_doc_types_rows.extend(
            [
                {"document_id": doc_id, "reduced_doc_type": doc_type}
                for doc_id in doc_repo_links[
                    (doc_repo_links["dataset_source_id"] == data_source["id"])
                ]["document_id"]
            ]
        )

    # Handle PwC
    else:
        # Get preprint pairs
        preprint_pairs = merged_doc_repo_links_documents[
            (merged_doc_repo_links_documents["dataset_source_id"] == data_source["id"])
            & (merged_doc_repo_links_documents["document_type"] == "preprint")
        ]

        # Add all document_ids to reduced_doc_types_rows
        reduced_doc_types_rows.extend(
            [
                {"document_id": doc_id, "reduced_doc_type": "preprint"}
                for doc_id in preprint_pairs["document_id"]
            ]
        )

        # Get research article pairs
        # This is the same just inverted to != "preprint"
        research_article_pairs = merged_doc_repo_links_documents[
            (merged_doc_repo_links_documents["dataset_source_id"] == data_source["id"])
            & (merged_doc_repo_links_documents["document_type"] != "preprint")
        ]

        # Add all document_ids to reduced_doc_types_rows
        reduced_doc_types_rows.extend(
            [
                {"document_id": doc_id, "reduced_doc_type": "research article"}
                for doc_id in research_article_pairs["document_id"]
            ]
        )

# Create reduced_doc_types dataframe
reduced_doc_types = pd.DataFrame(reduced_doc_types_rows)
```

- Modern scientific research increasingly requires the public sharing of research code, creating unique opportunities to study the relationship between academic authorship and software development.
    - many journals and platforms now require or recommend code and data sharing
    - this requirement creates traceable links between publications and code
    - these links enable systematic study of both article-repository and author-developer relationships

- Our data collection process leverages multiple complementary sources of linked scientific articles and code repositories to ensure comprehensive coverage.
    - PLOS: Traditional research articles with code requirements
    - JOSS and SoftwareX: Specialized software-focused publications
    - Papers with Code / ArXiv: Capturing pre-print landscape
    - to reduce the complexity of dataset processing and enrichment, we filter out any article-source-code-repository pairs which store code somewhere other than GitHub

- Through integration of multiple data sources, we extract detailed information about both the academic and software development aspects of each project.
    - specifically we utilize the Semantic Scholar API for article DOI resolution to ensure that we find the latest version for each article.
    - this is particularly important for working with preprints as they may have been published in a journal since their inclusion in the Papers with Code dataset
    - we then utilize the OpenAlex API to gather publication metadata (i.e. open access status, domain, publication date), author details (i.e. name, author position, corresponding author status), and article- and individual-level metrics (i.e. citation count, FWCI, h-index).
    - the GitHub API provides similar information for source code repositories, including repository metadata (i.e. name, description, languages, creation date), contributor details (i.e. username, name, email), and repository-level metrics (i.e. star count, fork count, issue count).

- while the majority of our data is sourced from Papers with Code, our additional collection from PLOS, JOSS, and SoftwareX as well as the enrichment from GitHub and OpenAlex together form one of the largest collections of linked, metadata enriched, datasets of paired scientific articles and associated source code repositories.
    - in total, we collect and enrich data for `{python} original_doc_repo_links_len` article-repository pairs

## A Predictive Model for Matching Article Authors and Source Code Contributors

### Annotated Dataset Creation

```{python}
# Load annotated dataset
annotated_dataset = sci_soft_models_data.load_annotated_dev_author_em_dataset()

# Get counts of positive and negative examples
n_positive_examples = len(annotated_dataset.loc[annotated_dataset["match"]])
n_negative_examples = len(annotated_dataset.loc[~annotated_dataset["match"]])
pct_positive_examples = round(n_positive_examples / len(annotated_dataset), 3) * 100
pct_negative_examples = round(n_negative_examples / len(annotated_dataset), 3) * 100

# Create function to split row details
def split_dev_details(dev_details: str) -> dict:
    details_list = dev_details.split(";\n")
    details_dict = {}
    for detail in details_list:
        try:
            key, value = detail.split(": ", 1)
        except Exception:
            print(detail)
            raise

        # Only keep username, name, and email
        if key in ["username", "name", "email"]:
            # If value is "None" replace with None
            if value == "None":
                value = None

            # Store to details dict
            details_dict[key] = value
    
    return details_dict

# Split dev details
all_dev_details = annotated_dataset["dev_details"].apply(split_dev_details)

# Convert to dataframe
all_dev_details_df = pd.DataFrame(all_dev_details.tolist())

# Drop duplicates on username
all_dev_details_df = all_dev_details_df.drop_duplicates(subset=["username"])

# Get counts
n_devs = len(all_dev_details_df)
n_devs_with_name = all_dev_details_df["name"].notnull().sum()
n_devs_with_email = all_dev_details_df["email"].notnull().sum()

pct_devs_with_name = round(n_devs_with_name / n_devs, 3) * 100
pct_devs_with_email = round(n_devs_with_email / n_devs, 3) * 100

# Get unique authors from original frame from unique semantic scholar id
n_authors = annotated_dataset["semantic_scholar_id"].nunique()
```

- The development of an accurate author-developer matching model requires high-quality labeled training data that captures the complexity of real-world identity matching.
    - entity matching between authors and developers is non-trivial
    - multiple forms of name variation and incomplete information
    - need to expand on specific matching challenges
    - add figure showing example matches/non-matches

- We developed an annotation process to create a robust training dataset while maximizing efficiency and accuracy.
    - focus on JOSS articles to increase positive match density
    - we create author-developer pairs for annotation by creating all possible combinations of authors and developers within a single JOSS article-repository pair
    - we take a random sample of 3000 pairs from the full set and have two independent annotators label each
    - structured conflict resolution process
    - need to add details about annotation guidelines/criteria

- The resulting annotated dataset provides a comprehensive foundation for training our predictive model while highlighting common patterns in author-developer identity matching.
    - after resolution of all annotated pairs, our annotated dataset contains `{python} n_positive_examples` (`{python} pct_positive_examples`%) positive and `{python} n_negative_examples` (`{python} pct_negative_examples`%) negative author-developer-account pairs
    - there are `{python} n_authors` unique authors and `{python} n_devs` unique developer accounts within this annotated set
    - however, not all developer accounts contain complete information, in our set `{python} n_devs_with_name` (`{python} pct_devs_with_name`%) have associated names and `{python} n_devs_with_email` (`{python} pct_devs_with_email`%) have associated emails

### Training and Evaluation

```{python}
training_split_counts = sci_soft_models_data.load_final_model_training_split_details()

# Take sum on rows
training_split_counts = training_split_counts.set_index("split").sum(axis=1)
training_split_pct = ((training_split_counts / training_split_counts.sum()) * 100).round(1)
```

- To optimize our predictive model for author-contributor matching, we evaluate a variety of Transformer-based base models and input features.
    - multiple transformer base models available
    - various potential feature combinations

- We employed a systematic evaluation to identify optimal combination of base models and input features.
    - first, to ensure that there was no data leakage, we split our dataset into training and test sets
    - specifically, we created two random sets of 10% of all unique authors and 10% of all unique developers, any pairs containing either the author or developer were placed into the test set
    - in doing so, we ensured that the model was never trained on any author or developer information later used for evaluation
    - due to the fact that each author and developer-account can be included in multiple annotated pairs, our final training set contains `{python} training_split_counts["train"]` (`{python} training_split_pct["train"]`%) and our test set contains `{python} training_split_counts["test"]` (`{python} training_split_pct["test"]`%) author-developer-account pairs
    - we used three different transformer models as our fine-tuning bases and fine-tuned each using all combinations of avaiable developer-account features, from including only the developer account username to including the developer's username, name, and email.
    - to avoid overfitting and ensure generalizability, we fine-tuned each of the base models for only a single training epoch.
    - model evaluation was performed using standard classification metrics, including accuracy, precision, recall, and F1 score

```{python}
exp_results = sci_soft_models_data.load_experimental_training_results()

# Remove the "time_pred" and "epoch_val" columns
exp_results = exp_results.drop(columns=["time_pred", "epoch_val"])

# Round accuracy, precision, recall, and f1 to 3 decimal places
exp_results = exp_results.round(
    {
        "accuracy": 3,
        "precision": 3,
        "recall": 3,
        "f1": 3,
    }
)

# Replace "no-optional-data" with "n/a"
exp_results["fieldset"] = exp_results["fieldset"].replace(
    "no-optional-data", "n/a"
)

# Str replace references to "dev_" in "fieldset" column
exp_results["fieldset"] = exp_results["fieldset"].str.replace("dev_", "")

# Str replace "_" with " " in "fieldset" column
exp_results["fieldset"] = exp_results["fieldset"].str.replace("_", " ")

# Str split fieldset values on "-" and rejoin with ", "
exp_results["fieldset"] = exp_results["fieldset"].str.split("-").apply(
    lambda x: ", ".join(x)
)

# Rename fieldset column to "Optional Features"
exp_results = exp_results.rename(columns={"fieldset": "Optional Feats."})

# Capitalize column names
exp_results.columns = exp_results.columns.str.title()

# Get best model f1 and get the accuracy, precision, recall, and f1 for that model
best_model_idx = exp_results["F1"].idxmax()
best_model_acc = exp_results.loc[best_model_idx, "Accuracy"]
best_model_prec = exp_results.loc[best_model_idx, "Precision"]
best_model_rec = exp_results.loc[best_model_idx, "Recall"]
best_model_f1 = exp_results.loc[best_model_idx, "F1"]
```

- After extensive model comparison we find that fine-tuning from [Microsoft's deberta-v3-base](https://huggingface.co/microsoft/deberta-v3-base) and including only the developer's username and name achieves the best performance for author-developer matching.
    - our best model achieves a binary F1 score of `{python} best_model_f1`, with an accuracy of `{python} best_model_acc`, precision of `{python} best_model_prec`, and recall of `{python} best_model_rec` (see @fig-em-model-confusion-matrix for a confusion matrix of model predictions on the test set).
    - analysis of feature importance
        - note that the addition of developer's name has a "larger effect" on model performance but that could simply be because of how many more developers have a name available than an email
        - also note that there is a model that performs just as well as this one using bert-multilingual and includes the developers email however we choose to use the deberta and name only version for its simplicity as well as the fact that deberta is a much more recently developed and released model which was pre-trained on a much larger dataset (including multilingual data).
        - considering that in most cases, deberta out-performs bert-multilingual, we believe that while the overall evaluation metrics between the top two performing models are the same, the deberta based model will generalize to other unseen data better than the bert-multilingual model
    - all model and feature set combination results are available in @tbl-em-model-comparison

```{python}
#| label: fig-em-model-confusion-matrix
#| fig-cap: "Confusion Matrix Produced From Evaluation of Best Performing Model (deberta-v3 with developer username, developer name, and author name)."

IPython.display.Image(sci_soft_models_data.FINAL_MODEL_TRAINING_DATA_DIR / "dev-author-em-confusion-matrix.png")
```

- To enable future research, we have made our trained model and supporting application library publicly available.
    - Python library implementation
    - HuggingFace model deployment

## Linking Authors and GitHub Developer Accounts

```{python}
# Compute stats
doc_type_stats = reduced_doc_types.groupby("reduced_doc_type").apply(
    lambda x: {
        "doc_type": x.name,
        "n_article_repo_pairs": len(x),
        "n_authors": merged_doc_repo_links_documents_document_contributors.loc[
            merged_doc_repo_links_documents_document_contributors["document_id"].isin(
                x["document_id"]
            )
        ]["researcher_id"].nunique(),
        "n_devs": merged_doc_repo_links_documents_repository_contributors.loc[
            merged_doc_repo_links_documents_repository_contributors["document_id"].isin(
                x["document_id"]
            )
        ]["developer_account_id"].nunique(),
    },
    include_groups=False,
)

# Compute stats for access status
access_stats = []
for access_status_int, access_status_name in [
    (0, "Closed"),
    (1, "Open"),
]:
    # Get total article-repo pairs
    access_stats.append(
        {
            "access_status": access_status_name,
            "n_article_repo_pairs": len(
                merged_doc_repo_links_documents[
                    merged_doc_repo_links_documents["is_open_access"]
                    == access_status_int
                ]
            ),
            "n_authors": merged_doc_repo_links_documents_document_contributors.loc[
                merged_doc_repo_links_documents_document_contributors["is_open_access"]
                == access_status_int
            ]["researcher_id"].nunique(),
            "n_devs": merged_doc_repo_links_documents_repository_contributors.loc[
                merged_doc_repo_links_documents_repository_contributors[
                    "is_open_access"
                ]
                == access_status_int
            ]["developer_account_id"].nunique(),
        }
    )

# Compute totals
total_article_repo_pairs = len(doc_repo_links)
total_authors = merged_document_contributor_doc_repo_links["researcher_id"].nunique()
total_devs = merged_repository_contributor_doc_repo_links[
    "developer_account_id"
].nunique()
```

- Our trained entity-matching model enables comprehensive identification of author-developer relationships while accounting for the complex realities of academic software development practices.
    - model applied to all possible author and developer-account combinations within each article-repository pair
    - recognition that one-to-one matching may not reflect reality
    - deliberate choice to allow many-to-many relationships in matching

- The presence of multiple developer accounts per individual reflects common practices in academic software development that must be accommodated in our analysis.
    - developers often maintain separate accounts for different projects or institutions
    - account transitions are common as researchers move between roles
    - CHAOSS project provides precedent for this type of identity resolution

- Further, while our model performs well overall, we note that there are some limitations to our approach.
    - in most cases predictions are trivial due to minor differences in text (spelling of author name to username)
    - however we do observe a few cases in which our model may not perform as well
    - namely, shorter names, articles and repositories which have contributors with the same last name (i.e. siblings or other relationship), and "organization" accounts (i.e. research lab GitHub accounts used for management, administration, and documentation or a project)
    - TODO: should we take a sample and estimate how widespread these problems are?
    - we include appropriate filtering during analysis to ensure that we do not include author-developer pairs which are unlikely to be the same individual

- Our final dataset provides unprecedented scale and scope for analyzing the relationship between academic authorship and software development contributions.
    - Specifically, our dataset contains `{python} total_article_repo_pairs` article-repository pairs, `{python} total_authors` distinct authors, and `{python} total_devs` distinct developer accounts.
    - a detailed breakdown of these counts by data source, domain, document type, and open access status is available in @tbl-rs-graph-overall-counts

::: {.content-visible when-format="html"}

```{python}
#| label: tbl-rs-graph-overall-counts
#| tbl-cap: "Counts of Article-Repository Pairs, Authors, and Developers broken out by Data Sources, Domains, Document Types, and Access Status."

# Construct multi-row span HTML table
# Columns should be: "n_article_repo_pairs", "n_authors", "n_devs"
# Rows should be:
# "By Data Source", "By Domain", "By Document Type", "By Access Status", and "Total"

# HTML templates
stats_piece_inital_row_template = """
<tr>
  <td rowspan="{n_rows}">{row_name}</td>
  <td>{value_name}</td>
  <td>{article_repo_pairs}</td>
  <td>{authors}</td>
  <td>{devs}</td>
</tr>
""".strip()

stats_piece_subsequent_row_template = """
<tr>
  <td>{value_name}</td>
  <td>{article_repo_pairs}</td>
  <td>{authors}</td>
  <td>{devs}</td>
</tr>
""".strip()

# Iter over stats portions (and total)
stats_portions_html = []
for stats_portion, stats_name, value_key in [
    (domain_stats, "<b>By Domain</b>", "domain"),
    (doc_type_stats, "<b>By Document Type</b>", "doc_type"),
    (access_stats, "<b>By Access Status</b>", "access_status"),
    (data_source_stats, "<b>By Data Source</b>", "data_source"),
    (
        [
            {
                "empty": "",
                "n_article_repo_pairs": f"<b>{total_article_repo_pairs}</b>",
                "n_authors": f"<b>{total_authors}</b>",
                "n_devs": f"<b>{total_devs}</b>",
            }
        ],
        "<b>Total</b>",
        "empty",
    ),
]:
    # Order by article-repo pairs
    stats_portion = sorted(
        stats_portion, key=lambda x: x["n_article_repo_pairs"], reverse=True
    )

    stats_portion_html = []
    for i, stats_piece in enumerate(stats_portion):
        if i == 0:
            stats_portion_html.append(
                stats_piece_inital_row_template.format(
                    n_rows=len(stats_portion),
                    row_name=stats_name,
                    value_name=stats_piece[value_key],
                    article_repo_pairs=stats_piece["n_article_repo_pairs"],
                    authors=stats_piece["n_authors"],
                    devs=stats_piece["n_devs"],
                )
            )
        else:
            stats_portion_html.append(
                stats_piece_subsequent_row_template.format(
                    value_name=stats_piece[value_key],
                    article_repo_pairs=stats_piece["n_article_repo_pairs"],
                    authors=stats_piece["n_authors"],
                    devs=stats_piece["n_devs"],
                )
            )

    stats_portions_html.append("\n".join(stats_portion_html))

# Concat and wrap in table
stats_table_html = f"""
<table>
  <tr>
    <th><b>Category</b></th>
    <th><b>Subset</b></th>
    <th><b>Article-Repository Pairs</b></th>
    <th><b>Authors</b></th>
    <th><b>Developers</b></th>
  </tr>
  {" ".join(stats_portions_html)}
</table>
""".strip()

IPython.display.HTML(stats_table_html)
```

:::

::: {.content-visible when-format="pdf"}

```{python}
# LaTeX templates
stats_piece_initial_row_template = """    \\multirow{{{n_rows}}}{{*}}{{\\textbf{{{row_name}}}}} & \\cellcolor{{gray!10}}{value_name} & \\cellcolor{{gray!10}}{article_repo_pairs} & \\cellcolor{{gray!10}}{authors} & \\cellcolor{{gray!10}}{devs} \\\\"""

stats_piece_subsequent_row_template = """    & {value_name} & {article_repo_pairs} & {authors} & {devs} \\\\"""

# Table header and footer templates
table_header = """\\begin{table}
\\centering
\\small
\\caption{Counts of Article-Repository Pairs, Authors, and Developers broken out by Data Sources, Domains, Document Types, and Access Status.}
\\label{tbl-rs-graph-overall-counts}
\\begin{tabular}{llrrr}
\\toprule
\\textbf{Category} & \\textbf{Subset} & \\textbf{Article-Repository Pairs} & \\textbf{Authors} & \\textbf{Developers} \\\\
\\midrule"""

table_footer = """\\bottomrule
\\end{tabular}
\\end{table}"""

# Generate table content
stats_portions_latex = []
for stats_portion, stats_name, value_key in [
    (domain_stats, "By Domain", "domain"),
    (doc_type_stats, "By Document Type", "doc_type"),
    (access_stats, "By Access Status", "access_status"),
    (data_source_stats, "By Data Source", "data_source"),
    (
        [
            {
                "empty": "",
                "n_article_repo_pairs": total_article_repo_pairs,
                "n_authors": total_authors,
                "n_devs": total_devs,
            }
        ],
        "Total",
        "empty",
    ),
]:
    # Order by article-repo pairs
    stats_portion = sorted(
        stats_portion, key=lambda x: x["n_article_repo_pairs"], reverse=True
    )

    stats_portion_latex = []
    if stats_name == "Total":
        stats_portion_latex.append(
            f"    \\textbf{{Total}} & & \\textbf{{{stats_portion[0]['n_article_repo_pairs']}}} & \\textbf{{{stats_portion[0]['n_authors']}}} & \\textbf{{{stats_portion[0]['n_devs']}}} \\\\"
        )
    else:
        # Add the first row with the category label and colored cells
        first_piece = stats_portion[0]
        stats_portion_latex.append(
            stats_piece_initial_row_template.format(
                n_rows=len(stats_portion),
                row_name=stats_name,
                value_name=first_piece[value_key],
                article_repo_pairs=first_piece["n_article_repo_pairs"],
                authors=first_piece["n_authors"],
                devs=first_piece["n_devs"],
            ).rstrip()
        )
        
        # Add subsequent rows with alternating colors
        for i, stats_piece in enumerate(stats_portion[1:]):
            color_cmd = "\\cellcolor{gray!10}" if i % 2 == 1 else ""
            stats_portion_latex.append(
                f"    & {color_cmd}{stats_piece[value_key]} & {color_cmd}{stats_piece['n_article_repo_pairs']} & {color_cmd}{stats_piece['n_authors']} & {color_cmd}{stats_piece['n_devs']} \\\\"
            )

    section_latex = "\n".join(stats_portion_latex)
    if stats_name != "Total":
        section_latex += "\\midrule"
    stats_portions_latex.append(section_latex)

# Combine all parts
stats_table_latex = f"""{table_header}
{"\n".join(stats_portions_latex)}
{table_footer}"""

IPython.display.Latex(stats_table_latex)
```

:::

```{python}
#| label: fig-docs-per-year
#| fig-cap: "Number of articles by publication year. Only publication years with 100 or more articles are included."

# Create a subset of the documents table with only document_id and publication_date
documents_pub_year = documents[["id", "publication_date"]].copy()
documents_pub_year["publication_year"] = pd.to_datetime(
    documents_pub_year["publication_date"],
    utc=True,
).dt.year

# Remove any documents with pub years that have less than 100 documents
year_counts = documents_pub_year["publication_year"].value_counts()
documents_pub_year = documents_pub_year[
    documents_pub_year["publication_year"].isin(
        year_counts[year_counts >= 100].index
    )
]

# Plot
ax = sns.countplot(
    data=documents_pub_year,
    x="publication_year",
)

# Rotate x-labels
_ = plt.xticks(rotation=45, ha="right")
```

# Preliminary Analysis Code Contributor Authorship and Development Dynamics of Research Teams

- To enrich our pre-existing dataset, we apply our trained predictive model across pairs of authors and developer accounts.
	- again, these pairs are all combinations of author and developer account within an individual paper
	- specifics, how many unique author-developer account pairs are we able to find
	- table of author-developer account pairs for by data source / by field
	- we next use this enriched dataset to understand software development dynamics within research teams, and characterize the authors who are and who aren’t code contributors.

## Software Development Dynamics Within Research Teams

- We begin by measuring the distributions of different coding and non-coding contributors across all of the article-code-repository pairs within our dataset.
	- explain more, what are the different types of contributions? (coding contributor, coding-with-authorship contributor, non-coding-author, etc.)
	- what are the basics / what do we see across the board? What are the distributions of each of these contributor types
	- compare against analysis built on CRediT statements?

- Next we investigate if these distributions change over time, or, by “research team size”.
	- define research team size, in our case this is the total number of author-developers + non-coding authors + non-credited developers
	- plot the medians of the contributor type distributions over time (by publication year)
	- results in summary

```{python}
# Create subset documents
docs_w_1_citation = documents.loc[documents["cited_by_count"] >= 1].copy()

# Take sample?
if USE_SAMPLE:
    docs_w_1_citation = docs_w_1_citation.sample(frac=0.02, random_state=12)

# Subset to only certain columns
docs_w_1_citation = docs_w_1_citation[
    [
        "id",
        "publication_date",
        "cited_by_count",
        "fwci",
        "is_open_access",
    ]
]

# Rename id to document_id
docs_w_1_citation = docs_w_1_citation.rename(columns={"id": "document_id"})

# Merge repository id in
docs_w_1_citation = docs_w_1_citation.merge(
    doc_repo_links[
        [
            "document_id",
            "repository_id",
        ]
    ],
    left_on="document_id",
    right_on="document_id",
)

# Merge in document details (domain, document type)
docs_w_1_citation = (
    docs_w_1_citation.merge(
        document_topics[["document_id", "topic_id"]],
        left_on="document_id",
        right_on="document_id",
    )
    .merge(
        repositories[["id", "creation_datetime", "last_pushed_datetime"]],
        left_on="repository_id",
        right_on="id",
    )
    .drop(
        columns=["id"],
    )
    .merge(
        topics[["id", "domain_name"]],
        left_on="topic_id",
        right_on="id",
    )
    .drop(
        columns=["id", "topic_id"],
    )
    .merge(
        reduced_doc_types,
        left_on="document_id",
        right_on="document_id",
    )
    .rename(
        columns={
            "domain_name": "domain",
            "reduced_doc_type": "article_type",
        }
    )
)

# Drop any documents that have more than one repository (and vice versa)
docs_w_1_citation = docs_w_1_citation.drop_duplicates(
    subset=["document_id"], keep=False
)
docs_w_1_citation = docs_w_1_citation.drop_duplicates(
    subset=["repository_id"], keep=False
)

# Iter over articles and get the team composition info
team_composition_rows = []
for _, row in docs_w_1_citation.iterrows():
    # Get a boolean value for "no pushes after publication"
    repo_details = repositories.loc[repositories["id"] == row["repository_id"]].iloc[0]

    # Get the number of authors
    author_ids = document_contributors.loc[
        document_contributors["document_id"] == row["document_id"]
    ]["researcher_id"].unique()
    n_authors = len(author_ids)

    # Get the number of devs
    dev_ids = repository_contributors.loc[
        repository_contributors["repository_id"] == row["repository_id"]
    ]["developer_account_id"].unique()
    n_devs = len(dev_ids)

    # Get the set of researcher_dev_links for the authors
    author_dev_links = researcher_dev_links.loc[
        researcher_dev_links["researcher_id"].isin(author_ids)
    ].sort_values("predictive_model_confidence", ascending=False)

    # Drop duplicates by developer_account_id (keeping first)
    # as we may have accidently matched the same dev to the multiple authors
    author_dev_links = author_dev_links.drop_duplicates(
        subset=["developer_account_id"],
        keep="first",
    )

    # Drop any author dev links that have less than 90% confidence
    author_dev_links = author_dev_links.loc[
        author_dev_links["predictive_model_confidence"] >= 0.9
    ]

    # Get the number of authors who were devs on this paper
    n_author_devs = 0
    n_non_author_devs = 0
    for author_id in author_ids:
        author_dev_ids = author_dev_links.loc[
            author_dev_links["researcher_id"] == author_id
        ]["developer_account_id"].unique()

        # Fast exit
        if len(author_dev_ids) == 0:
            continue

        # Something likely went wrong in matching
        if len(author_dev_ids) > 3:
            continue

        if any(author_dev_id in dev_ids for author_dev_id in author_dev_ids):
            n_author_devs += 1
        else:
            n_non_author_devs += 1

    # Append
    team_composition_rows.append(
        {
            "document_id": row["document_id"],
            "repository_id": row["repository_id"],
            "n_authors": n_authors,
            "n_author_devs": n_author_devs,
            "n_non_author_devs": n_non_author_devs,
            "n_devs": n_devs,
        }
    )

# Create dataframe
team_composition = pd.DataFrame(team_composition_rows)

# Merge with docs_w_1_citation
team_composition = team_composition.merge(
    docs_w_1_citation,
    left_on=["document_id", "repository_id"],
    right_on=["document_id", "repository_id"],
)

# Filter out papers with less than 3 authors or 1 dev
team_composition = team_composition.loc[team_composition["n_authors"] >= 3]
team_composition = team_composition.loc[team_composition["n_devs"] >= 1]

# Convert datetimes to datetime
team_composition["publication_date"] = pd.to_datetime(
    team_composition["publication_date"],
    utc=True,
)
team_composition["last_pushed_datetime"] = pd.to_datetime(
    team_composition["last_pushed_datetime"],
    utc=True,
)
team_composition["creation_datetime"] = pd.to_datetime(
    team_composition["creation_datetime"],
    utc=True,
)

# Calculate years since publication from 2024-11-01
team_composition["years_since_publication"] = (
    pd.to_datetime("2024-11-01", utc=True) - team_composition["publication_date"]
).dt.days / 365.25

# Calculate repo creation to last push
team_composition["repo_commit_duration"] = (
    team_composition["last_pushed_datetime"] - team_composition["creation_datetime"]
).dt.days / 365.25

# Create a "days_since_last_push" column
team_composition["days_from_publication_to_last_push"] = (
    team_composition["last_pushed_datetime"] - team_composition["publication_date"]
).dt.days

# Create a "days_since_last_push" column
team_composition["days_from_publication_to_last_push"] = (
    team_composition["last_pushed_datetime"] - team_composition["publication_date"]
).dt.days

# Must have a push within 90 days of publication
team_comp_no_push_after_pub = team_composition.loc[
    team_composition["days_from_publication_to_last_push"] <= 90
].copy()

# Create a "publication_year" column
team_comp_no_push_after_pub["publication_year"] = team_comp_no_push_after_pub[
    "publication_date"
].dt.year

# Drop columns that would conflict with conversion to float
team_comp_no_push_after_pub = team_comp_no_push_after_pub.drop(
    columns=[
        "publication_date",
        "last_pushed_datetime",
        "creation_datetime",
    ]
)

# Add dummies
team_comp_no_push_after_pub_dummies = pd.get_dummies(
    team_comp_no_push_after_pub,
    columns=["article_type", "domain"],
    drop_first=True,
)

# Cast all to float
team_comp_no_push_after_pub_dummies = team_comp_no_push_after_pub_dummies.astype(float)
```

```{python}
# Reuse the "team_composition" dataframe and
# plot the distribution of the
# number of authors, author devs, and non-author devs, and total devs
# Split by domain, article type, and open access status

# Melt the data to convert to long form
team_comp_no_push_after_pub_long = team_comp_no_push_after_pub.melt(
    id_vars=[
        "document_id",
        "publication_year",
        "domain",
        "article_type",
        "is_open_access",
    ],
    value_vars=[
        "n_authors",
        "n_author_devs",
        "n_non_author_devs",
    ],
    var_name="Contributor Type",
    value_name="count",
)

# Replace "team_member_type" values
team_comp_no_push_after_pub_long["Contributor Type"] = (
    team_comp_no_push_after_pub_long["Contributor Type"].replace(
        {
            "n_authors": "Authors",
            "n_author_devs": "Code Contrib. Authors",
            "n_non_author_devs": "Non-Author Code Contribs.",
        }
    )
)

# Plot
sns.catplot(
    data=team_comp_no_push_after_pub_long,
    x="Contributor Type",
    y="count",
    hue="Contributor Type",
    kind="box",
    showfliers=False,
)

_ = plt.xticks(rotation=45, ha="right")
```

```{python}
team_comp_no_push_after_pub_long.groupby("Contributor Type")["count"].describe().drop(
    columns=["count"],
).round(1).astype({
    "min": int,
    "25%": int,
    "50%": int,
    "75%": int,
    "max": int,
})
```

```{python}
# Create a copy of the team_comp_no_push_after_pub dataframe
team_comp_no_push_after_pub_counts = team_comp_no_push_after_pub.copy()

# Replace binary values in "is_open_access" to "Open Access" and "Closed Access"
team_comp_no_push_after_pub_counts["is_open_access"] = (
    team_comp_no_push_after_pub_counts.apply(
        lambda x: "Open" if x["is_open_access"] == 1 else "Closed",
        axis=1,
    )
)

# Count team composition values and always for each control variable
control_vars = {
    "is_open_access": "OA Status",
    "domain": "Domain",
    "article_type": "Article Type",
}
control_var_tables = {}
for control_var, control_display_name in control_vars.items():
    # Instead of taking median, let's return the mean and std in the following format
    # "mean ± std"
    mean_table = (
        team_comp_no_push_after_pub_counts.groupby(control_var)[
            ["n_authors", "n_author_devs", "n_non_author_devs"]
        ]
        .mean()
        .reset_index()
    )
    std_table = (
        team_comp_no_push_after_pub_counts.groupby(control_var)[
            ["n_authors", "n_author_devs", "n_non_author_devs"]
        ]
        .std()
        .reset_index()
    )

    # Merge and format to string
    count_table_rows = []
    for _, row in mean_table.iterrows():
        std_row = std_table.loc[std_table[control_var] == row[control_var]]
        count_table_rows.append(
            {
                control_var: row[control_var],
                "n_authors": f"{row['n_authors']:.1f} ± {std_row['n_authors'].iloc[0]:.1f}",
                "n_author_devs": f"{row['n_author_devs']:.1f} ± {std_row['n_author_devs'].iloc[0]:.1f}",
                "n_non_author_devs": f"{row['n_non_author_devs']:.1f} ± {std_row['n_non_author_devs'].iloc[0]:.1f}",
            }
        )

    # Create dataframe
    count_table = pd.DataFrame(count_table_rows)

    # Change name of the control_var column to "Subset"
    count_table = count_table.rename(columns={control_var: "Subset"})

    # Order columns
    count_table = count_table[
        ["Subset", "n_authors", "n_author_devs", "n_non_author_devs"]
    ]
    
    # Rename columns
    count_table = count_table.rename(
        columns={
            "n_authors": "Authors",
            "n_author_devs": "Code Cntrb. Auth.",
            "n_non_author_devs": "Non-Auth. Code Cntrb.",
        }
    )

    # Order alphabetically
    count_table = count_table.sort_values("Subset")
    
    # Append
    control_var_tables[control_display_name] = count_table
```

::: {.content-visible when-format="html"}

```{python}
#| label: tbl-team-composition-counts
#| tbl-cap: "Team Composition and Coding Status Counts Used in H1"

# Construct multi-row span HTML table
# Columns should be: "Authors", "Code Contributing Authors", "Non-Author Code Contributors"
# Rows should be:
# "Open Access Status", "Domain", "Article Type"

# HTML templates
count_piece_inital_row_template = """
<tr>
  <td rowspan="{n_rows}">{row_name}</td>
  <td>{value_name}</td>
  <td>{n_authors}</td>
  <td>{n_author_devs}</td>
  <td>{n_non_author_devs}</td>
</tr>
""".strip()

count_piece_subsequent_row_template = """
<tr>
  <td>{value_name}</td>
  <td>{n_authors}</td>
  <td>{n_author_devs}</td>
  <td>{n_non_author_devs}</td>
</tr>
""".strip()

# Iter over stats portions (and total)
count_portions_html = []
for key, count_table in control_var_tables.items():
    count_portion_html = []
    for i, control_value in enumerate(count_table["Subset"].unique()):
        if i == 0:
            count_portion_html.append(
                count_piece_inital_row_template.format(
                    n_rows=len(count_table),
                    row_name=key,
                    value_name=control_value,
                    n_authors=count_table.loc[
                        count_table["Subset"] == control_value,
                        "Authors",
                    ].iloc[0],
                    n_author_devs=count_table.loc[
                        count_table["Subset"] == control_value,
                        "Code Cntrb. Auth.",
                    ].iloc[0],
                    n_non_author_devs=count_table.loc[
                        count_table["Subset"] == control_value,
                        "Non-Auth. Code Cntrb.",
                    ].iloc[0],
                )
            )
        else:
            count_portion_html.append(
                count_piece_subsequent_row_template.format(
                    value_name=control_value,
                    n_authors=count_table.loc[
                        count_table["Subset"] == control_value,
                        "Authors",
                    ].iloc[0],
                    n_author_devs=count_table.loc[
                        count_table["Subset"] == control_value,
                        "Code Cntrb. Auth.",
                    ].iloc[0],
                    n_non_author_devs=count_table.loc[
                        count_table["Subset"] == control_value,
                        "Non-Auth. Code Cntrb.",
                    ].iloc[0],
                )
            )

    count_portions_html.append("\n".join(count_portion_html))

# Concat and wrap in table
count_table_html = f"""
<table>
  <tr>
    <th><b>Control</b></th>
    <th><b>Subset</b></th>
    <th><b>Authors</b></th>
    <th><b>Code Cntrb. Auth.</b></th>
    <th><b>Non-Auth. Code Cntrb.</b></th>
  </tr>
  {" ".join(count_portions_html)}
</table>
""".strip()

IPython.display.HTML(count_table_html)
```

:::

::: {.content-visible when-format="pdf"}

```{python}
# LaTeX templates
count_piece_initial_row_template = """    \\multirow{{{n_rows}}}{{*}}{{\\textbf{{{row_name}}}}} & \\cellcolor{{gray!10}}{value_name} & \\cellcolor{{gray!10}}{n_authors} & \\cellcolor{{gray!10}}{n_author_devs} & \\cellcolor{{gray!10}}{n_non_author_devs} \\\\"""

count_piece_subsequent_row_template = """    & {color_cmd}{value_name} & {color_cmd}{n_authors} & {color_cmd}{n_author_devs} & {color_cmd}{n_non_author_devs} \\\\"""

# Table header and footer templates
table_header = """\\begin{table}
\\centering
\\small
\\caption{Team Composition and Coding Status Counts Used in H1}
\\label{tbl-team-composition-counts}
\\begin{tabular}{llrrr}
\\toprule
\\textbf{Control} & \\textbf{Subset} & \\textbf{Authors} & \\textbf{Code Cntrb.\\ Auth.} & \\textbf{Non-Auth.\\ Code Cntrb.} \\\\
\\midrule"""

table_footer = """\\bottomrule
\\end{tabular}
\\end{table}"""

# Generate table content
count_portions_latex = []
for key, count_table in control_var_tables.items():
    count_portion_latex = []
    
    # Get unique control values
    control_values = count_table["Subset"].unique()
    
    # First row with category label
    first_value = control_values[0]
    count_portion_latex.append(
        count_piece_initial_row_template.format(
            n_rows=len(control_values),
            row_name=key,
            value_name=first_value,
            n_authors=count_table.loc[
                count_table["Subset"] == first_value,
                "Authors",
            ].iloc[0],
            n_author_devs=count_table.loc[
                count_table["Subset"] == first_value,
                "Code Cntrb. Auth.",
            ].iloc[0],
            n_non_author_devs=count_table.loc[
                count_table["Subset"] == first_value,
                "Non-Auth. Code Cntrb.",
            ].iloc[0],
        ).rstrip()
    )
    
    # Subsequent rows with alternating colors
    for i, control_value in enumerate(control_values[1:], 1):
        color_cmd = "\\cellcolor{gray!10}" if i % 2 == 1 else ""
        count_portion_latex.append(
            count_piece_subsequent_row_template.format(
                color_cmd=color_cmd,
                value_name=control_value,
                n_authors=count_table.loc[
                    count_table["Subset"] == control_value,
                    "Authors",
                ].iloc[0],
                n_author_devs=count_table.loc[
                    count_table["Subset"] == control_value,
                    "Code Cntrb. Auth.",
                ].iloc[0],
                n_non_author_devs=count_table.loc[
                    count_table["Subset"] == control_value,
                    "Non-Auth. Code Cntrb.",
                ].iloc[0],
            ).rstrip()
        )

    section_latex = "\n".join(count_portion_latex)
    if key != list(control_var_tables.keys())[-1]:  # Don't add midrule after last section
        section_latex += "\\midrule"
    count_portions_latex.append(section_latex)

# Combine all parts
count_table_latex = f"""{table_header}
{"\n".join(count_portions_latex)}
{table_footer}"""

IPython.display.Latex(count_table_latex)
```

:::

```{python}
def compute_article_level_models(
    y_col: str,
    data: pd.DataFrame,
    glm_family: sm.families.Family,
) -> dict[str, sm.GLM]:
    # Remove outliers
    no_outliers = data[
        data[y_col].between(
            data[y_col].quantile(0.03),
            data[y_col].quantile(0.97),
        )
    ].copy()

    # Remove nans
    no_outliers = no_outliers.dropna(subset=[y_col])

    # Common features to use in all models
    required_features = [
        y_col,
        "n_authors",
        "n_author_devs",
        "n_non_author_devs",
        "years_since_publication",
    ]

    # Iter over different control variables and create models for each
    models = {}
    for control_var in [
        "article_type",
        "domain",
        "is_open_access",
    ]:
        # Get control variable list
        control_variables = [
            col for col in no_outliers.columns if col.startswith(control_var)
        ]

        # Create control variable subset of the data
        control_var_subset = no_outliers[required_features + control_variables].copy()

        # Create interactions
        for coding_status_col in ["n_author_devs", "n_non_author_devs"]:
            for control_col in control_variables:
                control_var_subset[f"{coding_status_col} * {control_col}"] = (
                    control_var_subset[coding_status_col]
                    * control_var_subset[control_col]
                )

        # Drop inf and nan
        control_var_subset = control_var_subset.replace(
            [float("inf"), -float("inf")], float("nan")
        ).dropna()

        # Create x and y
        y = control_var_subset[y_col]
        x = control_var_subset.drop(columns=[y_col])
        x = sm.add_constant(x)

        # Fit model
        model = sm.GLM(y, x, family=glm_family).fit()
        models[control_var] = model

    return models
```

```{python}
# Create models for cited_by_count
article_cited_by_count_models = compute_article_level_models(
    "cited_by_count",
    team_comp_no_push_after_pub_dummies,
    glm_family=sm.families.NegativeBinomial(),
)
```

```{python}
# Get value count of publication year
year_counts = team_comp_no_push_after_pub["publication_year"].value_counts()

# Drop years with less than 100 publications
team_comp_no_push_after_pub_long = team_comp_no_push_after_pub_long.loc[
    team_comp_no_push_after_pub_long["publication_year"].isin(
        year_counts.loc[lambda x: x >= 100].index
    )
]

# Look at mean of all counts of team member types over time
sns.relplot(
    data=team_comp_no_push_after_pub_long,
    x="publication_year",
    y="count",
    hue="Contributor Type",
    kind="line",
    estimator="mean",
)
```

### Modeling Citations

- We model an article's total citations by the coding contributorship of the research team and controlled by a number of different factors.
- Each control variable is modeled separately and the results are presented in the tables below:
  - Controlling for article open access status: @{tbl-article-composition-oa-status}
  - Controlling for article domain: @{tbl-article-composition-domain}
  - Controlling for article type: @{tbl-article-composition-type}

#### Open Access Status

::: {.content-visible when-format="html"}

```{python}
#| label: tbl-article-composition-oa-status
#| tbl-cap: "Article Citations by Code Contributorship of Research Team Controlled by Open Access Status"

article_cited_by_count_models["is_open_access"].summary()
```

:::

```{python}
# LaTeX templates
model_results_row_template = """    {bold_str}\\cellcolor{{{color}}}{var_name}{stars} & {bold_str}\\cellcolor{{{color}}}{coef:.2f} & {bold_str}\\cellcolor{{{color}}}{p:.2f} & {bold_str}\\cellcolor{{{color}}}{ci_low:.2f} & {bold_str}\\cellcolor{{{color}}}{ci_high:.2f} \\\\"""

# Table header and footer templates
model_results_table_header = """\\begin{tabular}{l*{6}{r}}
\\toprule
\\textbf{Variable} & \\textbf{coef} & \\textbf{P>|z|} & \\multicolumn{2}{c}{\\textbf{[0.025 0.975]}} \\\\
\\midrule"""

model_results_table_footer = """\\bottomrule
\\end{tabular}"""

def convert_model_results_to_printable_pdf_ready(
    model: sm.GLM,
    tbl_cap: str,
    tbl_label: str,
) -> tuple[str, pd.DataFrame]:
    # Get only the dataframe
    summary_simple_tab = model.summary().tables[1]
    model_results_df = pd.read_html(StringIO(summary_simple_tab.as_html()), header=0, index_col=0)[0]

    # Add exponentiated coef
    model_results_df["exp(coef)"] = np.exp(model_results_df["coef"])

    # Keep only the specified columns
    model_results_df = model_results_df[
        ["exp(coef)", "coef", "z", "P>|z|", "[0.025", "0.975]"]
    ]

    # Set index name to "variable"
    model_results_df.index.name = "variable"
    model_results_df = model_results_df.reset_index()

    rows_latex = []
    for i, result_row in model_results_df.iterrows():
        # Determine if row should be bold and if stars should be added
        is_significant = result_row["P>|z|"] < 0.05
        stars = "~$^{***}$" if is_significant else ""
        
        # Handle bolding - only for significant rows
        if is_significant:
            bold_str = "\\bfseries"
        else:
            bold_str = ""
        
        # Set background color for alternating rows
        color = "gray!10" if i % 2 == 1 else "white"
        
        # Format the row
        rows_latex.append(
            model_results_row_template.format(
                bold_str=bold_str,
                var_name=result_row["variable"].replace("_", " "),
                # exp_coef=result_row["exp(coef)"],
                coef=result_row["coef"],
                # z=result_row["z"],
                p=result_row["P>|z|"],
                ci_low=result_row["[0.025"],
                ci_high=result_row["0.975]"],
                stars=stars,
                color=color
            )
        )

    # Combine all parts
    regression_table_latex = f"""\\begin{{table}}
\\centering
\\caption{{{tbl_cap}}}
\\label{{{tbl_label}}}
\\label{{tbl-reg-results}}
{model_results_table_header}
{chr(10).join(rows_latex)}
{model_results_table_footer}
\\end{{table}}"""

    return regression_table_latex, model_results_df
```

::: {.content-visible when-format="pdf"}

```{python}
model_results_latex, model_results_df = convert_model_results_to_printable_pdf_ready(
    article_cited_by_count_models["is_open_access"],
    tbl_cap=(
        "Article Citations by Code Contributorship of "
        "Research Team Controlled by Open Access Status"
    ),
    tbl_label="tbl-article-composition-oa-status",
)

IPython.display.Latex(model_results_latex)
```

:::

- open access articles
    - n_author_devs is not significant
    - gain `{python} np.exp(0.1554 - 0.0051)` more citations per non-author code contributor

- closed access articles
    - n_author_devs is not significant
    - gain `{python} np.exp(0.1554)` more citations per non-author code contributor

#### Domain

::: {.content-visible when-format="html"}

```{python}
#| label: tbl-article-composition-domain
#| tbl-cap: "Article Citations by Code Contributorship of Research Team Controlled by Domain"

article_cited_by_count_models["domain"].summary()
```

:::

::: {.content-visible when-format="pdf"}

```{python}
model_results_latex, model_results_df = convert_model_results_to_printable_pdf_ready(
    article_cited_by_count_models["domain"],
    tbl_cap=(
        "Article Citations by Code Contributorship of "
        "Research Team Controlled by Domain"
    ),
    tbl_label="tbl-article-composition-domain",
)

IPython.display.Latex(model_results_latex)
```

:::

- health sciences
    - gain `{python} np.exp(0.0944)` more citations per author code contributor compared to no author code contributor health science papers
    - gain `{python} np.exp(0.0708)` more citations per non-author code contributor compared to no non-author code contributor health science papers
   
- life sciences
    - n_author_devs is not significant
    - n_non_author_devs is not significant

- physical sciences
    - n_author_devs is not significant
    - gain `{python} np.exp(0.0803)` more citations per non-author code contributor compared to no non-author code contributor physical science papers

- social sciences
    - n_author_devs is not significant
    - n_non_author_devs is not significant

#### Article Type

::: {.content-visible when-format="html"}

```{python}
#| label: tbl-article-composition-type
#| tbl-cap: "Article Citations by Code Contributorship of Research Team Controlled by Article Type"

article_cited_by_count_models["article_type"].summary()
```

:::

::: {.content-visible when-format="pdf"}

```{python}
model_results_latex, model_results_df = convert_model_results_to_printable_pdf_ready(
    article_cited_by_count_models["article_type"],
    tbl_cap=(
        "Article Citations by Code Contributorship of "
        "Research Team Controlled by Article Type"
    ),
    tbl_label="tbl-article-composition-type",
)

IPython.display.Latex(model_results_latex)
```

:::

- preprint
    - n_author_devs is not significant
    - gain `{python} np.exp(0.1242)` more citations per non-author code contributor compared to no non-author code contributor preprints

- research article
    - gain `{python} np.exp(0.0893)` more citations per author code contributor compared to no author code contributor research articles
    - gain `{python} np.exp(0.0374)` more citations per non-author code contributor compared to no non-author code contributor research articles

- software article
    - n_author_devs is not significant
    - n_non_author_devs is not significant

### Team Composition and Project Duration

- We next investigate the relationship between the duration of a project and the coding contributorship of the research team.

```{python}
team_comp_no_push_after_pub_for_duration = team_comp_no_push_after_pub.copy()

# Calculate ratio
team_comp_no_push_after_pub_for_duration["non_author_to_author_devs_ratio"] = (
    team_comp_no_push_after_pub_for_duration["n_non_author_devs"]
    / team_comp_no_push_after_pub_for_duration["n_author_devs"]
)

# Drop inf and nan
team_comp_no_push_after_pub_for_duration = (
    team_comp_no_push_after_pub_for_duration.replace(
        [float("inf"), -float("inf")], float("nan")
    ).dropna()
)

# Remove negative "repo_commit_duration" repos
team_comp_no_push_after_pub_for_duration = team_comp_no_push_after_pub_for_duration[
    team_comp_no_push_after_pub_for_duration["repo_commit_duration"] >= 0
]

# Plot
sns.lmplot(
    data=team_comp_no_push_after_pub_for_duration,
    y="non_author_to_author_devs_ratio",
    x="repo_commit_duration",
    scatter_kws={"alpha": 0.2},
    line_kws={"color": "red"},
    order=1,
)
```

```{python}
pearsonr(
    team_comp_no_push_after_pub_for_duration["non_author_to_author_devs_ratio"],
    team_comp_no_push_after_pub_for_duration["repo_commit_duration"],
)
```

## Characteristics of Scientific Code Contributors

- Next we investigate the differences between coding and non-coding article authors.
	- specifics, author position in authorship list is a commonly used tool in scientometrics
	- similarly, metrics of “scientific impact” such as h-index, i10 index, and two-year mean citedness are also available to us.
	- plot / table of the distributions between coding and non-coding authors
	- ANOVA / Chi2 tests to see if these differences are significant
	- results in summary

- Just as before, we next investigate if these results are affected by article type and research domain.
	- subplot + stats tests for differences by each article type
	- subplot + stats tests for differences by each domain
	- results in summary

### Author Positions of Code Contributing Authors

```{python}
# Create subset documents
docs_for_per_repo_stats = documents[[
    "id",
    "publication_date",
    "is_open_access",
]].copy()

# Take sample?
if USE_SAMPLE:
    docs_for_per_repo_stats = docs_for_per_repo_stats.sample(frac=0.02, random_state=12)

# Rename id to document_id
docs_for_per_repo_stats = docs_for_per_repo_stats.rename(columns={"id": "document_id"})

# Merge repository id in
docs_for_per_repo_stats = docs_for_per_repo_stats.merge(
    doc_repo_links[
        [
            "document_id",
            "repository_id",
        ]
    ],
    left_on="document_id",
    right_on="document_id",
)

# Merge in document details (domain, document type)
docs_for_per_repo_stats = (
    docs_for_per_repo_stats.merge(
        document_topics[["document_id", "topic_id"]],
        left_on="document_id",
        right_on="document_id",
    )
    .merge(
        repositories[["id", "last_pushed_datetime"]],
        left_on="repository_id",
        right_on="id",
    )
    .drop(
        columns=["id"],
    )
    .merge(
        topics[["id", "domain_name"]],
        left_on="topic_id",
        right_on="id",
    )
    .drop(
        columns=["id", "topic_id"],
    )
    .merge(
        reduced_doc_types,
        left_on="document_id",
        right_on="document_id",
    )
    .rename(
        columns={
            "domain_name": "domain",
            "reduced_doc_type": "article_type",
        }
    )
)

# Drop any documents that have more than one repository (and vice versa)
docs_for_per_repo_stats = docs_for_per_repo_stats.drop_duplicates(
    subset=["document_id"], keep=False
)
docs_for_per_repo_stats = docs_for_per_repo_stats.drop_duplicates(
    subset=["repository_id"], keep=False
)

# Convert datetimes to datetimes
docs_for_per_repo_stats["publication_date"] = pd.to_datetime(
    docs_for_per_repo_stats["publication_date"],
    utc=True,
)
docs_for_per_repo_stats["last_pushed_datetime"] = pd.to_datetime(
    docs_for_per_repo_stats["last_pushed_datetime"],
    utc=True,
)

# Remove any documents that have pushes after 90 days
# past publication
docs_for_per_repo_stats = docs_for_per_repo_stats.loc[
    (docs_for_per_repo_stats["last_pushed_datetime"] - docs_for_per_repo_stats["publication_date"]).dt.days <= 90
]

# Drop publication date and last pushed datetime
docs_for_per_repo_stats = docs_for_per_repo_stats.drop(
    columns=[
        "publication_date",
        "last_pushed_datetime",
    ]
)

# Get document contributors
doc_contribs_for_code_char = document_contributors.loc[
    document_contributors["document_id"].isin(docs_for_per_repo_stats["document_id"])
]

# Merge with docs_for_per_repo_stats
doc_contribs_for_code_char = doc_contribs_for_code_char.merge(
    docs_for_per_repo_stats,
    left_on="document_id",
    right_on="document_id",
).drop(columns=["id"])

# Iter over rows and check if the author is a dev on the same article-repo pair
doc_contribs_for_code_char_rows = []
for i, row in doc_contribs_for_code_char.iterrows():
    # Get devs
    this_repo_devs = repository_contributors.loc[
        repository_contributors["repository_id"] == row["repository_id"]
    ]

    # Get the set of researcher_dev_links for the authors
    author_dev_links = researcher_dev_links.loc[
        researcher_dev_links["researcher_id"] == row.researcher_id
    ].sort_values("predictive_model_confidence", ascending=False)

    # Drop duplicates by developer_account_id (keeping first)
    # as we may have accidently matched the same dev to the multiple authors
    author_dev_links = author_dev_links.drop_duplicates(
        subset=["developer_account_id"],
        keep="first",
    )

    # Drop any author dev links that have less than 90% confidence
    author_dev_links = author_dev_links.loc[
        author_dev_links["predictive_model_confidence"] >= 0.9
    ]

    # If no author dev links return same rows and "is_code_contributor" as False
    if len(author_dev_links) == 0:
        doc_contribs_for_code_char_rows.append(
            {
                **row.to_dict(),
                "is_code_contributor": False,
            }
        )
    
    else:
        # Add same rows and add "is_code_contributor" based on if developer_account_id is in this_repo_devs
        doc_contribs_for_code_char_rows.append(
            {
                **row.to_dict(),
                "is_code_contributor": any(
                    author_dev_link["developer_account_id"]
                    in this_repo_devs["developer_account_id"].unique()
                    for _, author_dev_link in author_dev_links.iterrows()
                ),
            }
        )

# Create dataframe
doc_contribs_for_code_char = pd.DataFrame(doc_contribs_for_code_char_rows)

# Replace binary values in "is_open_access" to "Open Access" and "Closed Access"
doc_contribs_for_code_char["is_open_access"] = (
    doc_contribs_for_code_char.apply(
        lambda x: "Open Access" if x["is_open_access"] == 1 else "Closed Access",
        axis=1,
    )
)

# Replace binary values in "is_corresponding" to "Corresponding" and "Not Corresponding"
doc_contribs_for_code_char["is_corresponding"] = (
    doc_contribs_for_code_char.apply(
        lambda x: "Corresponding" if x["is_corresponding"] == 1 else "Not Corresponding",
        axis=1,
    )
)
```

```{python}
def _run_posthocs(
    data: pd.DataFrame,
    split_var: str,
) -> None:            
    # Conduct pairwise posthocs
    binom_results = []
    for split_val in data[split_var].unique():
        split_val_subset = data.loc[
            data[split_var] == split_val
        ]

        # Run test
        results = binomtest(
            sum(split_val_subset["is_code_contributor"]),
            len(split_val_subset),
            0.5,
        )
        
        # Conduct binomial test
        binom_results.append(
            {
                split_var: split_val,
                "p": results.pvalue,
                "statistic": results.statistic,
                "n": len(split_val_subset),
                "n_code_contributors": sum(split_val_subset["is_code_contributor"]),
            }
        )

        # P adjust
        p_values = false_discovery_control(
            [result["p"] for result in binom_results],
            method="bh",
        )

        # Replace p values
        for i, result in enumerate(binom_results):
            result["p"] = p_values[i]
        
    # Print results
    for result in binom_results:
        print(
            f"{split_var}: {result[split_var]}, "
            f"p: {result['p']}, statistic: {result['statistic']}, n: {result['n']}"
        )


def _run_chi2_and_posthocs(
    data: pd.DataFrame,
    control_var: str | None,
    split_var: str,
) -> None:
    if control_var is None:
        print("Overall")
        xtabs = pd.crosstab(
            data[split_var],
            data["is_code_contributor"],
        )
        print(xtabs)
        chi2, p, _, _ = chi2_contingency(xtabs)
        if p < 0.05:
            print(f"Chi2: {chi2}, p: {p}, n: {len(data)}")
            _run_posthocs(
                data,
                split_var=split_var,
            )

        else:
            print(f"Coding by {split_var} not significant")

    else:
        for control_val in data[control_var].unique():
            print(control_val)
            # Get data with control_val subset
            control_val_subset = data.loc[
                data[control_var] == control_val
            ]
            xtabs = pd.crosstab(
                control_val_subset[split_var],
                control_val_subset["is_code_contributor"],
            )
            print(xtabs)
            chi2, p, _, _ = chi2_contingency(xtabs)
            if p < 0.05:
                print(f"Chi2: {chi2}, p: {p}, n: {len(control_val_subset)}")
                _run_posthocs(
                    control_val_subset,
                    split_var=split_var,
                )

            else:
                print(f"Coding by {split_var} not significant")

        print()
        print()
```

```{python}
# Run chi2 and posthocs for overall author position
_run_chi2_and_posthocs(
    doc_contribs_for_code_char,
    control_var=None,
    split_var="position",
)
```

#### Domain

```{python}
_run_chi2_and_posthocs(
    doc_contribs_for_code_char,
    control_var="domain",
    split_var="position",
)
```

#### Article Type

```{python}
_run_chi2_and_posthocs(
    doc_contribs_for_code_char,
    control_var="article_type",
    split_var="position",
)
```

#### Open Access Status

```{python}
_run_chi2_and_posthocs(
    doc_contribs_for_code_char,
    control_var="is_open_access",
    split_var="position",
)
```

### Corresponding Status of Code Contributing Authors

```{python}
# Run chi2 and posthocs for overall corresponding status
_run_chi2_and_posthocs(
    doc_contribs_for_code_char,
    control_var=None,
    split_var="is_corresponding",
)
```

#### Domain

```{python}
_run_chi2_and_posthocs(
    doc_contribs_for_code_char,
    control_var="domain",
    split_var="is_corresponding",
)
```

#### Article Type

```{python}
_run_chi2_and_posthocs(
    doc_contribs_for_code_char,
    control_var="article_type",
    split_var="is_corresponding",
)
```

#### Open Access Status

```{python}
_run_chi2_and_posthocs(
    doc_contribs_for_code_char,
    control_var="is_open_access",
    split_var="is_corresponding",
)
```

### Modeling H-Index

- We model an authors total citations by their coding status and controlled by a number of different factors.
- Each control variable is modeled separately and the results are presented in the tables below:
  - Controlling for an authors most frequent author position: @{tbl-researcher-coding-status-author-position}
  - Controlling for an authors most frequent domain: @{tbl-researcher-coding-status-domain}
  - Controlling for an authors most frequent article type: @{tbl-researcher-coding-status-article-type}

```{python}
# First, get the set of researchers who have at least 3 documents
researchers_w_3_docs = researchers.loc[
    researchers["id"].isin(
        document_contributors["researcher_id"]
        .value_counts()
        .loc[lambda x: x >= 3]
        .index
    )
]

# Use sample?
if USE_SAMPLE:
    researchers_w_3_docs = researchers_w_3_docs.sample(frac=0.02, random_state=12)

# Next, for each researcher, get the set of documents they have contributed to
researchers_w_3_docs = document_contributors.loc[
    document_contributors["researcher_id"].isin(researchers_w_3_docs["id"])
].merge(
    researchers_w_3_docs,
    left_on="researcher_id",
    right_on="id",
)

# Attach document for publication date
researchers_w_3_docs = researchers_w_3_docs.merge(
    documents[["id", "publication_date"]],
    left_on="document_id",
    right_on="id",
).drop(
    columns=["id"],
)

# Keep only certain columns
researchers_w_3_docs = researchers_w_3_docs[
    [
        "researcher_id",
        "document_id",
        "publication_date",
        "position",
        "is_corresponding",
        "works_count",
        "cited_by_count",
        "h_index",
        "i10_index",
        "two_year_mean_citedness",
    ]
]

# Next, for each researcher_doc, attach the document details (domain, reduced_doc_type)
researchers_w_3_docs = (
    researchers_w_3_docs.merge(
        document_topics[["document_id", "topic_id"]],
        left_on="document_id",
        right_on="document_id",
    )
    .merge(
        topics[["id", "domain_name"]],
        left_on="topic_id",
        right_on="id",
    )
    .drop(
        columns=["id", "topic_id"],
    )
    .merge(
        reduced_doc_types,
        left_on="document_id",
        right_on="document_id",
    )
)

# Now for each of these, we want to see if they have coded on the document
researchers_coded = []
for _, row in researchers_w_3_docs.iterrows():
    # Check for dev account
    dev_links = researcher_dev_links.loc[
        researcher_dev_links["researcher_id"] == row["researcher_id"]
    ]
    if len(dev_links) == 0:
        researchers_coded.append(
            {
                "researcher_id": row["researcher_id"],
                "document_id": row["document_id"],
                "coded_on_article": 0,
            }
        )
        continue

    # Skip this person if they have more than 3 links
    # Likely something went wrong
    if len(dev_links) > 3:
        continue

    # Get repos associated with document
    repo_links_for_doc = doc_repo_links.loc[
        doc_repo_links["document_id"] == row["document_id"]
    ]

    # Skip if there is more than 1 repo associated with the document
    # We just don't know how to handle these cases right now
    if len(repo_links_for_doc) > 1:
        continue

    # Also skip if 0
    if len(repo_links_for_doc) == 0:
        continue

    # Get the repo_id for the single repo
    repo_id = repo_links_for_doc["repository_id"].iloc[0]

    # Get the repo_contributors for this repository
    repo_contributors = repository_contributors.loc[
        repository_contributors["repository_id"] == repo_id
    ]

    # Check if any of the dev accounts are in the repo contribs
    researcher_coded = (
        len(
            set(repo_contributors["developer_account_id"].unique()).intersection(
                set(dev_links["developer_account_id"].unique()),
            )
        )
        > 0
    )

    # Finally assert any of the repo_contributors are the dev account
    # associated with the researcher
    researchers_coded.append(
        {
            "researcher_id": row["researcher_id"],
            "document_id": row["document_id"],
            "coded_on_article": int(researcher_coded),
        }
    )

# Create dataframe
researchers_coded_df = pd.DataFrame(researchers_coded)

# Merge with researchers_w_3_docs
researchers_w_3_docs_and_coded = researchers_coded_df.merge(
    researchers_w_3_docs,
    left_on=["researcher_id", "document_id"],
    right_on=["researcher_id", "document_id"],
)

def _mode_or_recent_reduce(group: pd.DataFrame, col: str) -> str:
    # Get the mode
    mode = group[col].mode().tolist()
    if len(mode) == 1:
        return mode[0]

    # Otherwise, iter over most recent publications until value in mode is found
    group_ordered_by_date = group.sort_values("publication_date", ascending=False)
    for _, row in group_ordered_by_date.iterrows():
        if row[col] in mode:
            return row[col]


def _agg_apply(group: pd.DataFrame) -> dict:
    return {
        "n_documents": group["document_id"].nunique(),
        "n_coded": group["coded_on_article"].sum(),
        "works_count": group["works_count"].iloc[0],
        "cited_by_count": group["cited_by_count"].iloc[0],
        "h_index": group["h_index"].iloc[0],
        "i10_index": group["i10_index"].iloc[0],
        "two_year_mean_citedness": group["two_year_mean_citedness"].iloc[0],
        "position": _mode_or_recent_reduce(group, "position"),
        "domain_name": _mode_or_recent_reduce(group, "domain_name"),
        "reduced_doc_type": _mode_or_recent_reduce(group, "reduced_doc_type"),
    }


researchers_w_3_docs_and_coded_agg = (
    researchers_w_3_docs_and_coded.groupby("researcher_id")[
        [col for col in researchers_w_3_docs_and_coded if col != "researcher_id"]
    ]
    .apply(_agg_apply)
    .reset_index(name="dicts")
)
researchers_w_3_docs_and_coded_agg = researchers_w_3_docs_and_coded_agg.join(
    pd.json_normalize(researchers_w_3_docs_and_coded_agg["dicts"])
).drop(columns=["dicts"])

# Create three features for coding status
# "any" coding status
# "majority" coding status
# "always" coding status
# determine type by taking the percentage of documents coded on
# and determining if it is greater than 0, greater than 0.5, or 1
researchers_w_3_docs_and_coded_agg["coding_pct"] = (
    researchers_w_3_docs_and_coded_agg["n_coded"]
    / researchers_w_3_docs_and_coded_agg["n_documents"]
)

researchers_w_3_docs_and_coded_agg["any_coding"] = (
    (researchers_w_3_docs_and_coded_agg["coding_pct"] > 0)
    & (researchers_w_3_docs_and_coded_agg["coding_pct"] <= 0.5)
).astype(int)
researchers_w_3_docs_and_coded_agg["majority_coding"] = (
    (researchers_w_3_docs_and_coded_agg["coding_pct"] > 0.5)
    & (researchers_w_3_docs_and_coded_agg["coding_pct"] < 1)
).astype(int)
researchers_w_3_docs_and_coded_agg["always_coding"] = (
    researchers_w_3_docs_and_coded_agg["coding_pct"] == 1
).astype(int)

# Drop n_documents, n_coded and
# rename "position" to "common_author_position",
# "domain_name" to "common_domain",
# and "reduced_doc_type" to "common_article_type"
researchers_w_3_docs_and_coded_agg = researchers_w_3_docs_and_coded_agg.drop(
    columns=["n_documents", "n_coded"]
)
researchers_w_3_docs_and_coded_agg = researchers_w_3_docs_and_coded_agg.rename(
    columns={
        "position": "common_author_position",
        "domain_name": "common_domain",
        "reduced_doc_type": "common_article_type",
    }
)

# Get dummies for categorical variables
researchers_w_3_docs_and_coded_agg_dummies = pd.get_dummies(
    researchers_w_3_docs_and_coded_agg,
    columns=["common_author_position", "common_domain", "common_article_type"],
    drop_first=True,
)

# Cast all to float
researchers_w_3_docs_and_coded_agg_dummies = (
    researchers_w_3_docs_and_coded_agg_dummies.astype(float)
)
```

```{python}
def compute_researcher_level_models(
    y_col: str,
    data: pd.DataFrame,
    glm_family: sm.families.Family,
) -> dict[str, sm.GLM]:
    # Remove all "zero" y_col authors
    no_outliers = data[data[y_col] > 0].copy()

    # Remove outliers
    no_outliers = no_outliers[
        no_outliers[y_col].between(
            no_outliers[y_col].quantile(0.03),
            no_outliers[y_col].quantile(0.97),
        )
    ].copy()

    # Common features to use in all models
    required_features = [
        y_col,
        "works_count",
        "any_coding",
        "majority_coding",
        "always_coding",
    ]

    # Iter over different control variables and create models for each
    models = {}
    for control_var in [
        "common_author_position",
        "common_article_type",
        "common_domain",
    ]:
        # Get control variable list
        control_variables = [
            col for col in no_outliers.columns if col.startswith(control_var)
        ]

        # Create control variable subset of the data
        control_var_subset = no_outliers[required_features + control_variables].copy()

        # Create interactions
        for coding_status_col in ["any_coding", "majority_coding", "always_coding"]:
            for control_col in control_variables:
                control_var_subset[f"{coding_status_col} * {control_col}"] = (
                    control_var_subset[coding_status_col]
                    * control_var_subset[control_col]
                )

        # Drop inf and nan
        control_var_subset = control_var_subset.replace(
            [float("inf"), -float("inf")], float("nan")
        ).dropna()

        # Create x and y
        y = control_var_subset[y_col]
        x = control_var_subset.drop(columns=[y_col])
        x = sm.add_constant(x)

        # Fit model
        model = sm.GLM(y, x, family=glm_family).fit()
        models[control_var] = model

    return models
```

::: {.content-visible when-format="html"}

```{python}
#| label: tbl-researcher-coding-status-counts
#| tbl-cap: "Counts of Researcher Coding Status Used in H5"
#| echo: false

# Count number of people who code any, majority, and always for each control variable
control_vars = {
    "common_author_position": "Freq. Author Pos.",
    "common_domain": "Freq. Domain",
    "common_article_type": "Freq. Article Type",
}
control_var_tables = {}
for control_var, control_display_name in control_vars.items():
    # Get counts of coding status
    count_table = (
        researchers_w_3_docs_and_coded_agg.groupby(control_var)[
            ["any_coding", "majority_coding", "always_coding"]
        ]
        .sum()
        .reset_index()
    )

    # Add totals
    for val in count_table[control_var].unique():
        count_table.loc[
            count_table[control_var] == val, "total"
        ] = len(
            researchers_w_3_docs_and_coded_agg.loc[
                researchers_w_3_docs_and_coded_agg[control_var] == val
            ]
        )

    # Convert total to int
    count_table["total"] = count_table["total"].astype(int)

    # Change name of the control_var column to "Control Values"
    count_table = count_table.rename(columns={control_var: "Subset"})

    # Order columns
    count_table = count_table[
        ["Subset", "any_coding", "majority_coding", "always_coding", "total"]
    ]
    
    # Rename columns
    count_table = count_table.rename(
        columns={
            "any_coding": "Any Coding",
            "majority_coding": "Majority Coding",
            "always_coding": "Always Coding",
            "total": "Total",
        }
    )

    # Order alphabetically
    count_table = count_table.sort_values("Subset")
    
    # Append
    control_var_tables[control_display_name] = count_table

# Constuct HTML Table

# Construct multi-row span HTML table
# Columns should be: "Coded Any", "Coded Majority", "Coded Always", "Total"
# Rows should be:
# "Most Freq. Author Position", "Most Freq. Domain", "Most Freq. Article Type"

# HTML templates
count_piece_inital_row_template = """
<tr>
  <td rowspan="{n_rows}">{row_name}</td>
  <td>{value_name}</td>
  <td>{any_coding}</td>
  <td>{majority_coding}</td>
  <td>{always_coding}</td>
  <td>{total}</td>
</tr>
""".strip()

count_piece_subsequent_row_template = """
<tr>
  <td>{value_name}</td>
  <td>{any_coding}</td>
  <td>{majority_coding}</td>
  <td>{always_coding}</td>
  <td>{total}</td>
</tr>
""".strip()

# Iter over stats portions (and total)
count_portions_html = []
for key, count_table in control_var_tables.items():
    count_portion_html = []
    for i, control_value in enumerate(count_table["Subset"].unique()):
        if i == 0:
            count_portion_html.append(
                count_piece_inital_row_template.format(
                    n_rows=len(count_table),
                    row_name=key,
                    value_name=control_value,
                    any_coding=count_table.loc[
                        count_table["Subset"] == control_value, "Any Coding"
                    ].iloc[0],
                    majority_coding=count_table.loc[
                        count_table["Subset"] == control_value, "Majority Coding"
                    ].iloc[0],
                    always_coding=count_table.loc[
                        count_table["Subset"] == control_value, "Always Coding"
                    ].iloc[0],
                    total=count_table.loc[
                        count_table["Subset"] == control_value, "Total"
                    ].iloc[0],
                )
            )
        else:
            count_portion_html.append(
                count_piece_subsequent_row_template.format(
                    value_name=control_value,
                    any_coding=count_table.loc[
                        count_table["Subset"] == control_value, "Any Coding"
                    ].iloc[0],
                    majority_coding=count_table.loc[
                        count_table["Subset"] == control_value, "Majority Coding"
                    ].iloc[0],
                    always_coding=count_table.loc[
                        count_table["Subset"] == control_value, "Always Coding"
                    ].iloc[0],
                    total=count_table.loc[
                        count_table["Subset"] == control_value, "Total"
                    ].iloc[0],
                )
            )

    count_portions_html.append("\n".join(count_portion_html))

# Concat and wrap in table
count_table_html = f"""
<table>
  <tr>
    <th><b>Control</b></th>
    <th><b>Subset</b></th>
    <th><b>Any Coding</b></th>
    <th><b>Majority Coding</b></th>
    <th><b>Always Coding</b></th>
    <th><b>Total</b></th>
  </tr>
  {" ".join(count_portions_html)}
</table>
""".strip()

IPython.display.HTML(count_table_html)
```

:::

::: {.content-visible when-format="pdf"}

```{python}
# LaTeX templates
count_piece_initial_row_template = """    \\multirow{{{n_rows}}}{{*}}{{\\textbf{{{row_name}}}}} & \\cellcolor{{{color}}}{value_name} & \\cellcolor{{{color}}}{any_coding} & \\cellcolor{{{color}}}{majority_coding} & \\cellcolor{{{color}}}{always_coding} & \\cellcolor{{{color}}}{total} \\\\"""

count_piece_subsequent_row_template = """    & \\cellcolor{{{color}}}{value_name} & \\cellcolor{{{color}}}{any_coding} & \\cellcolor{{{color}}}{majority_coding} & \\cellcolor{{{color}}}{always_coding} & \\cellcolor{{{color}}}{total} \\\\"""

# Table header and footer templates
table_header = """\\begin{table}
\\centering
\\small
\\caption{Counts of Researcher Coding Status Used in H5}
\\label{tbl-researcher-coding-status-counts}
\\begin{tabular}{llrrrr}
\\toprule
\\textbf{Control} & \\textbf{Subset} & \\textbf{Any Coding} & \\textbf{Majority Coding} & \\textbf{Always Coding} & \\textbf{Total} \\\\
\\midrule"""

table_footer = """\\bottomrule
\\end{tabular}
\\end{table}"""

# Generate table content
count_portions_latex = []
for key, count_table in control_var_tables.items():
    count_portion_latex = []
    
    # Get control values
    control_values = count_table["Subset"].unique()
    
    # First row with category label
    first_value = control_values[0]
    count_portion_latex.append(
        count_piece_initial_row_template.format(
            n_rows=len(control_values),
            row_name=key,
            value_name=first_value,
            color="white",
            any_coding=count_table.loc[
                count_table["Subset"] == first_value, "Any Coding"
            ].iloc[0],
            majority_coding=count_table.loc[
                count_table["Subset"] == first_value, "Majority Coding"
            ].iloc[0],
            always_coding=count_table.loc[
                count_table["Subset"] == first_value, "Always Coding"
            ].iloc[0],
            total=count_table.loc[
                count_table["Subset"] == first_value, "Total"
            ].iloc[0]
        )
    )
    
    # Subsequent rows with alternating colors
    for i, control_value in enumerate(control_values[1:], 1):
        color = "gray!10" if i % 2 == 1 else "white"
        count_portion_latex.append(
            count_piece_subsequent_row_template.format(
                value_name=control_value,
                color=color,
                any_coding=count_table.loc[
                    count_table["Subset"] == control_value, "Any Coding"
                ].iloc[0],
                majority_coding=count_table.loc[
                    count_table["Subset"] == control_value, "Majority Coding"
                ].iloc[0],
                always_coding=count_table.loc[
                    count_table["Subset"] == control_value, "Always Coding"
                ].iloc[0],
                total=count_table.loc[
                    count_table["Subset"] == control_value, "Total"
                ].iloc[0]
            )
        )

    section_latex = "\n".join(count_portion_latex)
    if key != list(control_var_tables.keys())[-1]:  # Don't add midrule after last section
        section_latex += "\\midrule"
    count_portions_latex.append(section_latex)

# Combine all parts
coding_status_table_latex = f"""{table_header}
{chr(10).join(count_portions_latex)}
{table_footer}"""

IPython.display.Latex(coding_status_table_latex)
```

:::

```{python}
# Create models for h_index
author_h_index_models = compute_researcher_level_models(
    "h_index",
    researchers_w_3_docs_and_coded_agg_dummies,
    glm_family=sm.families.Gaussian(sm.families.links.Log()),
)
```


#### Author Position

::: {.content-visible when-format="html"}

```{python}
#| label: tbl-researcher-coding-status-author-position
#| tbl-cap: "Researcher H-Index by Coding Status Controlled by Most Freq. Author Position"

author_h_index_models["common_author_position"].summary()
```

:::

::: {.content-visible when-format="pdf"}

```{python}
model_results_latex, model_results_df = convert_model_results_to_printable_pdf_ready(
    author_h_index_models["common_author_position"],
    tbl_cap=(
        "Researcher H-Index by Coding Status Controlled by Most Freq. Author Position"
    ),
    tbl_label="tbl-researcher-coding-status-author-position",
)

IPython.display.Latex(model_results_latex)
```

:::

- first authors
   - any coding has a positive association with citations (`{python} np.exp(0.234)`)
   - majority coding has a negative association with citations (`{python} np.exp(-0.121)`)
   - always coding has a negative association with citations (`{python} np.exp(-0.269)`)

- middle authors
   - any coding has a negative association with citations (`{python} np.exp(-0.290)`)
   - majority coding has a negative association with citations (`{python} np.exp(-0.101)`)
   - always coding is not significant

- last authors
   - any coding has a negative association with citations (`{python} np.exp(-0.135)`)
   - majority coding is not significant
   - always coding is not significant

In general, any coding has a positive association while majority and always coding have negative associations with citations.
"The more you code the less you are cited" -- granted that we don't have a lot of data for always coding authors (which itself backs up qual lit).

in general, coding is associated with about a ~10 - 30% decrease in citations for a number of conditions when compared to non-coding first authors.

#### Domain

::: {.content-visible when-format="html"}

```{python}
#| label: tbl-researcher-coding-status-domain
#| tbl-cap: "Researcher H-Index by Coding Status Controlled by Most Freq. Domain"

author_h_index_models["common_domain"].summary()
```

:::

::: {.content-visible when-format="pdf"}

```{python}
model_results_latex, model_results_df = convert_model_results_to_printable_pdf_ready(
    author_h_index_models["common_domain"],
    tbl_cap=(
        "Researcher H-Index by Coding Status Controlled by Most Freq. Domain"
    ),
    tbl_label="tbl-researcher-coding-status-domain",
)

IPython.display.Latex(model_results_latex)
```

:::

- health sciences
   - any coding is not significant
   - majority coding has a negative association with citations (`{python} np.exp(-0.284)`)
   - always coding has a negative association with citations (`{python} np.exp(-0.491)`)
   
- life sciences
   - any coding is not significant
   - majority coding is not significant
   - always coding has a positive association with citations (`{python} np.exp(+0.469)`)

- physical sciences
   - any coding is not significant
   - majority coding is not significant
   - always coding is not significant

- social sciences
   - any coding has a negative associate with citations (`{python} np.exp(-0.219)`)
   - majority coding is not significant
   - always coding has a positive association with citations (`{python} np.exp(+0.462)`)

We couldn't significant results for a majority of the groupings so I don't think that we can say anything too strongly, however,
health sciences doesn't seem to favor coding (which I think is inline most with "RSE" dynamics). Social sciences is split, and social science is broad so this may be a "qual" vs "quant" split, if you include mixed methods researchers in there its hard to parse.

Physical sciences being entirely non-significant is interesting because a majority of our data comes from Physical sciences. this could indicate that coding is just a part of the culture and doesn't have a significant impact on citations (which is inline with CS being the bulk of our physical sciences data).

#### Article Type

::: {.content-visible when-format="html"}

```{python}
#| label: tbl-researcher-coding-status-article-type
#| tbl-cap: "Researcher H-Index by Coding Status Controlled by Most Freq. Article Type"

author_h_index_models["common_article_type"].summary()
```

:::

::: {.content-visible when-format="pdf"}

```{python}
model_results_latex, model_results_df = convert_model_results_to_printable_pdf_ready(
    author_h_index_models["common_article_type"],
    tbl_cap=(
        "Researcher H-Index by Coding Status Controlled by Most Freq. Article Type"
    ),
    tbl_label="tbl-researcher-coding-status-article-type",
)

IPython.display.Latex(model_results_latex)
```

:::

- preprint
   - any coding is not significant
   - majority coding has a negative association with citations (`{python} np.exp(-0.395)`)
   - always coding has a negative association with citations (`{python} np.exp(-0.461)`)

- research article
   - any coding is not significant
   - majority coding has a positive association with citations (`{python} np.exp(+0.055)`)
   - always coding has a positive association with citations (`{python} np.exp(+0.081)`)

- software article
   - any coding has a positive association with citations (`{python} np.exp(+0.558)`)
   - majority coding is not significant
   - always coding has a positive association with citations (`{python} np.exp(+0.384)`)

Preprints (from arXiv) have a negative association with coding generally. Research articles have a very slim positive association with coding. Software articles have a strongly positive association with coding (unsuprising).

# Appendix

## Full Comparison of Models and Optional Features for Author-Developer-Account Matching

::: {.content-visible when-format="html"}

```{python}
#| label: tbl-em-model-comparison
#| tbl-cap: "Comparison of Models for Author-Developer-Account Matching"

exp_results
```

:::

::: {.content-visible when-format="pdf"}

```{python}
def df_to_latex_table(
    df: pd.DataFrame,
    caption: str,
    label: str,
    alignment: str = None,
    float_format: str = ".3f"
) -> str:
    # Generate column alignment if not provided
    if alignment is None:
        # Use 'l' for string/object columns, 'r' for numeric columns
        alignment = "".join(
            ["l" if dtype.kind in "OUS" else "r" for dtype in df.dtypes]
        )
    else:
        # Ensure provided alignment matches number of columns
        if len(alignment) != len(df.columns):
            raise ValueError(f"Alignment string length ({len(alignment)}) must match number of columns ({len(df.columns)})")
    
    # Create header row
    header_row = " & ".join([f"\\textbf{{{col}}}" for col in df.columns]) + " \\\\"
    
    # Create data rows with alternating colors
    data_rows = []
    for i, row in enumerate(df.itertuples(index=False)):
        color = "gray!10" if i % 2 == 1 else "white"
        
        # Format each cell value
        cells = []
        for val, dtype in zip(row, df.dtypes):
            if pd.isna(val):
                cells.append("")
            elif dtype.kind in "fc":  # Float or complex
                try:
                    cells.append(f"{float(val):{float_format}}")
                except (ValueError, TypeError):
                    cells.append(str(val))
            else:
                cells.append(str(val))
        
        # Create the row with consistent cell coloring
        colored_cells = [f"\\cellcolor{{{color}}}{cell}" for cell in cells]
        row_str = " & ".join(colored_cells) + " \\\\"
        data_rows.append(f"    {row_str}")
    
    # Combine into final table
    table_template = f"""\\begin{{table}}
\\centering
\\caption{{{caption}}}
\\label{{tbl-{label}}}
\\begin{{tabular}}{{{alignment}}}
\\toprule
{header_row}
\\midrule
{chr(10).join(data_rows)}
\\bottomrule
\\end{{tabular}}
\\end{{table}}"""

    return table_template

```

```{python}
# Convert to LaTeX table
model_comparison_latex = df_to_latex_table(
    exp_results,
    caption="Comparison of Models for Author-Developer-Account Matching",
    label="tbl-em-model-comparison",
    alignment="llrrrr",
)

IPython.display.Latex(model_comparison_latex)
```

:::