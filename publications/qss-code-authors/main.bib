@article{edwards2013knowledge,
  title={Knowledge infrastructures: Intellectual frameworks and research challenges},
  author={Edwards, Paul N and Jackson, Steven J and Chalmers, Melissa K and Bowker, Geoffrey C and Borgman, Christine L and Ribes, David and Burton, Matt and Calvert, Scout},
  year={2013}
}

@article{mayernik2017assessing,
  title={Assessing and tracing the outcomes and impact of research infrastructures},
  author={Mayernik, Matthew S and Hart, David L and Maull, Keith E and Weber, Nicholas M},
  journal={Journal of the Association for Information Science and Technology},
  volume={68},
  number={6},
  pages={1341--1359},
  year={2017},
  publisher={Wiley Online Library}
}

@article{howison2015currentandfuturemeasures,
    author = {Howison, James and Deelman, Ewa and McLennan, Michael J. and Ferreira da Silva, Rafael and Herbsleb, James D.},
    title = {Understanding the scientific software ecosystem and its impact: Current and future measures},
    journal = {Research Evaluation},
    volume = {24},
    number = {4},
    pages = {454-470},
    year = {2015},
    month = {07},
    abstract = {Software is increasingly important to the scientific enterprise, and science-funding agencies are increasingly funding software work. Accordingly, many different participants need insight into how to understand the relationship between software, its development, its use, and its scientific impact. In this article, we draw on interviews and participant observation to describe the information needs of domain scientists, software component producers, infrastructure providers, and ecosystem stewards, including science funders. We provide a framework by which to categorize different types of measures and their relationships as they reach around from funding, development, scientific use, and through to scientific impact. We use this framework to organize a presentation of existing measures and techniques, and to identify areas in which techniques are either not widespread, or are entirely missing. We conclude with policy recommendations designed to improve insight into the scientific software ecosystem, make it more understandable, and thereby contribute to the progress of science.},
    issn = {0958-2029},
    doi = {10.1093/reseval/rvv014},
    url = {https://doi.org/10.1093/reseval/rvv014},
    eprint = {https://academic.oup.com/rev/article-pdf/24/4/454/5445388/rvv014.pdf},
}

@inproceedings{howison2011incentives,
author = {Howison, James and Herbsleb, James D.},
title = {Scientific software production: incentives and collaboration},
year = {2011},
isbn = {9781450305563},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1958824.1958904},
doi = {10.1145/1958824.1958904},
abstract = {Software plays an increasingly critical role in science, including data analysis, simulations, and managing workflows. Unlike other technologies supporting science, software can be copied and distributed at essentially no cost, potentially opening the door to unprecedented levels of sharing and collaborative innovation. Yet we do not have a clear picture of how software development for science fits into the day-to-day practice of science, or how well the methods and incentives of its production facilitate realization of this potential. We report the results of a multiple-case study of software development in three fields: high energy physics, structural biology, and microbiology. In each case, we identify a typical publication, and use qualitative methods to explore the production of the software used in the science represented by the publication. We identify several different production systems, characterized primarily by differences in incentive structures. We identify ways in which incentives are matched and mismatched with the needs of the science fields, especially with respect to collaboration.},
booktitle = {Proceedings of the ACM 2011 Conference on Computer Supported Cooperative Work},
pages = {513–522},
numpages = {10},
keywords = {scholarly publishing, scientific software, software development, software ecosystem},
location = {Hangzhou, China},
series = {CSCW '11}
}

@misc{muna2016astropyproblem,
      title={The Astropy Problem}, 
      author={Demitri Muna and Michael Alexander and Alice Allen and Richard Ashley and Daniel Asmus and Ruyman Azzollini and Michele Bannister and Rachael Beaton and Andrew Benson and G. Bruce Berriman and Maciej Bilicki and Peter Boyce and Joanna Bridge and Jan Cami and Eryn Cangi and Xian Chen and Nicholas Christiny and Christopher Clark and Michelle Collins and Johan Comparat and Neil Cook and Darren Croton and Isak Delberth Davids and Éric Depagne and John Donor and Leonardo A. dos Santos and Stephanie Douglas and Alan Du and Meredith Durbin and Dawn Erb and Daniel Faes and J. G. Fernández-Trincado and Anthony Foley and Sotiria Fotopoulou and Søren Frimann and Peter Frinchaboy and Rafael Garcia-Dias and Artur Gawryszczak and Elizabeth George and Sebastian Gonzalez and Karl Gordon and Nicholas Gorgone and Catherine Gosmeyer and Katie Grasha and Perry Greenfield and Rebekka Grellmann and James Guillochon and Mark Gurwell and Marcel Haas and Alex Hagen and Daryl Haggard and Tim Haines and Patrick Hall and Wojciech Hellwing and Edmund Christian Herenz and Samuel Hinton and Renee Hlozek and John Hoffman and Derek Holman and Benne Willem Holwerda and Anthony Horton and Cameron Hummels and Daniel Jacobs and Jens Juel Jensen and David Jones and Arna Karick and Luke Kelley and Matthew Kenworthy and Ben Kitchener and Dominik Klaes and Saul Kohn and Piotr Konorski and Coleman Krawczyk and Kyler Kuehn and Teet Kuutma and Michael T. Lam and Richard Lane and Jochen Liske and Diego Lopez-Camara and Katherine Mack and Sam Mangham and Qingqing Mao and David J. E. Marsh and Cecilia Mateu and Loïc Maurin and James McCormac and Ivelina Momcheva and Hektor Monteiro and Michael Mueller and Roberto Munoz and Rohan Naidu and Nicholas Nelson and Christian Nitschelm and Chris North and Juan Nunez-Iglesias and Sara Ogaz and Russell Owen and John Parejko and Vera Patrício and Joshua Pepper and Marshall Perrin and Timothy Pickering and Jennifer Piscionere and Richard Pogge and Radek Poleski and Alkistis Pourtsidou and Adrian M. Price-Whelan and Meredith L. Rawls and Shaun Read and Glen Rees and Hanno Rein and Thomas Rice and Signe Riemer-Sørensen and Naum Rusomarov and Sebastian F. Sanchez and Miguel Santander-García and Gal Sarid and William Schoenell and Aleks Scholz and Robert L. Schuhmann and William Schuster and Peter Scicluna and Marja Seidel and Lijing Shao and Pranav Sharma and Aleksandar Shulevski and David Shupe and Cristóbal Sifón and Brooke Simmons and Manodeep Sinha and Ian Skillen and Bjoern Soergel and Thomas Spriggs and Sundar Srinivasan and Abigail Stevens and Ole Streicher and Eric Suchyta and Joshua Tan and O. Grace Telford and Romain Thomas and Chiara Tonini and Grant Tremblay and Sarah Tuttle and Tanya Urrutia and Sam Vaughan and Miguel Verdugo and Alexander Wagner and Josh Walawender and Andrew Wetzel and Kyle Willett and Peter K. G. Williams and Guang Yang and Guangtun Zhu and Andrea Zonca},
      year={2016},
      eprint={1610.03159},
      archivePrefix={arXiv},
      primaryClass={astro-ph.IM},
      url={https://arxiv.org/abs/1610.03159}, 
}

@misc{hasselbring2024researchsoftwarecategories,
      title={Toward Research Software Categories}, 
      author={Wilhelm Hasselbring and Stephan Druskat and Jan Bernoth and Philine Betker and Michael Felderer and Stephan Ferenz and Anna-Lena Lamprecht and Jan Linxweiler and Bernhard Rumpe},
      year={2024},
      eprint={2404.14364},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2404.14364}, 
}

@inproceedings{krafczyk2019scientificreproducbility,
author = {Krafczyk, Matthew and Shi, August and Bhaskar, Adhithya and Marinov, Darko and Stodden, Victoria},
title = {Scientific Tests and Continuous Integration Strategies to Enhance Reproducibility in the Scientific Software Context},
year = {2019},
isbn = {9781450367561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3322790.3330595},
doi = {10.1145/3322790.3330595},
abstract = {Continuous integration (CI) is a well-established technique in commercial and open-source software projects, although not routinely used in scientific publishing. In the scientific software context, CI can serve two functions to increase reproducibility of scientific results: providing an established platform for testing the reproducibility of these results, and demonstrating to other scientists how the code and data generate the published results. We explore scientific software testing and CI strategies using two articles published in the areas of applied mathematics and computational physics. We discuss lessons learned from reproducing these articles as well as examine and discuss existing tests. We introduce the notion of a "scientific test" as one that produces computational results from a published article. We then consider full result reproduction within a CI environment. If authors find their work too time or resource intensive to easily adapt to a CI context, we recommend the inclusion of results from reduced versions of their work (e.g., run at lower resolution, with shorter time scales, with smaller data sets) alongside their primary results within their article. While these smaller versions may be less interesting scientifically, they can serve to verify that published code and data are working properly. We demonstrate such reduction tests on the two articles studied.},
booktitle = {Proceedings of the 2nd International Workshop on Practical Reproducible Evaluation of Computer Systems},
pages = {23–28},
numpages = {6},
keywords = {continuous integration, reproducibility, scientific software, software reliability, software testing},
location = {Phoenix, AZ, USA},
series = {P-RECS '19}
}

@article{Trisovic2021ALS,
  title={A large-scale study on research code quality and execution},
  author={Ana Trisovic and Matthew K. Lau and Thomas Pasquier and Merc{\`e} Crosas},
  journal={Scientific Data},
  year={2021},
  volume={9},
}

@article{ram2013git,
  title={Git can facilitate greater reproducibility and increased transparency in science},
  author={Ram, Karthik},
  journal={Source code for biology and medicine},
  volume={8},
  pages={1--8},
  year={2013},
  publisher={Springer}
}

@article{Cao2023TheRO,
  title={The Rise of Open Science: Tracking the Evolution and Perceived Value of Data and Methods Link-Sharing Practices},
  author={Hancheng Cao and Jesse Dodge and Kyle Lo and Daniel A. McFarland and Lucy Lu Wang},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.03193},
}

@article{trujillo2022penumbra,
  title={The penumbra of open source: projects outside of centralized platforms are longer maintained, more academic and more collaborative},
  author={Trujillo, Milo Z and H{\'e}bert-Dufresne, Laurent and Bagrow, James},
  journal={EPJ Data Science},
  volume={11},
  number={1},
  pages={31},
  year={2022},
  publisher={Springer Berlin Heidelberg}
}

@article{Katz2020RecognizingTV,
  title={Recognizing the value of software: a software citation guide},
  author={Daniel S. Katz and Neil P. Chue Hong and Tim Clark and August Muench and Shelley Stall and Daina R. Bouquin and Matthew Cannon and Scott C. Edmunds and Telli Faez and Patricia Feeney and Martin Fenner and Michael Friedman and Gerry Grenier and Melissa Harrison and Joerg Heber and Adam Leary and Catriona J. MacCallum and Hollydawn Murray and {\'E}rika Pastrana and Kath Perry and Douglas C. Schuster and Martina Stockhause and Jake S. Yeston},
  journal={F1000Research},
  year={2020},
  volume={9},
}

@article{Merow2023BetterIA,
  title={Better incentives are needed to reward academic software development},
  author={Cory Merow and Brad L. Boyle and Brian J. Enquist and Xiao Feng and Jamie M. Kass and Brian Salvin Maitner and Brian McGill and Hannah L. Owens and Daniel S. Park and Andrea Paz and Gonzalo E. Pinilla‐Buitrago and Mark C. Urban and Sara Varela and Adam M Wilson},
  journal={Nature Ecology \& Evolution},
  year={2023},
  volume={7},
  pages={626-627},
}

@article{Westner2024CyclingOT,
  title={Cycling on the Freeway: The Perilous State of Open Source Neuroscience Software},
  author={Britta U. Westner and Daniel R. McCloy and Eric Larson and Alexandre Gramfort and Daniel S. Katz and Arfon M. Smith and invited co-signees},
  journal={ArXiv},
  year={2024},
  volume={abs/2403.19394},
}

@article{Carver2022ASO,
  title={A survey of the state of the practice for research software in the United States},
  author={Jeffrey C. Carver and Nic Weber and Karthik Ram and Sandra Gesing and Daniel S. Katz},
  journal={PeerJ Computer Science},
  year={2022},
  volume={8},
}

@software{olivier_philippe_2019_2585783,
  author       = {Olivier Philippe and
                  Martin Hammitzsch and
                  Stephan Janosch and
                  Anelda van der Walt and
                  Ben van Werkhoven and
                  Simon Hettrick and
                  Daniel S. Katz and
                  Katrin Leinweber and
                  Sandra Gesing and
                  Stephan Druskat and
                  Scott Henwood and
                  Nicholas R. May and
                  Nooriyah P. Lohani and
                  Manodeep Sinha},
  title        = {softwaresaved/international-survey: Public release
                   for 2018 results
                  },
  month        = mar,
  year         = 2019,
  publisher    = {Zenodo},
  version      = {2018-v.1.0.2},
  doi          = {10.5281/zenodo.2585783},
  url          = {https://doi.org/10.5281/zenodo.2585783},
}

@article{Larivire2020InvestigatingTD,
  title={Investigating the division of scientific labor using the Contributor Roles Taxonomy (CRediT)},
  author={Vincent Larivi{\`e}re and David Pontille and Cassidy R. Sugimoto},
  journal={Quantitative Science Studies},
  year={2020},
  pages={1-18},
}

@book{biagioli2014scientific,
  title={Scientific authorship: Credit and intellectual property in science},
  author={Biagioli, Mario and Galison, Peter},
  year={2014},
  publisher={Routledge}
}

@article{brand2015beyond,
  title={Beyond authorship: Attribution, contribution, collaboration, and credit.},
  author={Brand, Amy and Allen, Liz and Altman, Micah and Hlava, Marjorie and Scott, Jo},
  journal={Learned Publishing},
  volume={28},
  number={2},
  year={2015}
}

@article{lariviere2016contributorship,
author = {Vincent Larivière and Nadine Desrochers and Benoît Macaluso and Philippe Mongeon and Adèle Paul-Hus and Cassidy R Sugimoto},
title ={Contributorship and division of labor in knowledge production},
journal = {Social Studies of Science},
volume = {46},
number = {3},
pages = {417-435},
year = {2016},
doi = {10.1177/0306312716650046},
    note ={PMID: 28948891},
URL = {https://doi.org/10.1177/0306312716650046},
eprint = {https://doi.org/10.1177/0306312716650046}
}


@article{HAEUSSLER2013688,
title = {Credit where credit is due? The impact of project contributions and social factors on authorship and inventorship},
journal = {Research Policy},
volume = {42},
number = {3},
pages = {688-703},
year = {2013},
issn = {0048-7333},
doi = {10.1016/j.respol.2012.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S0048733312002259},
author = {Carolin Haeussler and Henry Sauermann},
keywords = {Guest authorship, Ghost authorship, Attribution, Social status, Project contributions, Patent–paper-pairs},
abstract = {We examine the extent to which different types of substantive project contributions as well as social factors predict whether a scientist is named as author on a paper and inventor on a patent resulting from the same project. Using unique survey data from over 2000 life scientists, we find that the predictors of authorship differ from those of inventorship. A wider range of project contributions may result in authorship, and social factors appear to play a larger role in authorship decisions than in inventorship decisions. We also find evidence that project contributions and social factors interact in predicting authorship, suggesting that the two sets of factors should be considered jointly rather than seen as independent determinants of attribution. In addition to providing novel insights into the functioning of the authorship and inventorship system, our results have important implications for administrators, managers, and policy makers, as well as for innovation scholars who often rely on patents and publications as measures of scientists’ performance.}
}

@article{gotzsche2007ghost,
  title={Ghost authorship in industry-initiated randomised trials},
  author={G{\o}tzsche, Peter C and Hr{\'o}bjartsson, Asbj{\o}rn and Johansen, Helle Krogh and Haahr, Mette T and Altman, Douglas G and Chan, An-Wen},
  journal={PLoS medicine},
  volume={4},
  number={1},
  pages={e19},
  year={2007},
  publisher={Public Library of Science San Francisco, USA}
}

@article{chaoqun2021gendered,
author = {Chaoqun Ni  and Elise Smith  and Haimiao Yuan  and Vincent Larivière  and Cassidy R. Sugimoto },
title = {The gendered nature of authorship},
journal = {Science Advances},
volume = {7},
number = {36},
pages = {eabe4639},
year = {2021},
doi = {10.1126/sciadv.abe4639},
URL = {https://www.science.org/doi/abs/10.1126/sciadv.abe4639},
eprint = {https://www.science.org/doi/pdf/10.1126/sciadv.abe4639},
abstract = {Authorship is rife with injustice and malpractice, with women expressing concerns about the fair attribution of credit. Authorship is the primary form of symbolic capital in science. Despite this, authorship is rife with injustice and malpractice, with women expressing concerns regarding the fair attribution of credit. Based on an international survey, we examine gendered practices in authorship communication, disagreement, and fairness. Our results demonstrate that women were more likely to experience authorship disagreements and experience them more often. Their contributions to research papers were more often devalued by both men and women. Women were more likely to discuss authorship with coauthors at the beginning of the project, whereas men were more likely to determine authorship unilaterally at the end. Women perceived that they received less credit than deserved, while men reported the opposite. This devaluation of women’s work in science creates cumulative disadvantages in scientific careers. Open discussion regarding power dynamics related to gender is necessary to develop more equitable distribution of credit for scientific labor.}}

@article{Li2020DeepEM,
  title={Deep entity matching with pre-trained language models},
  author={Yuliang Li and Jinfeng Li and Yoshihiko Suhara and AnHai Doan and Wang Chiew Tan},
  journal={Proceedings of the VLDB Endowment},
  year={2020},
  volume={14},
  pages={50 - 60},
}

@inproceedings{Brunner2020EntityMW,
  title={Entity Matching with Transformer Architectures - A Step Forward in Data Integration},
  author={Ursin Brunner and Kurt Stockinger},
  booktitle={International Conference on Extending Database Technology},
  year={2020},
}

@article{haak2012orcid,
  title={ORCID: a system to uniquely identify researchers},
  author={Haak, Laurel L and Fenner, Martin and Paglione, Laura and Pentz, Ed and Ratner, Howard},
  journal={Learned publishing},
  volume={25},
  number={4},
  pages={259--264},
  year={2012},
  publisher={Wiley Online Library}
}
