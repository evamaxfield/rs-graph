[
  {
    "objectID": "qss-code-authors.html",
    "href": "qss-code-authors.html",
    "title": "Code Contribution and Scientific Authorship",
    "section": "",
    "text": "Software and collaboration are two essential ingredients in contemporary science but are often investigated separately: Collaboration is often studied using digital trace data or bibliometric metadata to understand, for example, who works with whom (Newman, Herrera, and Parente 2014), who receives what credit (Shen and Barabási 2014), and to what effect (Ahuja 2000). Similarly, scientific software is often quantitatively studied by extracting mentions (Du et al. 2021) or formal citations (Du et al. 2022) that connect scientists and software to specific impact metrics (i.e., citation counts)(Park and Wolfram 2019).\nIn the following paper, we unite these two literatures to better understand how the development of software affects the allocation of credit in collaborative settings. In doing so, we take advantage of advances in natural language processing and machine learning to develop unique data sources that complement traditional bibliometric indicators. We first build a dataset of 138,596 paired research articles and code repositories—giving us a verified connection between the software that supports new knowledge claims made in the published literature. Next, we develop a predictive model that matches the authors of a research article and the developer accounts of a linked source code repository—giving us a unique way to understand the allocation of credit for software contributions. We apply the predictive model to our dataset and then use commit histories from the public software repositories’ to test three hypotheses.\nWe identify several patterns that distinguish code contributions from formal scientific recognition. We show that ~28.6% (n=6,529) of articles have non-author code-contributors—individuals who may have helped create the software but received no formal authorship credit. We find that code-contributing authors are associated with a modest increase in article-level impact metrics (on average, a ~5.1% increase in citations per code-contributing author), but these effects become statistically non-significant when controlling for domain, article type, and open access status. First authors are significantly more likely to be code contributors than other author positions across all conditions tested. We additionally find a negative relationship between coding frequency and scholarly impact: authors who contribute code more frequently show progressively lower h-indices than their non-coding peers, a pattern that persists when controlling for publication count, and author’s most common author position, domain, and article type.\nThe remainder of this paper proceeds as follows. First, we review related work regarding software development and the recognition of code contributors in scientific credit systems. In doing so, we motivate three specific hypotheses that guide our investigation. Next, we detail our data and methods, describing how we linked article-repository pairs, trained and evaluated a predictive model for entity matching, and applied this model across each article-repository pair. We then present our analysis, focusing on article-level dynamics before moving to individual-level patterns, formally accepting or rejecting each hypothesis based on our findings. We conclude by discussing the results, limitations of our work, and areas for future improvement."
  },
  {
    "objectID": "qss-code-authors.html#h1-research-team-composition-and-scientific-attention",
    "href": "qss-code-authors.html#h1-research-team-composition-and-scientific-attention",
    "title": "Code Contribution and Scientific Authorship",
    "section": "2.1 H1: Research Team Composition and Scientific Attention",
    "text": "2.1 H1: Research Team Composition and Scientific Attention\nIn collaborative settings, experimental and theoretical research tends to receive more citations than methods-focused contributions, except when methods represent foundational shifts in a field (Aksnes 2006; Liu, Zhang, and Li 2023; Chen et al. 2024). Software is often positioned as a methodological contribution and, as a result, can be found in some of the highest-cited papers of the 21st century (Jin et al. 2015; Hampton et al. 2013; Hasselbring et al. 2024).\nPrior work also establishes a positive relationship between team size and citation count, where larger and more diverse teams typically produce higher-impact scientific work (Franceschet and Costantini 2010; Larivière et al. 2014). Similar research in empirical software studies found that larger development teams tend to create more reliable software with fewer defects Herbsleb and Mockus (2003), though this comes at the expense of slower development cycles. These findings suggest that team size may be particularly important in scientific software development, where technical robustness and scientific innovation are crucial (Milewicz and Raybourn 2018).\nWe argue that the unique characteristics of scientific software development—including implementing novel algorithms, requiring deep domain knowledge, and an increased emphasis on reproducibility (Muna et al. 2016; Howison and Herbsleb 2013)—make team composition especially relevant. Development in organized teams may enhance scientific impact through multiple mechanisms: they can produce more robust and generalizable software tools for methodological contributions while enabling more sophisticated computational analyses and larger-scale data processing for experimental work.\nGiven these patterns in team dynamics, software development practices, and citation practices in collaborative research, we propose that:\nH1: The number of individuals contributing code to a publication’s associated repository positively correlates with the article’s citation count."
  },
  {
    "objectID": "qss-code-authors.html#h2-author-roles-and-technical-contributions",
    "href": "qss-code-authors.html#h2-author-roles-and-technical-contributions",
    "title": "Code Contribution and Scientific Authorship",
    "section": "2.2 H2: Author Roles and Technical Contributions",
    "text": "2.2 H2: Author Roles and Technical Contributions\nAuthor positions in scientific publications signal specific roles and responsibilities (Shen and Barabási 2014), a relationship that can be contemporarily studied through contribution role taxonomies like CRediT (Larivière et al. 2016). Analyses of contribution patterns consistently show that software development, data analysis, and visualization tasks typically fall to first authors (Larivière et al. 2016; Júnior et al. 2016; Larivière, Pontille, and Sugimoto 2020; Sauermann and Haeussler 2017). While these contribution patterns are often inferred from self-reported roles (e.g., CRediT) or disciplinary conventions, the increasing linkage of publications to version-controlled software repositories provides a novel window into the actual coding contributions associated with scientific papers. Given the established tendency for first authors to undertake primary research execution, including technical development, we anticipate that this pattern will be directly observable via the commit histories of scientific software repositories. Contribution records from source code repositories provide a unique method to verify these established contribution patterns. Given prior findings about the distribution of technical responsibilities within research teams, we expect these repository records to reflect similar patterns of engagement with software development:\nH2a: First authors have higher code contribution rates than authors in other positions.\nFurthermore, studies using taxonomies like CRediT reveal that first authors and corresponding authors, while occasionally the same individual, often take on distinct responsibilities (Birnbaum et al. 2023). Corresponding authors traditionally bear responsibility for the integrity, archiving, and long-term availability of research artifacts and underlying data (Teunis, Nota, and Schwab 2015; Sauermann and Haeussler 2017; Silva et al. 2013; Bhandari et al. 2014). As software becomes an increasingly critical, complex, and integral research artifact in many disciplines, it logically follows that stewardship of these software components—encompassing their maintenance, documentation, and accessibility—should align with the responsibilities of being a corresponding author. Given prior findings about the distribution of responsibilities within research teams and the expectation of stewardship of created artifacts, we expect repository records to reflect similar patterns of engagement with software development:\nH2b: Corresponding authors have higher code contribution rates than non-corresponding authors."
  },
  {
    "objectID": "qss-code-authors.html#h3-code-contribution-and-individual-scientific-impact",
    "href": "qss-code-authors.html#h3-code-contribution-and-individual-scientific-impact",
    "title": "Code Contribution and Scientific Authorship",
    "section": "2.3 H3: Code Contribution and Individual Scientific Impact",
    "text": "2.3 H3: Code Contribution and Individual Scientific Impact\nDespite the increasingly central role of software in science, researchers who dedicate significant effort to its development often face systemic hurdles in receiving formal scientific credit. Their contributions may be relegated to acknowledgment sections rather than accorded authorship, and the scholarly practice of software citation remains inconsistent, frequently undervaluing crucial maintenance and extension work Weber and Thomer (2014). The h-index, a widely used proxy for an individual’s cumulative scientific impact, is derived from an individual’s record of formally authored publications and the citations these publications receive (Hirsch 2005). Consequently, if substantial time and intellectual effort are invested in software development that does not consistently translate into formal authorship on associated research papers, or if the software outputs themselves are not robustly and formally cited in a way that accrues to the individual developer, then the primary activities that build a researcher’s h-index are effectively diminished or bypassed.\nThis creates a structural misalignment where contributions essential for scientific advancement (i.e., software development and maintenance) may not adequately capture and could even detract from time spent on activities that bolster traditional bibliometric indicators of individual success. While collaborative software development may yield short-term benefits through increased citations on individual papers (as suggested by H1), researchers specializing in code development may face long-term career disadvantages as their expertise becomes increasingly divorced from traditional publication pathways that drive academic recognition and advancement. Researchers who frequently contribute code and software might, despite their contribution to science, exhibit lower h-indices compared to peers whose outputs more closely align with traditional publication and citation pathways. Based on these challenges in the recognition and citation of software contributions and their potential impact on h-index accumulation, we hypothesize:\nH3: The frequency with which individual researchers contribute code to their research projects negatively correlates with their h-index."
  },
  {
    "objectID": "qss-code-authors.html#software-development-dynamics-within-research-teams",
    "href": "qss-code-authors.html#software-development-dynamics-within-research-teams",
    "title": "Code Contribution and Scientific Authorship",
    "section": "4.1 Software Development Dynamics Within Research Teams",
    "text": "4.1 Software Development Dynamics Within Research Teams\nUnderstanding the composition and dynamics of software development teams provides essential context for analyzing how code contributions relate to scientific recognition and impact. To ensure reliable analysis, we focus on a subset of our article-repository pairs that meet several filtering conditions. First, we require that each article-repository pair have at least one citation, which helps ensure the research has received a basic level of engagement from the scientific community. Next, we require that repository commit activity stop before 90 days past the article publication date. Disallowing long-term projects ensures we do not include projects that may add additional code contributors later while still allowing a grace period during which developers can update repositories with additional documentation and publication information. We then subset the data to only include article-repository pairs with research teams of typical size by removing those with fewer than three authors and more than 12 authors, the 97th percentile for research team size. Finally, we filter out any author-developer pairs associated with these projects with predictive model confidence of less than 0.97 to ensure that we only include high-confidence matches2. This filtering process results in a dataset of 22,804 article-repository pairs. A table with the counts of article-repository pairs, authors, and developers by data sources, domains, document types, and access status for this filtered dataset is shown in Table 5.\n2 Figure 3 shows the distribution of predictive model confidence scores for author-developer pairs to justify this threshold. We chose the 0.97 threshold to ensure that we only include high-confidence matches while retaining a large proportion of the data (87,175 author-developer pairs) as only 2,911 author-developer-account pairs have a confidence less than 0.97 in the whole unfiltered dataset.3 We define code-contributing authors as those who have at least one code commit to the repository associated with the article-repository pair.Within this filtered dataset, we categorized individuals into three groups: code-contributing authors (CC-A) who both authored papers and code to associated repositories3, non-code-contributing authors (NCC-A) who authored papers but showed no evidence of code contributions, and code-contributing non-authors (CC-NA) who contributed code but received no authorship recognition. This categorization revealed that papers in our dataset typically have 4.9 ± 1.9 total authors, with 1.0 ± 0.7 code-contributing authors and 3.9 ± 2.0 non-code-contributing authors. Beyond the author list, papers averaged 0.4 ± 1.7 code-contributing non-authors. Table 2 details these distributions by domain, article type, and open access status.\nPerhaps most striking is our finding that 6,529 papers (~28.6%) have at least one code contributor who was not matched to any article author. Within this substantial subset of papers, we found an average of 1.6 ± 2.8 un-matched code contributors per paper. To better understand the nature of these developers contributions, we conducted an analysis using a random sample of 200 code-contributing non-authors (see Section 7.4 for complete methodology).\nOur analysis revealed three distinct groups: ~39% (n=78) represent true non-authors (individuals who likely should not be matched to an author), ~30.5% (n=61) appear to be missed classifications (individuals who likely should have been matched to authors), and the remaining ~30.5% (n=61) were unclear due to limited profile information. The majority of contributors across all groups made code changes rather than just documentation updates. Among true non-authors, ~77.6% (n=59) contributed code, with the top 25th percentile of these contributors contributing ~10.7% of total repository commits and ~14.4% of absolute code changes. The unclear cases showed substantially higher contribution levels— many were repository owners with extensive engagement. Yet, even among the non-owners, the median contributor accounted for ~34.6% of repository commits and ~12.7% of absolute changes. Due to the limited size of our sample (n=200 code-contributing non-authors), we do not generalize these findings to the entire population but note that they describe the diverse nature of code contribution patterns from potentially unacknowledged contributors.\nThese findings reveal a more complex dynamic between software development and authorship recognition than previously documented. While our finding that each paper averages only a single code-contributing author aligns with previous research showing technical tasks typically fall to first authors (Larivière, Pontille, and Sugimoto 2020), the substantial contributions made by unrecognized contributors suggest systematic gaps in how scientific software development work is credited.\n\n\n\n\nTable 2: Mean and Standard Deviation of Non-Code-Contributing Authors (NCC-A), Code-Contributing Authors (CC-A), and Code-Contributing Non-Authors (CC-NA) Research Team Members by Domain, Article Type, and Open Access Status. Only includes research teams from article-repository pairs with a most recent commit no later than 90 days after publication and excludes research teams in the top 3% of total author sizes.\n\n\n\n\n\n\nControl\nSubset\nTotal Authors\nNCC-A\nCC-A\nCC-NA\n\n\nOA Status\nClosed\n5.1 ± 1.9\n4.1 ± 1.9\n1.0 ± 0.7\n0.5 ± 2.0\n\n\nOpen\n4.9 ± 1.9\n3.9 ± 2.0\n1.0 ± 0.7\n0.4 ± 1.6\n\n\nDomain\nHealth Sciences\n6.1 ± 2.5\n5.1 ± 2.6\n0.9 ± 0.6\n0.4 ± 1.2\n\n\nLife Sciences\n5.2 ± 2.1\n4.2 ± 2.2\n1.0 ± 0.7\n0.4 ± 1.2\n\n\nPhysical Sciences\n4.8 ± 1.8\n3.8 ± 1.9\n1.0 ± 0.7\n0.5 ± 1.8\n\n\nSocial Sciences\n4.5 ± 1.7\n3.5 ± 1.8\n1.1 ± 0.7\n0.3 ± 1.1\n\n\nArticle Type\npreprint\n4.8 ± 1.8\n3.8 ± 1.9\n1.0 ± 0.7\n0.6 ± 2.1\n\n\nresearch article\n4.9 ± 1.9\n3.9 ± 2.0\n1.0 ± 0.7\n0.4 ± 1.6\n\n\nsoftware article\n4.7 ± 1.9\n3.2 ± 1.9\n1.5 ± 1.4\n0.9 ± 1.1\n\n\nOverall\n\n4.9 ± 1.9\n3.9 ± 2.0\n1.0 ± 0.7\n0.4 ± 1.7\n\n\n\n\n\n\n\n\nWhen examining these patterns over time and across different team sizes (Figure 1), we found that the number of code-contributing authors and unrecognized contributors has remained relatively stable. This stability over time suggests that while the exclusion of code contributors from authorship is not worsening, it represents a persistent feature of scientific software development rather than a historical artifact or transition period in research practices. Similarly, the number of code-contributing non-authors remains constant even as team size grows, indicating that larger research teams do not necessarily adopt more inclusive authorship practices for code contributors despite representing broader collaborative efforts.\n\n\n\n\n\n\n\n\nFigure 1: Average number of contributors per article, by contribution type, along with A) the year the article was published and B) the total number of authors included in the article. Only includes research teams from article-repository pairs with a most recent commit no later than 90 days after publication and excludes research teams in the top 3% of total author sizes for publication years with 50 or more articles. Shaded areas show the 95% confidence interval for the mean.\n\n\n\n\n\n\n4.1.1 Modeling Article Citations\nBuilding upon previous work examining the effects of team size and team diversity on scientific impact and software quality (see Section 2), we investigate how the number of code contributors within a research team may be associated with an article’s research impact. We hypothesized that more code contributors might signal greater technical complexity in research, which may be associated with higher citation counts as the community builds upon more technically sophisticated works.\nUsing our filtered dataset of article-repository pairs (Table 5), we conducted multiple generalized linear regression analyses to examine these relationships while controlling for various factors. Without controlling for domain, open access, or article type differences (Table 9), our analysis revealed a modest positive association between the number of code-contributing authors and article citations, with each code-contributing author associated with, on average, a ~5.1% increase in article citations (p &lt; 0.001).\nWhen controlling for article type (Table 12), we observed divergent patterns between preprints and research articles. For preprints, each code-contributing non-author was associated with a statistically significant ~3.2% decrease in citations (p &lt; 0.005). In contrast, research articles showed more positive associations: we found a significant positive relationship between code-contributing authors and citations (p &lt; 0.001), though we cannot estimate the precise magnitude due to the non-significant main effect in the model. Additionally, each code-contributing non-author was associated with a ~0.1% increase in expected citations for research articles (p &lt; 0.001).\nBased on these findings, we partially accept our hypothesis (H1) that “the number of individuals contributing code to a publication’s associated repository positively correlates with the article’s citation count.” Several important nuances qualify this acceptance: the relationship is statistically significant but modest in magnitude and differs substantially between research articles (positive association) and preprints (negative association for non-author code contributors). These variations suggest that the relationship between code contributions and citation impact is context-dependent and more complex than initially hypothesized."
  },
  {
    "objectID": "qss-code-authors.html#characteristics-of-scientific-code-contributors",
    "href": "qss-code-authors.html#characteristics-of-scientific-code-contributors",
    "title": "Code Contribution and Scientific Authorship",
    "section": "4.2 Characteristics of Scientific Code Contributors",
    "text": "4.2 Characteristics of Scientific Code Contributors\n\n4.2.1 Author Positions of Code Contributing Authors\nBuilding upon previous work examining the relationship between authorship position and research contributions, we investigate how author position may relate to code contribution patterns. We hypothesized that first authors, traditionally contributing the bulk of intellectual and experimental work, are most likely to contribute code to a project. In contrast, middle and last authors often provide oversight and guidance and would be less likely to contribute code.\nTo analyze these patterns within our previously filtered dataset of article-repository pairs (Table 5), we conducted Chi-square tests of independence between author position and code contribution status. These tests revealed significant associations between author position and likelihood of code contribution overall and when controlling for research domain, article type, and open access status (all p &lt; 0.01), indicating that the proportion of authors contributing code differs significantly based on author position. Following these significant associations, we examined the specific proportions across positions (Table 13): 68.6% of first authors contributed code to their projects, compared to only 9.3% of middle authors and 7.6% of last authors. The differences in these proportions remained statistically significant across all tested scenarios, regardless of research domain, article type, or open access status.\nBased on these findings, we accept our hypothesis (H2a) that “first authors have higher code contribution rates than authors in other positions.” The data demonstrates that the proportion of first authors who contribute code (68.6%) is significantly higher than the proportion of both middle authors (9.3%) and last authors (7.6%). This relationship remains robust and statistically significant across all tested conditions, including variations in research domain, article type, and open access status, indicating a fundamental connection between authorship position and technical contribution in scientific research.\n\n\n4.2.2 Corresponding Status of Code Contributing Authors\nBuilding upon our analysis of author position, we next examine how corresponding author status relates to code contribution patterns. We hypothesized that corresponding authors, who traditionally maintain research artifacts and serve as primary points of contact, would be more likely to contribute code compared to non-corresponding authors, as this role often involves responsibility for project resources and materials.\nTo analyze these relationships within our filtered dataset of article-repository pairs, we conducted Chi-square tests of independence between corresponding author status and code contribution status. Our analysis revealed patterns contrary to our initial hypothesis. The proportion of code contributors was low among both groups, with only 27.2% of corresponding authors and 20.0% of non-corresponding authors contributing code to their projects. Further examination (Table 14) showed that this pattern holds across nearly all conditions, with only one exception: corresponding authors in closed-access publications showed no significant difference in their proportion of code contributors. However, this was tested with a sample of less than 200 authors.\nBased on these findings, we reject our hypothesis (H2b) that “corresponding authors have higher code contribution rates than non-corresponding authors.” Contrary to our expectations, our analysis revealed that the proportion of code contributors among corresponding authors (27.2%) did not significantly differ from the proportion among non-corresponding authors (20.0%). This pattern of similar proportions remained consistent across most studied conditions, with a single exception in closed-access publications which we believe is due to the relatively small sample size available for testing (n=194).\n\n\n4.2.3 Modeling Author H-Index\nBuilding upon previous work examining career implications for researchers who prioritize software development (see Section 2), we investigated how varying levels of code contribution relate to scholarly impact through h-index metrics. To ensure a robust analysis, we applied several key data filtering steps. We only included researchers with at least three publications in our dataset, removed those with more than three developer account associations, and used each researcher’s most common domain, article type, and author position, with ties broken by the most recent occurrence. We removed h-index outliers by excluding researchers below the bottom 3rd and above the top 97th percentiles. Finally, we removed any author-developer-account pairs with predictive model confidence of less than 0.97. Table 15 summarizes the number of researchers in each coding frequency group, categorized by author position, publication type, and research domain.\nWe categorized researchers’ coding contributions into mutually exclusive groups: non-coders (no code contributions), any coding (code contribution in less than half of article-repository pairs), majority coding (code contribution in at least half, but not all, article-repository pairs), and always coding (code contribution in every article-repository pair).\nFigure 2 shows the distribution of author h-indices across these coding frequency groups, grouped by author position, publication type, and research domain.\n\n\n\n\n\n\n\n\nFigure 2: Distribution of author h-index by coding frequency across three key publication factors. Results are grouped by each author’s most frequent: (1) position in publication bylines (first, middle, or last), (2) publication type (preprint, research article, or software article), and (3) research domain (Social Sciences, Physical Sciences, Health Sciences, or Life Sciences). Within each subplot, h-indices are divided by the author’s coding frequency: ‘none’ (no coding in any of their publications), ‘any’ (coding in at least one but fewer than half of their publications), ‘majority’ (coding in at least half but not all of their publications), and ‘always’ (coding in each of their publications). Authors are only included if they have three or more publications within our dataset and are associated with no more than three developer accounts, with each association having a predicted model confidence of at least 97%.\n\n\n\n\n\nUsing multiple generalized linear regressions, we find a consistent and statistically significant negative relationship between code contribution frequency and h-index across multiple analytical controls. Our initial uncontrolled analysis (Table 16) indicates increasingly adverse h-index effects as researcher coding frequency increases. Compared to non-coding authors, researchers were associated with progressively lower h-indices: occasional code contributors showed a ~27.3% lower h-index (p &lt; 0.001), majority code contributors demonstrated a ~53.5% lower h-index (p &lt; 0.001), and always coding authors exhibited a ~62.1% lower h-index (p &lt; 0.001).\nWhen controlling for author position (Table 17), we found a general pattern of reduced h-indices with increased code contribution, with one notable exception. Occasional coding first authors were associated with a ~14.9% higher h-index (p &lt; 0.001), while always coding first authors saw a ~21.6% reduction compared to non-coding first authors (p &lt; 0.001). For middle and last authors, the pattern was more consistently negative. Middle authors who occasionally coded showed a ~26.6% lower h-index (p &lt; 0.001), and those who always coded demonstrated a ~52.9% lower h-index (p &lt; 0.001). Similarly, last authors who occasionally coded experienced a ~13.1% lower h-index (p &lt; 0.001), with always coding authors showing a ~45.7% lower h-index (p &lt; 0.001).\nWhen controlling for research domain (Table 18), majority coding scientists showed significant h-index reductions across all domains. Health sciences researchers saw the most dramatic reduction at ~76.5% (p &lt; 0.001), followed by physical sciences at ~52.6% (p &lt; 0.001), social sciences at ~51.4% (p &lt; 0.001), and life sciences at ~47.1% (p &lt; 0.001).\nAnalyzing by common article type (Table 19) revealed similar patterns. For authors primarily publishing preprints, the h-index reductions were substantial: ~25.6% for occasional coding, ~53.5% for majority coding, and ~62.9% for always coding authors. Authors primarily publishing software articles showed slightly better but still significant reductions: ~33.1% for majority coding and ~33.0% for always coding authors.\nBased on these findings, we accept our hypothesis (H3) that “the frequency with which individual researchers contribute code to their research projects is negatively correlated with their h-index.” Our analysis demonstrates a clear and statistically significant negative relationship between coding frequency and scholarly impact as measured by the researcher’s h-index. This relationship was robust across multiple analytical controls, including author position, research domain, and article type. These results are particularly striking because our models include publication count as an input feature, suggesting that these h-index reductions persist even when accounting for total research output."
  },
  {
    "objectID": "qss-code-authors.html#limitations",
    "href": "qss-code-authors.html#limitations",
    "title": "Code Contribution and Scientific Authorship",
    "section": "5.1 Limitations",
    "text": "5.1 Limitations\nOur data collection approach introduces several methodological constraints to consider when interpreting these results. By focusing exclusively on GitHub repositories, we likely miss contributions stored on alternative platforms such as GitLab, Bitbucket, or institutional repositories, potentially skewing our understanding of contribution patterns. As Trujillo, Hébert-Dufresne, and Bagrow (2022), Cao et al. (2023), and Escamilla et al. (2022) have all noted, while GitHub is the predominate host of scientific software, significant portions of research code exist on other platforms. Additionally, our reliance on public repositories means we cannot account for private repositories or code that were never publicly shared, potentially underrepresenting sensitive research areas or proprietary methods.\nOur predictive modeling approach for matching authors with developer accounts presents additional limitations. The model’s performance can be affected by shorter names where less textual information is available for matching, potentially creating biases against researchers from cultures with shorter naming conventions. Organization accounts used for project management pose particular challenges for accurate matching, and while we implemented filtering mechanisms to minimize their impact, some misclassifications may persist. Furthermore, our approach may not capture all code contributors if multiple individuals developed code but only one uploaded it to a repository—creating attribution artifacts that may systematically underrepresent specific contributors, particularly junior researchers or technical staff who may not have direct repository access.\nOur analytical approach required substantial data filtering to ensure reliable results, introducing potential selection biases in our sample. By focusing on article-repository pairs with commit activity no later than 90 days past the date of article publication and at least three authors and less than 12 authors, we may have systematically excluded certain types of research projects, particularly those with extended development timelines or extensive collaborations. Our categorization of coding status (non-coder, any coding, majority coding, always coding) necessarily simplifies complex contribution patterns. It does not account for code contributions’ quality, complexity, or significance. Additionally, our reliance on OpenAlex metadata introduces certain limitations to our analysis. While OpenAlex provides good overall coverage, it lags behind proprietary databases in indexing references and citations. The lag in OpenAlex data may affect our citation-based analyses and the completeness of author metadata used in our study (Alperin et al. 2024)."
  },
  {
    "objectID": "qss-code-authors.html#future-work",
    "href": "qss-code-authors.html#future-work",
    "title": "Code Contribution and Scientific Authorship",
    "section": "5.2 Future Work",
    "text": "5.2 Future Work\nFuture technical improvements may enhance our understanding of the relationship between software development and scientific recognition systems. Expanding analysis beyond GitHub to include other code hosting platforms would provide a more comprehensive understanding of scientific software development practices across domains and institutional contexts. More sophisticated entity-matching techniques could improve author-developer account identification, particularly for cases with limited information or common names. Developing more nuanced measures and classifications of code contribution type, quality, and significance beyond binary contribution identification would better capture the true impact of technical contributions to research (as we have started to do in Section 7.1.2.2). These methodological advances would enable more precise tracking of how code contributions translate—or fail to translate—into formal scientific recognition, providing clearer evidence for policy interventions.\nOur findings point to several directions for future research on the changing nature of scientific labor and recognition. Longitudinal studies tracking how code contribution patterns affect career trajectories would provide valuable insights into the long-term impacts of the observed h-index disparities and whether these effects vary across career stages. Comparative analyses across different scientific domains could reveal discipline-specific norms and practices around software recognition, potentially identifying models that more equitably credit technical contributions. Qualitative studies examining how research teams make authorship decisions regarding code contributors would complement our quantitative findings by illuminating the social and organizational factors influencing recognition practices. Additionally, to better understand corresponding authors’ role in maintaining research artifacts, future work could remove the 90-day post-publication commit activity filter to examine long-term sustainability actions. However, this approach must address introducing contributors unrelated to the original paper.\nDespite their growing importance, the persistent underrecognition of software contributions suggests a need for structural interventions in how we conceptualize and reward scientific work. Building upon efforts like CRediT (Brand et al. 2015), future work should investigate potential policy changes to better align institutional incentives with the diverse spectrum of contributions that drive modern scientific progress. However, as the example of CRediT demonstrates, even well-intentioned taxonomies may reproduce existing hierarchies or create new forms of inequality if they fail to address underlying power dynamics in scientific communities. The challenge is not merely technical but social: creating recognition systems that simultaneously support innovation, ensure appropriate credit, maintain research integrity, and foster equitable participation in an increasingly computational scientific enterprise."
  },
  {
    "objectID": "qss-code-authors.html#extended-data-and-methods",
    "href": "qss-code-authors.html#extended-data-and-methods",
    "title": "Code Contribution and Scientific Authorship",
    "section": "7.1 Extended Data and Methods",
    "text": "7.1 Extended Data and Methods\n\n7.1.1 Building a Dataset of Linked Scientific Articles and Code Repositories\nThe increasing emphasis on research transparency has led many journals and platforms to require or recommend code and data sharing (Stodden, Guo, and Ma 2013; Sharma et al. 2024), creating traceable links between publications and code. These explicit links enable systematic study of both article-repository and author-developer account relationships (Hata et al. 2021; Kelley and Garijo 2021; Stankovski and Garijo 2024; Milewicz, Pinto, and Rodeghero 2019).\nOur dataset collection process leveraged four sources of linked scientific articles and code repositories, each with specific mechanisms for establishing these connections:\n\nPublic Library of Science (PLOS): We extracted repository links from PLOS articles’ mandatory data and code availability statements.\nJournal of Open Source Software (JOSS): JOSS requires explicit code repository submission and review as a core part of its publication process.\nSoftwareX: Similar to JOSS, SoftwareX mandates code repositories as a publication requirement.\nPapers with Code: This platform directly connects machine learning preprints with their implementations. We focus solely on the “official” article-repository relationships rather than the “unverified” or “unofficial” links.\n\nWe enriched these article-repository pairs with metadata from multiple sources to create a comprehensive and analyzable dataset. We utilized the Semantic Scholar API for DOI resolution to ensure we found the latest version of each article. This resolution step was particularly important when working with preprints, as journals may have published these papers since their inclusion in the Papers with Code dataset. Using Semantic Scholar, we successfully resolved ~56.3% (n=78,021) of all DOIs within our dataset4.\n4 Broken out by dataset source, we resolved ~2.1% (n=125) of all PLOS DOIs, ~4.0% (n=93) of all JOSS DOIs, ~0.0% (n=0) of all SoftwareX DOIs, and ~49.2% (n=63,817) of all Papers with Code (arXiv) DOIs.We then utilized the OpenAlex API to gather detailed publication metadata, including:\n\nPublication characteristics (open access status, domain, publication date)\nAuthor details (name, author position, corresponding author status)\nArticle- and individual-level metrics (citation counts, FWCI, h-index)\n\nSimilarly, the GitHub API provided comprehensive information for source code repositories:\n\nRepository metadata (name, description, programming languages, creation date)\nContributor details (username, display name, email)\nRepository-level metrics (star count, fork count, issue count)\n\n\n\n7.1.2 Developing a Predictive Model for Author-Developer Account Matching\n\n7.1.2.1 Annotated Dataset Creation\nCreating an accurate author-developer account matching model required high quality, labeled training data that reflects real-world identity-matching challenges. Exact matching on names or emails proved insufficient due to variations in formatting (e.g., “J. Doe” vs. “Jane Doe”), use of institutional versus personal email addresses, and incomplete information. However, author and developer account information often contain sufficient similarities for probabilistic matching, such as when author “Jane Doe” corresponds to username “jdoe” or “janedoe123.”\nTo efficiently build our training and evaluation dataset, we used JOSS articles as we believed they typically feature higher author-developer-account overlap, increasing positive match density. Our dataset creation process followed these steps:\n\nWe generated semantic embeddings for each developer account and author name using the multi-qa-MiniLM-L6-cos-v1 model from the Sentence Transformers Python library (Reimers and Gurevych 2019).\nWe calculated cosine similarity between all potential author-developer-account pairs for each article-repository pair.\nWe selected the three most similar authors for each developer account for annotation efficiency.\n\nFrom these generated author-developer-account pairs, we randomly selected 3,000 for classification by two independent annotators as either matches or non-matches, resolving disagreements through discussion and verification. The resulting dataset contains 451 (~15.0%) positive matches and 2,548 (~85.0%) negative matches, comprising 2,027 unique authors and 2,733 unique developer accounts.\nOur collected data for annotation confirmed that exact matching would be insufficient—only 2,191 (~80.2%) of developer accounts had associated display names and just 839 (~30.7%) had associated email addresses.\n\n\n7.1.2.2 Training and Evaluation\nOur training and evaluation methodology began with careful dataset preparation to prevent data leakage between training and test sets. To ensure complete separation of authors and developers, we randomly selected 10% of unique authors and 10% of unique developers, designating any pairs containing these selected entities for the test set. This entity-based splitting strategy resulted in 2,442 (~81.4%) pairs for training and 557 (~18.6%) pairs for testing.\nFor our predictive model, we evaluated three transformer-based architectures that have demonstrated strong performance in entity-matching tasks:\n\nDeBERTa-v3-base (He, Gao, and Chen 2021; He et al. 2021)\nmBERT (bert-base-multilingual-cased) (Devlin et al. 2018)\nDistilBERT (Sanh et al. 2019)\n\nWe systematically evaluated these base models across different combinations of developer-account features, ranging from using only the username to incorporating complete profile information (username, display name, and email address). We fine-tuned all models using the Adam optimizer with a linear learning rate of 1e-5 for training and a batch size of 8 for training and evaluation. Given the size of our dataset and the binary nature of our classification task, models were trained for a single epoch to prevent overfitting.\nWe evaluated model performance using precision, recall, and F1-score. This evaluation framework allowed us to directly compare model architectures and feature combinations while accounting for the balance between precision and recall in identifying correct matches.\nOur comprehensive model evaluation revealed that fine-tuning DeBERTa-v3-base (He, Gao, and Chen 2021) with developer username and display name as input features produces optimal performance for author-developer matching. This model configuration achieved a binary F1 score of 0.944, with an accuracy of 0.984, precision of 0.938, and recall of 0.95. Table 3 presents a complete comparison of model architectures and feature combinations.\n\n\n\n\nTable 3: Comparison of Models for Author-Developer-Account Matching\n\n\n\n\n\n\n\n\n\n\nOptional Feats.\nModel\nAccuracy\nPrecision\nRecall\nF1\n\n\n\n\n0\nname\ndeberta\n0.984\n0.938\n0.950\n0.944\n\n\n1\nname, email\nbert-multilingual\n0.984\n0.938\n0.950\n0.944\n\n\n2\nname, email\ndeberta\n0.982\n0.907\n0.975\n0.940\n\n\n3\nname\nbert-multilingual\n0.982\n0.938\n0.938\n0.938\n\n\n4\nname\ndistilbert\n0.978\n0.936\n0.912\n0.924\n\n\n5\nname, email\ndistilbert\n0.978\n0.936\n0.912\n0.924\n\n\n6\nemail\ndeberta\n0.957\n0.859\n0.838\n0.848\n\n\n7\nemail\nbert-multilingual\n0.950\n0.894\n0.738\n0.808\n\n\n8\nn/a\ndeberta\n0.946\n0.847\n0.762\n0.803\n\n\n9\nn/a\nbert-multilingual\n0.941\n0.862\n0.700\n0.772\n\n\n10\nn/a\ndistilbert\n0.856\n0.000\n0.000\n0.000\n\n\n11\nemail\ndistilbert\n0.856\n0.000\n0.000\n0.000\n\n\n\n\n\n\n\n\n\n\nAnalysis of each model’s performance revealed that including developer display names had the most significant positive impact on model performance compared to username alone. We also observed that mBERT’s performance was comparable to DeBERTa’s while using the developer email address as an additional input feature. However, we selected the DeBERTa configuration as it consistently performed well across various feature combinations.\nTo facilitate the reuse of our work, we have made our trained model and supporting code publicly available. Complete fine-tuning, evaluation, and inference code is available as the Python package: sci-soft-models, and the fine-tuned model has been released on HuggingFace (evamxb/dev-author-em-clf).\n\n\n7.1.2.3 Model Limitations\nWhile our model demonstrates strong performance, we acknowledge certain limitations in our approach:\n\nShort name sensitivity: Shorter names (both usernames and display names) can affect the model’s performance, as less textual information is available for matching.\nOrganization accounts: Research lab accounts used for project management present a potential challenge for accurate matching, as they do not correspond to individual authors. However, our filtering mechanisms applied before analysis help minimize their impact on modeling.\n\n\n\n\n7.1.3 Dataset Characteristics and Repository Types\nOur compiled dataset appears to contain a mix of repository types, varying from analysis script repositories to software tools and likely some “code dumps” (where code is copied to a new repository immediately before publication). This diversity is reflected in the commit duration patterns across different publication types. The median commit duration for repositories in our analysis is:\n\n47 days for preprints\n104 days for research articles\n247 days for software articles\n\nComplete statistics on commit durations, including count, mean, and quantile details, are available in Table 4.\n\n\n\n\nTable 4: Commit duration (in days) distributions for different publication types. Only includes article-repository pairs with a most recent commit no later than 90 days after publication and excludes publications from research teams in the top 3% of total author sizes.\n\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n10%\n25%\n50%\n75%\n90%\nmax\n\n\narticle_type\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npreprint\n3080\n104\n178\n-1520\n0\n5\n47\n130\n270\n2091\n\n\nresearch article\n19502\n184\n247\n-931\n0\n13\n104\n258\n473\n3176\n\n\nsoftware article\n222\n372\n470\n-1\n0\n22\n247\n510\n904\n3007"
  },
  {
    "objectID": "qss-code-authors.html#distributions-of-author-developer-account-prediction-confidence",
    "href": "qss-code-authors.html#distributions-of-author-developer-account-prediction-confidence",
    "title": "Code Contribution and Scientific Authorship",
    "section": "7.2 Distributions of Author-Developer-Account Prediction Confidence",
    "text": "7.2 Distributions of Author-Developer-Account Prediction Confidence\n\n\n\n\n\n\n\n\nFigure 3: Distribution of author-developer-account prediction confidence scores. The plot on the left shows the distribution of all prediction confidence scores. The plot on the right shows the distribution of prediction confidence scores for author-developer-account pairs with a confidence score greater than or equal to 0.97.\n\n\n\n\n\nThresholding the predictive model confidence at 0.97 resulted in a ~3.2% (n=2,911) reduction in the number of author-developer-account pairs (from an unfiltered total of 90,086 author-developer-account pairs). This threshold was chosen to ensure a high level of confidence in the matches while retaining a large number of pairs for analysis."
  },
  {
    "objectID": "qss-code-authors.html#filtered-dataset-description-for-article-citation-author-position-and-author-correspondence-analysis",
    "href": "qss-code-authors.html#filtered-dataset-description-for-article-citation-author-position-and-author-correspondence-analysis",
    "title": "Code Contribution and Scientific Authorship",
    "section": "7.3 Filtered Dataset Description for Article-Citation, Author-Position, and Author-Correspondence Analysis",
    "text": "7.3 Filtered Dataset Description for Article-Citation, Author-Position, and Author-Correspondence Analysis\n\n\n\n\nTable 5: Counts of article-repository pairs, authors, and developers for research teams. Only includes research teams from article-repository pairs with a most recent commit no later than 90 days after publication and excludes research teams in the top 3% of total author sizes.\n\n\n\n\n\n\nCategory\nSubset\nArticle-Repository Pairs\nAuthors\nDevelopers\n\n\nBy Domain\nHealth Sciences\n1167\n6487\n1462\n\n\nLife Sciences\n1994\n9586\n2594\n\n\nPhysical Sciences\n18202\n62061\n22964\n\n\nSocial Sciences\n1441\n5972\n1913\n\n\nBy Document Type\npreprint\n3080\n13082\n4679\n\n\nresearch article\n19502\n70919\n23993\n\n\nsoftware article\n222\n1023\n495\n\n\nBy Access Status\nClosed\n1190\n5597\n1835\n\n\nOpen\n21614\n77234\n26849\n\n\nBy Data Source\njoss\n86\n416\n281\n\n\nplos\n2847\n14331\n3497\n\n\npwc\n19735\n66217\n24452\n\n\nsoftwarex\n136\n609\n215\n\n\nTotal\n\n22804\n80620\n28290"
  },
  {
    "objectID": "qss-code-authors.html#sec-appendix-additional-cc-na-analysis",
    "href": "qss-code-authors.html#sec-appendix-additional-cc-na-analysis",
    "title": "Code Contribution and Scientific Authorship",
    "section": "7.4 Additional Analysis of Code-Contributing Non-Authors",
    "text": "7.4 Additional Analysis of Code-Contributing Non-Authors\n\n7.4.1 Sample Construction and Labeling\nTo better understand the nature and extent of contributions made by code-contributing non-authors in our dataset, we conducted a detailed analysis using a random sample of 200 individuals from our filtered dataset used in other analyses (Table 5). Our analysis combined qualitative labeling of contribution types with quantitative analysis of commit activity, additions, and deletions made by these contributors to their respective article-repository pairs.\n\n7.4.1.1 Annotation Process\nTwo independent annotators labeled each of the 200 code-contributing non-authors across three dimensions after completing two rounds of trial labeling on 20 cases to establish agreement. The final labeling criteria were:\nContribution Type:\n\n“docs”: Contributors who only modified documentation files (README, LICENSE, etc.) or made changes limited to code comments.\n“code”: Contributors who modified actual code files (.py, .R, .js, etc.) with substantive changes or modified code support files (requirements.txt, pyproject.toml, package.json, etc.). Contributors who made code and documentation changes were labeled “code.”\n“other”: Contributors whose changes did not fit the above categories, including those who committed to upstream forks or merged code without authoring it.\n\nAuthor Matching Assessment:\n\n“yes”: Contributors who should have been matched to an author (missed classification).\n“no”: Contributors correctly classified as non-authors.\n“unclear”: Cases with insufficient information for determination.\n\nBot Account Detection:\n\n“yes”: Automated accounts (GitHub Actions, Dependabot, etc.).\n“no”: Human users.\n\nAfter establishing agreement, each annotator independently labeled 90 contributors— the final sample of 200 created by combining both sets plus the 20 cases used for criteria development.\n\n\n7.4.1.2 Quantitative Metrics\nFor each code-contributing non-author, we collected commit activity data using the GitHub API contributor stats endpoint:\n\nNumber of Commits: The total number of commits made by the code contributor to the article-repository pair.\nNumber of Additions: The total number of lines of code added by the code contributor to the article-repository pair.\nNumber of Deletions: The total number of lines of code deleted by the code contributor to the article-repository pair.\nNumber of Total Repository Commits: The total number of commits made to the article-repository pair, regardless of code contributor.\nNumber of Total Repository Additions: The number of lines of code added to the article-repository pair, regardless of code contributor.\nNumber of Total Repository Deletions: The total number of lines of code deleted from the article-repository pair, regardless of code contributor.\n\nWe additionally calculated the absolute change for each code contributor as the sum of additions and deletions, which provides a measure of the total impact of their contributions. Further, we normalized these metrics by the total number of commits, additions, deletions, and absolute changes made to the article-repository pair, regardless of code contributor. This normalization allows us to compare the relative contribution of each code-contributing non-author to the overall amount of changes to the repository.\n\n\n\n7.4.2 Results\nWe find that ~39% (n=78) of code-contributing non-authors were correctly classified as non-authors, ~30.5% (n=61) were unclear due to insufficient profile information, and ~30.5% (n=61) appeared to be missed classifications that should have been matched to authors. Only two accounts (~1%) were identified as bot accounts.\nWhen broken out by contribution type, we find that:\n\n“true non-authors” (n=78): 59 contributed code, 13 contributed documentation, and 4 contributed some other type of change\n“missed classifications” (n=61): 49 contributed code, 12 contributed documentation, and 0 contributed some other type of change\n“unclear” (n=61): 50 contributed code, 8 contributed documentation, and 3 contributed some other type of change\n\nTable 6 and Table 7 present commit statistics for true non-authors and unclear cases, respectively. Among true non-authors making code contributions, the top quartile (75th percentile and above) contributed ~10.7% of total repository commits and ~14.4% of absolute changes (additions + deletions). The unclear cases showed substantially higher contribution levels. Code contributors in this group comprised ~50.5% of total repository commits and ~41.7% of repository absolute changes, even at the 25th percentile.\n\n\n\n\nTable 6: Commit statistics for code-contributing non-authors labeled as true non-authors. Statistics are calculated as the proportion of commits, additions, deletions, and absolute changes made by the code-contributing non-author to the total commits, additions, deletions, and absolute changes made to the article-repository pair, regardless of code-contributor.\n\n\n\n\n\n\n\n\n\n\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ncommit_stats\n0.178\n0.307\n0.001\n0.007\n0.029\n0.107\n1.0\n\n\naddition_stats\n0.227\n0.380\n0.000\n0.001\n0.007\n0.192\n1.0\n\n\ndeletion_stats\n0.193\n0.358\n0.000\n0.000\n0.006\n0.149\n1.0\n\n\nabs_stats\n0.222\n0.379\n0.000\n0.001\n0.010\n0.144\n1.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 7: Commit statistics for code-contributing non-authors labeled as unclear. Statistics are calculated as the proportion of commits, additions, deletions, and absolute changes made by the code-contributing non-author to the total commits, additions, deletions, and absolute changes made to the article-repository pair, regardless of code-contributor.\n\n\n\n\n\n\n\n\n\n\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ncommit_stats\n0.747\n0.366\n0.004\n0.505\n1.0\n1.0\n1.0\n\n\naddition_stats\n0.722\n0.420\n0.000\n0.310\n1.0\n1.0\n1.0\n\n\ndeletion_stats\n0.737\n0.417\n0.000\n0.722\n1.0\n1.0\n1.0\n\n\nabs_stats\n0.733\n0.404\n0.000\n0.417\n1.0\n1.0\n1.0\n\n\n\n\n\n\n\n\n\n\nWe observed a notable pattern where very few true non-authors (n=4) were repository owners, while ~49.2% of unclear cases (n=30) owned the repositories they contributed to. This suggests that many unclear contributors were likely primary code authors who could not be matched due to limited profile information. When excluding repository owners from the unclear group (Table 8), the median contribution drops to ~34.6% of total commits and ~12.7% of absolute changes, though this still represents substantial technical involvement.\n\n\n\n\nTable 8: Commit statistics for code-contributing non-authors labeled as unclear, excluding repository owners. Statistics are calculated as the proportion of commits, additions, deletions, and absolute changes made by the code-contributing non-author to the total commits, additions, deletions, and absolute changes made to the article-repository pair, regardless of code-contributor.\n\n\n\n\n\n\n\n\n\n\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ncommit_stats\n0.454\n0.399\n0.004\n0.041\n0.346\n0.841\n1.0\n\n\naddition_stats\n0.380\n0.443\n0.000\n0.016\n0.094\n0.917\n1.0\n\n\ndeletion_stats\n0.486\n0.492\n0.000\n0.003\n0.431\n0.999\n1.0\n\n\nabs_stats\n0.392\n0.440\n0.000\n0.011\n0.127\n0.919\n1.0\n\n\n\n\n\n\n\n\n\n\nOur analysis provides evidence that code-contributing non-authors represent a heterogeneous group with varying contribution levels. While defining “substantial” contribution worthy of authorship remains challenging, our findings reveal a clear mix of legitimate non-authors and potentially missed classifications, with both groups often contributing meaningful portions of repository commits and code changes.\nOur sample size of 200 limits generalizability to the full population of code-contributing non-authors. Additionally, the manual annotation process introduces potential subjectivity despite our established criteria, and our reliance on publicly available GitHub profiles may systematically underestimate contributions from developers with minimal profile information."
  },
  {
    "objectID": "qss-code-authors.html#article-citation-linear-model-results",
    "href": "qss-code-authors.html#article-citation-linear-model-results",
    "title": "Code Contribution and Scientific Authorship",
    "section": "7.5 Article Citation Linear Model Results",
    "text": "7.5 Article Citation Linear Model Results\n\n\n\n\nTable 9: Article citations by code contributorship of the research team. Generalized linear model fit with negative binomial distribution and log link function.\n\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ncited_by_count\nNo. Observations:\n22127\n\n\nModel:\nGLM\nDf Residuals:\n22122\n\n\nModel Family:\nNegativeBinomial\nDf Model:\n4\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-76532.\n\n\nDate:\nThu, 19 Jun 2025\nDeviance:\n22300.\n\n\nTime:\n03:21:46\nPearson chi2:\n3.39e+04\n\n\nNo. Iterations:\n13\nPseudo R-squ. (CS):\n0.2877\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n0.9845\n0.026\n38.459\n0.000\n0.934\n1.035\n\n\nTotal Authors\n0.0692\n0.004\n18.491\n0.000\n0.062\n0.077\n\n\nCode-Contrib. Authors\n0.0501\n0.010\n4.831\n0.000\n0.030\n0.070\n\n\nCode-Contrib. Non-Authors\n-0.0025\n0.004\n-0.575\n0.565\n-0.011\n0.006\n\n\nYears Since Publication\n0.3905\n0.004\n96.908\n0.000\n0.383\n0.398\n\n\n\n\n\n\n\n\n\n\n\n\nTable 10: Article citations by code contributorship of the research team controlled by open access status. Generalized linear model fit with negative binomial distribution and log link function.\n\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ncited_by_count\nNo. Observations:\n22127\n\n\nModel:\nGLM\nDf Residuals:\n22119\n\n\nModel Family:\nNegativeBinomial\nDf Model:\n7\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-76464.\n\n\nDate:\nThu, 19 Jun 2025\nDeviance:\n22163.\n\n\nTime:\n03:21:46\nPearson chi2:\n3.36e+04\n\n\nNo. Iterations:\n13\nPseudo R-squ. (CS):\n0.2921\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n0.6140\n0.061\n10.089\n0.000\n0.495\n0.733\n\n\nTotal Authors\n0.0697\n0.004\n18.614\n0.000\n0.062\n0.077\n\n\nCode-Contrib. Authors\n0.0513\n0.045\n1.130\n0.259\n-0.038\n0.140\n\n\nCode-Contrib. Non-Authors\n0.0017\n0.016\n0.110\n0.912\n-0.029\n0.032\n\n\nYears Since Publication\n0.3811\n0.004\n93.064\n0.000\n0.373\n0.389\n\n\nIs Open Access\n0.4104\n0.060\n6.896\n0.000\n0.294\n0.527\n\n\nCode-Contrib. Authors × Is Open Access\n-0.0012\n0.047\n-0.025\n0.980\n-0.093\n0.090\n\n\nCode-Contrib. Non-Authors × Is Open Access\n-0.0036\n0.016\n-0.223\n0.824\n-0.035\n0.028\n\n\n\n\n\n\n\n\n\n\n\n\nTable 11: Article citations by code contributorship of the research team controlled by domain. Generalized linear model fit with negative binomial distribution and log link function.\n\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ncited_by_count\nNo. Observations:\n22127\n\n\nModel:\nGLM\nDf Residuals:\n22113\n\n\nModel Family:\nNegativeBinomial\nDf Model:\n13\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-76441.\n\n\nDate:\nThu, 19 Jun 2025\nDeviance:\n22118.\n\n\nTime:\n03:21:46\nPearson chi2:\n3.34e+04\n\n\nNo. Iterations:\n13\nPseudo R-squ. (CS):\n0.2936\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n0.8852\n0.067\n13.267\n0.000\n0.754\n1.016\n\n\nTotal Authors\n0.0709\n0.004\n18.636\n0.000\n0.063\n0.078\n\n\nCode-Contrib. Authors\n0.0229\n0.052\n0.440\n0.660\n-0.079\n0.125\n\n\nCode-Contrib. Non-Authors\n0.0129\n0.027\n0.486\n0.627\n-0.039\n0.065\n\n\nYears Since Publication\n0.3969\n0.004\n97.373\n0.000\n0.389\n0.405\n\n\nDomain Life Sciences\n-0.2037\n0.075\n-2.698\n0.007\n-0.352\n-0.056\n\n\nDomain Physical Sciences\n0.1265\n0.063\n2.018\n0.044\n0.004\n0.249\n\n\nDomain Social Sciences\n-0.1886\n0.080\n-2.354\n0.019\n-0.346\n-0.032\n\n\nCode-Contrib. Authors × Domain Life Sciences\n0.0667\n0.063\n1.053\n0.292\n-0.057\n0.191\n\n\nCode-Contrib. Authors × Domain Physical Sciences\n0.0173\n0.053\n0.324\n0.746\n-0.087\n0.122\n\n\nCode-Contrib. Authors × Domain Social Sciences\n0.1115\n0.065\n1.706\n0.088\n-0.017\n0.239\n\n\nCode-Contrib. Non-Authors × Domain Life Sciences\n-0.0371\n0.035\n-1.062\n0.288\n-0.106\n0.031\n\n\nCode-Contrib. Non-Authors × Domain Physical Sciences\n-0.0163\n0.027\n-0.605\n0.545\n-0.069\n0.037\n\n\nCode-Contrib. Non-Authors × Domain Social Sciences\n-0.0367\n0.037\n-1.002\n0.316\n-0.109\n0.035\n\n\n\n\n\n\n\n\n\n\n\n\nTable 12: Article citations by code contributorship of the research team controlled by article type. Generalized linear model fit with negative binomial distribution and log link function.\n\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ncited_by_count\nNo. Observations:\n22127\n\n\nModel:\nGLM\nDf Residuals:\n22116\n\n\nModel Family:\nNegativeBinomial\nDf Model:\n10\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-76088.\n\n\nDate:\nThu, 19 Jun 2025\nDeviance:\n21411.\n\n\nTime:\n03:21:46\nPearson chi2:\n3.25e+04\n\n\nNo. Iterations:\n13\nPseudo R-squ. (CS):\n0.3158\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n0.5108\n0.042\n12.292\n0.000\n0.429\n0.592\n\n\nTotal Authors\n0.0689\n0.004\n18.363\n0.000\n0.062\n0.076\n\n\nCode-Contrib. Authors\n-0.0191\n0.027\n-0.703\n0.482\n-0.072\n0.034\n\n\nCode-Contrib. Non-Authors\n-0.0320\n0.010\n-3.270\n0.001\n-0.051\n-0.013\n\n\nYears Since Publication\n0.4024\n0.004\n99.445\n0.000\n0.394\n0.410\n\n\nArticle Type Research Article\n0.4854\n0.038\n12.797\n0.000\n0.411\n0.560\n\n\nArticle Type Software Article\n-0.4715\n0.131\n-3.609\n0.000\n-0.728\n-0.215\n\n\nCode-Contrib. Authors × Article Type Research Article\n0.0925\n0.029\n3.138\n0.002\n0.035\n0.150\n\n\nCode-Contrib. Authors × Article Type Software Article\n-0.0676\n0.065\n-1.038\n0.299\n-0.195\n0.060\n\n\nCode-Contrib. Non-Authors × Article Type Research Article\n0.0390\n0.011\n3.573\n0.000\n0.018\n0.060\n\n\nCode-Contrib. Non-Authors × Article Type Software Article\n0.0901\n0.074\n1.215\n0.224\n-0.055\n0.235"
  },
  {
    "objectID": "qss-code-authors.html#post-hoc-tests-for-coding-vs-non-coding-authors-by-position",
    "href": "qss-code-authors.html#post-hoc-tests-for-coding-vs-non-coding-authors-by-position",
    "title": "Code Contribution and Scientific Authorship",
    "section": "7.6 Post-Hoc Tests for Coding vs Non-Coding Authors by Position",
    "text": "7.6 Post-Hoc Tests for Coding vs Non-Coding Authors by Position\n\n\n\n\nTable 13: Counts of Code-Contributing Authors (‘Coding’) and Total Authors by Position and Bonferroni Corrected p-values from Post-Hoc Binomial Tests. Significant p-values are indicated with asterisks: p &lt; 0.05 (*), p &lt; 0.01 (**), p &lt; 0.001 (***).\n\n\n\n\n\n\nControl\nSubset\nPosition\nCoding\nTotal\np\n\n\nDomain\nHealth Sciences\nFirst\n745\n1167\n0.000***\n\n\nMiddle\n255\n4741\n0.000***\n\n\nLast\n109\n1156\n0.000***\n\n\nLife Sciences\nFirst\n1277\n1992\n0.000***\n\n\nMiddle\n446\n6289\n0.000***\n\n\nLast\n252\n1987\n0.000***\n\n\nPhysical Sciences\nFirst\n12582\n18116\n0.000***\n\n\nMiddle\n4992\n51004\n0.000***\n\n\nLast\n1207\n17965\n0.000***\n\n\nSocial Sciences\nFirst\n971\n1438\n0.000***\n\n\nMiddle\n442\n3649\n0.000***\n\n\nLast\n139\n1437\n0.000***\n\n\nArticle Type\nPreprint\nFirst\n2118\n3053\n0.000***\n\n\nMiddle\n912\n8626\n0.000***\n\n\nLast\n194\n3050\n0.000***\n\n\nResearch Article\nFirst\n13320\n19438\n0.000***\n\n\nMiddle\n5069\n56453\n0.000***\n\n\nLast\n1467\n19273\n0.000***\n\n\nSoftware Article\nFirst\n137\n222\n0.004**\n\n\nMiddle\n154\n604\n0.000***\n\n\nLast\n46\n222\n0.000***\n\n\nOpen Access Status\nClosed Access\nFirst\n838\n1184\n0.000***\n\n\nMiddle\n333\n3647\n0.000***\n\n\nLast\n76\n1169\n0.000***\n\n\nOpen Access\nFirst\n14737\n21529\n0.000***\n\n\nMiddle\n5802\n62036\n0.000***\n\n\nLast\n1631\n21376\n0.000***\n\n\nOverall\nOverall\nFirst\n15575\n22713\n0.000***\n\n\nMiddle\n6135\n65683\n0.000***\n\n\nLast\n1707\n22545\n0.000***\n\n\n\n\n\n\n\n\nCounts of authors in Table 13 may differ slightly from counts in Table 5. Table 5 counts unique authors, while Table 13 counts unique author-document pairs (i.e., the same author may appear in multiple documents)."
  },
  {
    "objectID": "qss-code-authors.html#post-hoc-tests-for-coding-vs-non-coding-authors-by-corresponding-status",
    "href": "qss-code-authors.html#post-hoc-tests-for-coding-vs-non-coding-authors-by-corresponding-status",
    "title": "Code Contribution and Scientific Authorship",
    "section": "7.7 Post-Hoc Tests for Coding vs Non-Coding Authors by Corresponding Status",
    "text": "7.7 Post-Hoc Tests for Coding vs Non-Coding Authors by Corresponding Status\n\n\n\n\nTable 14: Counts of Code-Contributing Authors (‘Coding’) and Total Authors by Corresponding Status and Bonferroni Corrected p-values from Post-Hoc Binomial Tests. Significant p-values are indicated with asterisks: p &lt; 0.05 (*), p &lt; 0.01 (**), p &lt; 0.001 (***).\n\n\n\n\n\n\nControl\nSubset\nIs Corresponding\nCoding\nTotal\np\n\n\nDomain\nHealth Sciences\nCorresponding\n542\n3010\n0.000***\n\n\nNot Corresponding\n567\n4054\n0.000***\n\n\nLife Sciences\nCorresponding\n1031\n4942\n0.000***\n\n\nNot Corresponding\n944\n5326\n0.000***\n\n\nPhysical Sciences\nCorresponding\n2549\n7238\n0.000***\n\n\nNot Corresponding\n16232\n79847\n0.000***\n\n\nSocial Sciences\nCorresponding\n429\n1515\n0.000***\n\n\nNot Corresponding\n1123\n5009\n0.000***\n\n\nArticle Type\nPreprint\nCorresponding\n16\n51\n0.022*\n\n\nNot Corresponding\n3208\n14678\n0.000***\n\n\nResearch Article\nCorresponding\n4464\n16458\n0.000***\n\n\nNot Corresponding\n15392\n78706\n0.000***\n\n\nSoftware Article\nCorresponding\n71\n196\n0.000***\n\n\nNot Corresponding\n266\n852\n0.000***\n\n\nOpen Access Status\nClosed Access\nCorresponding\n86\n194\n0.263\n\n\nNot Corresponding\n1161\n5806\n0.000***\n\n\nOpen Access\nCorresponding\n4465\n16511\n0.000***\n\n\nNot Corresponding\n17705\n88430\n0.000***\n\n\nOverall\nOverall\nCorresponding\n4551\n16705\n0.000***\n\n\nNot Corresponding\n18866\n94236\n0.000***\n\n\n\n\n\n\n\n\nCounts of authors in Table 14 may differ slightly from counts in Table 5. Table 5 counts unique authors, while Table 14 counts unique author-document pairs (i.e., the same author may appear in multiple documents)."
  },
  {
    "objectID": "qss-code-authors.html#filtered-dataset-description-for-h-index-analysis",
    "href": "qss-code-authors.html#filtered-dataset-description-for-h-index-analysis",
    "title": "Code Contribution and Scientific Authorship",
    "section": "7.8 Filtered Dataset Description for h-Index Analysis",
    "text": "7.8 Filtered Dataset Description for h-Index Analysis\n\n\n\n\nTable 15: Counts of Total Authors, n Any Coding Authors, n Majority Coding Authors, and n Always Coding Authors by Common Domain, Document Type, and Author Position. Authors are only included if they have three or more publications within our dataset and are associated with no more than three developer accounts, with each association having a predicted model confidence of at least 97%.\n\n\n\n\n\n\nCategory\nSubset\nTotal Authors\nAny Code\nMajority Code\nAlways Code\n\n\nBy Commmon Domain\nHealth Sciences\n1507\n339\n196\n82\n\n\nLife Sciences\n1440\n351\n236\n129\n\n\nPhysical Sciences\n49430\n14753\n7951\n3720\n\n\nSocial Sciences\n1304\n276\n219\n178\n\n\nBy Document Type\nPreprint\n29038\n9255\n4828\n2151\n\n\nResearch Article\n24265\n6419\n3657\n1830\n\n\nSoftware Article\n378\n45\n117\n128\n\n\nBy Author Position\nFirst\n11459\n1671\n4864\n3249\n\n\nLast\n10208\n2260\n550\n186\n\n\nMiddle\n32014\n11788\n3188\n674\n\n\nTotal\n\n53681\n15719\n8602\n4109"
  },
  {
    "objectID": "qss-code-authors.html#h-index-linear-model-results",
    "href": "qss-code-authors.html#h-index-linear-model-results",
    "title": "Code Contribution and Scientific Authorship",
    "section": "7.9 h-Index Linear Model Results",
    "text": "7.9 h-Index Linear Model Results\n\n\n\n\nTable 16: Code-contributing authors h-index by coding status. Generalized linear model fit with Gaussian distribution and log link function.\n\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nh_index\nNo. Observations:\n49483\n\n\nModel:\nGLM\nDf Residuals:\n49478\n\n\nModel Family:\nGaussian\nDf Model:\n4\n\n\nLink Function:\nLog\nScale:\n198.76\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-2.0115e+05\n\n\nDate:\nThu, 19 Jun 2025\nDeviance:\n9.8342e+06\n\n\nTime:\n03:21:49\nPearson chi2:\n9.83e+06\n\n\nNo. Iterations:\n46\nPseudo R-squ. (CS):\n0.1757\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n3.1825\n0.004\n838.738\n0.000\n3.175\n3.190\n\n\nWorks Count\n0.0001\n1.98e-06\n66.904\n0.000\n0.000\n0.000\n\n\nAny Coding\n-0.3213\n0.008\n-42.806\n0.000\n-0.336\n-0.307\n\n\nMajority Coding\n-0.7591\n0.014\n-53.908\n0.000\n-0.787\n-0.732\n\n\nAlways Coding\n-0.9583\n0.025\n-38.142\n0.000\n-1.008\n-0.909\n\n\n\n\n\n\n\n\n\n\n\n\nTable 17: Code-contributing authors h-index by coding status controlled by most freq. author position. Generalized linear model fit with Gaussian distribution and log link function.\n\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nh_index\nNo. Observations:\n49483\n\n\nModel:\nGLM\nDf Residuals:\n49470\n\n\nModel Family:\nGaussian\nDf Model:\n12\n\n\nLink Function:\nLog\nScale:\n180.37\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-1.9874e+05\n\n\nDate:\nThu, 19 Jun 2025\nDeviance:\n8.9231e+06\n\n\nTime:\n03:21:49\nPearson chi2:\n8.92e+06\n\n\nNo. Iterations:\n44\nPseudo R-squ. (CS):\n0.2702\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n2.3605\n0.032\n73.290\n0.000\n2.297\n2.424\n\n\nWorks Count\n0.0001\n1.96e-06\n67.646\n0.000\n0.000\n0.000\n\n\nAny Coding\n0.1571\n0.042\n3.745\n0.000\n0.075\n0.239\n\n\nMajority Coding\n-0.0490\n0.038\n-1.301\n0.193\n-0.123\n0.025\n\n\nAlways Coding\n-0.2248\n0.043\n-5.176\n0.000\n-0.310\n-0.140\n\n\nCommon Author Position Last\n1.0623\n0.033\n32.552\n0.000\n0.998\n1.126\n\n\nCommon Author Position Middle\n0.7533\n0.033\n23.135\n0.000\n0.690\n0.817\n\n\nAny Coding × Common Author Position Last\n-0.3112\n0.044\n-7.129\n0.000\n-0.397\n-0.226\n\n\nAny Coding × Common Author Position Middle\n-0.4674\n0.043\n-10.893\n0.000\n-0.551\n-0.383\n\n\nMajority Coding × Common Author Position Last\n-0.3726\n0.047\n-7.861\n0.000\n-0.465\n-0.280\n\n\nMajority Coding × Common Author Position Middle\n-0.6241\n0.043\n-14.485\n0.000\n-0.709\n-0.540\n\n\nAlways Coding × Common Author Position Last\n-0.3783\n0.074\n-5.126\n0.000\n-0.523\n-0.234\n\n\nAlways Coding × Common Author Position Middle\n-0.5100\n0.066\n-7.678\n0.000\n-0.640\n-0.380\n\n\n\n\n\n\n\n\n\n\n\n\nTable 18: Code-contributing authors h-index by coding status controlled by most freq. domain. Generalized linear model fit with Gaussian distribution and log link function.\n\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nh_index\nNo. Observations:\n49483\n\n\nModel:\nGLM\nDf Residuals:\n49466\n\n\nModel Family:\nGaussian\nDf Model:\n16\n\n\nLink Function:\nLog\nScale:\n197.48\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-2.0098e+05\n\n\nDate:\nThu, 19 Jun 2025\nDeviance:\n9.7684e+06\n\n\nTime:\n03:21:49\nPearson chi2:\n9.77e+06\n\n\nNo. Iterations:\n48\nPseudo R-squ. (CS):\n0.1823\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n3.3098\n0.018\n186.605\n0.000\n3.275\n3.345\n\n\nWorks Count\n0.0001\n2.06e-06\n66.916\n0.000\n0.000\n0.000\n\n\nAny Coding\n-0.3737\n0.044\n-8.398\n0.000\n-0.461\n-0.286\n\n\nMajority Coding\n-1.4599\n0.104\n-14.068\n0.000\n-1.663\n-1.256\n\n\nAlways Coding\n-1.1856\n0.187\n-6.357\n0.000\n-1.551\n-0.820\n\n\nCommon Domain Life Sciences\n0.1075\n0.025\n4.247\n0.000\n0.058\n0.157\n\n\nCommon Domain Physical Sciences\n-0.1400\n0.018\n-7.701\n0.000\n-0.176\n-0.104\n\n\nCommon Domain Social Sciences\n-0.1638\n0.030\n-5.395\n0.000\n-0.223\n-0.104\n\n\nAny Coding × Common Domain Life Sciences\n0.0747\n0.058\n1.279\n0.201\n-0.040\n0.189\n\n\nAny Coding × Common Domain Physical Sciences\n0.0538\n0.045\n1.191\n0.234\n-0.035\n0.142\n\n\nAny Coding × Common Domain Social Sciences\n0.0117\n0.073\n0.161\n0.872\n-0.131\n0.155\n\n\nMajority Coding × Common Domain Life Sciences\n0.8336\n0.119\n6.982\n0.000\n0.600\n1.068\n\n\nMajority Coding × Common Domain Physical Sciences\n0.7193\n0.105\n6.865\n0.000\n0.514\n0.925\n\n\nMajority Coding × Common Domain Social Sciences\n0.7679\n0.135\n5.703\n0.000\n0.504\n1.032\n\n\nAlways Coding × Common Domain Life Sciences\n0.2730\n0.213\n1.281\n0.200\n-0.145\n0.691\n\n\nAlways Coding × Common Domain Physical Sciences\n0.2279\n0.188\n1.210\n0.226\n-0.141\n0.597\n\n\nAlways Coding × Common Domain Social Sciences\n0.3030\n0.219\n1.381\n0.167\n-0.127\n0.733\n\n\n\n\n\n\n\n\n\n\n\n\nTable 19: Code-contributing authors h-index by coding status controlled by most freq. article type. Generalized linear model fit with Gaussian distribution and log link function.\n\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nh_index\nNo. Observations:\n49483\n\n\nModel:\nGLM\nDf Residuals:\n49470\n\n\nModel Family:\nGaussian\nDf Model:\n12\n\n\nLink Function:\nLog\nScale:\n195.37\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-2.0072e+05\n\n\nDate:\nThu, 19 Jun 2025\nDeviance:\n9.6651e+06\n\n\nTime:\n03:21:49\nPearson chi2:\n9.67e+06\n\n\nNo. Iterations:\n47\nPseudo R-squ. (CS):\n0.1927\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n3.0879\n0.006\n532.656\n0.000\n3.077\n3.099\n\n\nWorks Count\n0.0001\n1.99e-06\n64.759\n0.000\n0.000\n0.000\n\n\nAny Coding\n-0.2938\n0.011\n-27.311\n0.000\n-0.315\n-0.273\n\n\nMajority Coding\n-0.7606\n0.021\n-36.189\n0.000\n-0.802\n-0.719\n\n\nAlways Coding\n-0.9799\n0.040\n-24.644\n0.000\n-1.058\n-0.902\n\n\nCommon Article Type Research Article\n0.1836\n0.008\n24.214\n0.000\n0.169\n0.198\n\n\nCommon Article Type Software Article\n0.2231\n0.055\n4.034\n0.000\n0.115\n0.331\n\n\nAny Coding × Common Article Type Research Article\n-0.0296\n0.015\n-1.984\n0.047\n-0.059\n-0.000\n\n\nAny Coding × Common Article Type Software Article\n0.1663\n0.103\n1.622\n0.105\n-0.035\n0.367\n\n\nMajority Coding × Common Article Type Research Article\n0.0071\n0.028\n0.251\n0.802\n-0.048\n0.063\n\n\nMajority Coding × Common Article Type Software Article\n0.3797\n0.090\n4.220\n0.000\n0.203\n0.556\n\n\nAlways Coding × Common Article Type Research Article\n0.0018\n0.052\n0.034\n0.973\n-0.101\n0.104\n\n\nAlways Coding × Common Article Type Software Article\n0.3679\n0.108\n3.417\n0.001\n0.157\n0.579"
  },
  {
    "objectID": "qss-code-authors.html#analysis-of-project-duration-and-percentage-code-contributors-who-are-authors",
    "href": "qss-code-authors.html#analysis-of-project-duration-and-percentage-code-contributors-who-are-authors",
    "title": "Code Contribution and Scientific Authorship",
    "section": "7.10 Analysis of Project Duration and Percentage Code-Contributors Who Are Authors",
    "text": "7.10 Analysis of Project Duration and Percentage Code-Contributors Who Are Authors\nIn our pre-registered analysis plan (https://osf.io/fc74m), we initially hypothesized that there would be a positive relationship between project duration and authorship recognition. Specifically, we posited that sustained technical engagement and scientific recognition might be meaningfully related, with longer project durations potentially leading to higher rates of code-contributor authorship. We saw repository histories as providing a unique opportunity to examine this relationship, leading us to hypothesize that projects with longer commit durations would be associated with higher percentages of developers receiving authorship recognition (pre-registered as H2).\nHowever, our analysis found no evidence to support this hypothesis. When examining the relationship between a repository’s commit duration and the percentage of developers who receive authorship recognition, we found no significant correlation (r = 0.00, p = n.s.). This suggests that the length of time a project has been in development has no meaningful relationship with the proportion of developers who are recognized as authors.\nWe ultimately decided to exclude this analysis for two key methodological reasons. First, our approach of using repository-level commit duration as a proxy for individual contribution patterns proved too coarse-grained. A more precise analysis would need to examine individual-level contribution durations and patterns rather than overall project length. Second, our method did not account for the varying levels of contribution that different developers make to a repository. Simply correlating overall project duration with authorship rates fails to capture the nuanced ways that sustained, meaningful technical contributions might influence authorship decisions.\nThese limitations suggest potential directions for future work that could more rigorously examine the relationship between long-term technical engagement and scientific recognition. Such work could benefit from a more granular analysis of individual contribution patterns, incorporating measures of contribution significance and sustainability rather than just temporal duration."
  },
  {
    "objectID": "extra-notebooks/cc-na-anno.html",
    "href": "extra-notebooks/cc-na-anno.html",
    "title": "CC-NA Annotation Sample Generation",
    "section": "",
    "text": "This notebook was used to generate the sample of code-contributing non-authors used in our brief extended analysis in the QSS paper.\n\nimport time\nfrom pathlib import Path\n\nimport pandas as pd\nfrom gh_tokens_loader import GitHubTokensCycler\nfrom ghapi.all import GhApi\nfrom sqlalchemy import create_engine, text\nfrom tqdm import tqdm\n\nfrom rs_graph.db import models as db_models\nfrom rs_graph.db.constants import PROD_DATABASE_FILEPATH\n\nEXTRA_DATA_DIR = Path.cwd().parent / \"extra-data\"\n\n# Get db engine for production database\ndb_conn = create_engine(f\"sqlite:///{PROD_DATABASE_FILEPATH}\")\n\n\ndef read_table(table: str) -&gt; pd.DataFrame:\n    return pd.read_sql(text(f\"SELECT * FROM {table}\"), db_conn)\n\n\n# Read all data from database\ndoc_repo_links = read_table(db_models.DocumentRepositoryLink.__tablename__)\nresearchers = read_table(db_models.Researcher.__tablename__)\ndevs = read_table(db_models.DeveloperAccount.__tablename__)\ndocuments = read_table(db_models.Document.__tablename__)\ndocument_contributors = read_table(db_models.DocumentContributor.__tablename__)\nrepositories = read_table(db_models.Repository.__tablename__)\nrepository_contributors = read_table(db_models.RepositoryContributor.__tablename__)\ntopics = read_table(db_models.Topic.__tablename__)\ndocument_topics = read_table(db_models.DocumentTopic.__tablename__)\ndataset_sources = read_table(db_models.DatasetSource.__tablename__)\nresearcher_dev_links = read_table(\n    db_models.ResearcherDeveloperAccountLink.__tablename__\n)\ndocument_alternate_dois = read_table(db_models.DocumentAlternateDOI.__tablename__)\n\n# Drop all \"updated_datetime\" and \"created_datetime\" columns\nfor df in [\n    doc_repo_links,\n    researchers,\n    devs,\n    documents,\n    document_contributors,\n    repositories,\n    repository_contributors,\n    topics,\n    document_topics,\n    dataset_sources,\n    researcher_dev_links,\n]:\n    df.drop(columns=[\"updated_datetime\", \"created_datetime\"], inplace=True)\n\n# Specifically drop doc_repo_links \"id\" column\n# It isn't used and will get in the way later when we do a lot of joins\ndoc_repo_links.drop(columns=[\"id\"], inplace=True)\n\n# Construct reduced doc_repo_links\noriginal_doc_repo_links_len = len(doc_repo_links)\ndoc_repo_links = doc_repo_links.drop_duplicates(subset=[\"document_id\"], keep=False)\ndoc_repo_links = doc_repo_links.drop_duplicates(subset=[\"repository_id\"], keep=False)\n\n# Reduce other tables to only documents / repositories in the updated doc_repo_links\ndocuments = documents[documents[\"id\"].isin(doc_repo_links[\"document_id\"])]\nrepositories = repositories[repositories[\"id\"].isin(doc_repo_links[\"repository_id\"])]\ndocument_contributors = document_contributors[\n    document_contributors[\"document_id\"].isin(documents[\"id\"])\n]\nrepository_contributors = repository_contributors[\n    repository_contributors[\"repository_id\"].isin(repositories[\"id\"])\n]\ndocument_topics = document_topics[document_topics[\"document_id\"].isin(documents[\"id\"])]\n\n# Reduce researchers and devs to only those in the\n# updated document_contributors and repository_contributors\nresearchers = researchers[\n    researchers[\"id\"].isin(document_contributors[\"researcher_id\"])\n]\ndevs = devs[devs[\"id\"].isin(repository_contributors[\"developer_account_id\"])]\nresearcher_dev_links = researcher_dev_links[\n    (\n        researcher_dev_links[\"researcher_id\"].isin(researchers[\"id\"])\n        & researcher_dev_links[\"developer_account_id\"].isin(devs[\"id\"])\n    )\n]\n\n# Sort document topics and keep first\ndocument_topics = document_topics.sort_values(\"score\", ascending=False)\ndocument_topics = document_topics.drop_duplicates(subset=[\"document_id\"], keep=\"first\")\n\n# Create document, document topic merged table\nmerged_document_topics = pd.merge(\n    document_topics, topics, left_on=\"topic_id\", right_on=\"id\"\n)\n\n# Create basic merged tables\nmerged_document_contributor_doc_repo_links = pd.merge(\n    document_contributors, doc_repo_links, left_on=\"document_id\", right_on=\"document_id\"\n)\nmerged_repository_contributor_doc_repo_links = pd.merge(\n    repository_contributors,\n    doc_repo_links,\n    left_on=\"repository_id\",\n    right_on=\"repository_id\",\n)\n\n# Compute stats for data sources\ndata_source_stats = []\nfor _, data_source in dataset_sources.iterrows():\n    # Get total article-repo pairs\n    data_source_stats.append(\n        {\n            \"data_source\": data_source[\"name\"],\n            \"n_article_repo_pairs\": len(\n                doc_repo_links[doc_repo_links[\"dataset_source_id\"] == data_source[\"id\"]]\n            ),\n            \"n_authors\": merged_document_contributor_doc_repo_links.loc[\n                merged_document_contributor_doc_repo_links[\"dataset_source_id\"]\n                == data_source[\"id\"]\n            ][\"researcher_id\"].nunique(),\n            \"n_devs\": merged_repository_contributor_doc_repo_links.loc[\n                merged_repository_contributor_doc_repo_links[\"dataset_source_id\"]\n                == data_source[\"id\"]\n            ][\"developer_account_id\"].nunique(),\n        }\n    )\n\n# Create topic merged tables\nmerged_doc_repo_links_topics = pd.merge(\n    doc_repo_links, document_topics, left_on=\"document_id\", right_on=\"document_id\"\n).merge(topics, left_on=\"topic_id\", right_on=\"id\")\nmerged_doc_repo_links_topics_document_contributors = pd.merge(\n    merged_doc_repo_links_topics,\n    document_contributors,\n    left_on=\"document_id\",\n    right_on=\"document_id\",\n)\nmerged_doc_repo_links_topics_repository_contributors = pd.merge(\n    merged_doc_repo_links_topics,\n    repository_contributors,\n    left_on=\"repository_id\",\n    right_on=\"repository_id\",\n)\n\n# Compute stats for domains\ndomain_stats = []\nfor domain in merged_doc_repo_links_topics.domain_name.unique():\n    # Get total article-repo pairs\n    domain_stats.append(\n        {\n            \"domain\": domain,\n            \"n_article_repo_pairs\": len(\n                merged_doc_repo_links_topics[\n                    merged_doc_repo_links_topics[\"domain_name\"] == domain\n                ]\n            ),\n            \"n_authors\": merged_doc_repo_links_topics_document_contributors.loc[\n                merged_doc_repo_links_topics_document_contributors[\"domain_name\"]\n                == domain\n            ][\"researcher_id\"].nunique(),\n            \"n_devs\": merged_doc_repo_links_topics_repository_contributors.loc[\n                merged_doc_repo_links_topics_repository_contributors[\"domain_name\"]\n                == domain\n            ][\"developer_account_id\"].nunique(),\n        }\n    )\n\n# Create document merged tables\nmerged_doc_repo_links_documents = pd.merge(\n    doc_repo_links, documents, left_on=\"document_id\", right_on=\"id\"\n)\nmerged_doc_repo_links_documents_document_contributors = pd.merge(\n    merged_doc_repo_links_documents,\n    document_contributors,\n    left_on=\"document_id\",\n    right_on=\"document_id\",\n)\nmerged_doc_repo_links_documents_repository_contributors = pd.merge(\n    merged_doc_repo_links_documents,\n    repository_contributors,\n    left_on=\"repository_id\",\n    right_on=\"repository_id\",\n)\n\n# Compute stats for document types\n# This isn't a standard data pull\n# In short:\n# - pairs from PLOS are \"research articles\"\n# - pairs from JOSS are \"software articles\"\n# - pairs from SoftwareX are \"software articles\"\n# - pairs from Papers with Code / ArXiv are \"pre-prints\"\n#   UNLESS they have been published in a journal\n# All of those should be easy to assert / apply a label to with the exception\n# of Papers with Code / ArXiv pre-prints that have been published in a journal\n# In that case, we need to look at the existing document type in the database\n# If the document type is \"preprint\" use preprint, otherwise, if it's anything else,\n# use \"research article\"\n\n# Create a \"reduced_doc_types\" dataframe with document_id and \"reduced_doc_type\"\n# columns\nreduced_doc_types_rows = []\n# We can use the \"reduced_doc_types\" dataframe to calculate the stats\n\n# Iter over data sources even though we are looking for doc types\nfor _, data_source in dataset_sources.iterrows():\n    # Get total article-repo pairs\n    doc_type = None\n    if data_source[\"name\"] in [\"plos\", \"joss\", \"softwarex\"]:\n        if data_source[\"name\"] == \"plos\":\n            doc_type = \"research article\"\n        else:\n            doc_type = \"software article\"\n\n        # Add all document_ids to reduced_doc_types_rows\n        reduced_doc_types_rows.extend(\n            [\n                {\"document_id\": doc_id, \"reduced_doc_type\": doc_type}\n                for doc_id in doc_repo_links[\n                    (doc_repo_links[\"dataset_source_id\"] == data_source[\"id\"])\n                ][\"document_id\"]\n            ]\n        )\n\n    # Handle PwC\n    else:\n        # Get preprint pairs\n        preprint_pairs = merged_doc_repo_links_documents[\n            (merged_doc_repo_links_documents[\"dataset_source_id\"] == data_source[\"id\"])\n            & (merged_doc_repo_links_documents[\"document_type\"] == \"preprint\")\n        ]\n\n        # Add all document_ids to reduced_doc_types_rows\n        reduced_doc_types_rows.extend(\n            [\n                {\"document_id\": doc_id, \"reduced_doc_type\": \"preprint\"}\n                for doc_id in preprint_pairs[\"document_id\"]\n            ]\n        )\n\n        # Get research article pairs\n        # This is the same just inverted to != \"preprint\"\n        research_article_pairs = merged_doc_repo_links_documents[\n            (merged_doc_repo_links_documents[\"dataset_source_id\"] == data_source[\"id\"])\n            & (merged_doc_repo_links_documents[\"document_type\"] != \"preprint\")\n        ]\n\n        # Add all document_ids to reduced_doc_types_rows\n        reduced_doc_types_rows.extend(\n            [\n                {\"document_id\": doc_id, \"reduced_doc_type\": \"research article\"}\n                for doc_id in research_article_pairs[\"document_id\"]\n            ]\n        )\n\n# Create reduced_doc_types dataframe\nreduced_doc_types = pd.DataFrame(reduced_doc_types_rows)\n\n\n# Create subset documents\ndocs_w_1_citation = documents.loc[documents[\"cited_by_count\"] &gt;= 1].copy()\n\n# Take sample?\n# docs_w_1_citation = docs_w_1_citation.sample(frac=0.1, random_state=12)\n\n# Subset to only certain columns\ndocs_w_1_citation = docs_w_1_citation[\n    [\n        \"id\",\n        \"publication_date\",\n        \"cited_by_count\",\n        \"fwci\",\n        \"is_open_access\",\n    ]\n]\n\n# Rename id to document_id\ndocs_w_1_citation = docs_w_1_citation.rename(columns={\"id\": \"document_id\"})\n\n# Merge repository id in\ndocs_w_1_citation = docs_w_1_citation.merge(\n    doc_repo_links[\n        [\n            \"document_id\",\n            \"repository_id\",\n        ]\n    ],\n    left_on=\"document_id\",\n    right_on=\"document_id\",\n)\n\n# Merge in document details (domain, document type)\ndocs_w_1_citation = (\n    docs_w_1_citation.merge(\n        document_topics[[\"document_id\", \"topic_id\"]],\n        left_on=\"document_id\",\n        right_on=\"document_id\",\n    )\n    .merge(\n        repositories[[\"id\", \"creation_datetime\", \"last_pushed_datetime\"]],\n        left_on=\"repository_id\",\n        right_on=\"id\",\n    )\n    .drop(\n        columns=[\"id\"],\n    )\n    .merge(\n        topics[[\"id\", \"domain_name\"]],\n        left_on=\"topic_id\",\n        right_on=\"id\",\n    )\n    .drop(\n        columns=[\"id\", \"topic_id\"],\n    )\n    .merge(\n        reduced_doc_types,\n        left_on=\"document_id\",\n        right_on=\"document_id\",\n    )\n    .rename(\n        columns={\n            \"domain_name\": \"domain\",\n            \"reduced_doc_type\": \"article_type\",\n        }\n    )\n)\n\n# Drop any documents that have more than one repository (and vice versa)\ndocs_w_1_citation = docs_w_1_citation.drop_duplicates(\n    subset=[\"document_id\"], keep=False\n)\ndocs_w_1_citation = docs_w_1_citation.drop_duplicates(\n    subset=[\"repository_id\"], keep=False\n)\n\n# Iter over articles and get the team composition info\nteam_composition_rows = []\ncc_na_individuals = []\nfor _, row in tqdm(\n    docs_w_1_citation.iterrows(),\n    total=len(docs_w_1_citation),\n):\n    # Get the number of authors\n    author_ids = document_contributors.loc[\n        document_contributors[\"document_id\"] == row[\"document_id\"]\n    ][\"researcher_id\"].unique()\n    n_authors = len(author_ids)\n\n    # Get the number of devs\n    dev_ids = repository_contributors.loc[\n        repository_contributors[\"repository_id\"] == row[\"repository_id\"]\n    ][\"developer_account_id\"].unique()\n    n_devs = len(dev_ids)\n\n    # Get the set of dev_researcher_links for the devs\n    dev_researcher_links = researcher_dev_links.loc[\n        researcher_dev_links[\"developer_account_id\"].isin(dev_ids)\n    ].sort_values(\"predictive_model_confidence\", ascending=False)\n\n    # Drop duplicates by dev_id (keeping first)\n    # as we may have accidently matched the same author to the multiple devs\n    dev_researcher_links = dev_researcher_links.drop_duplicates(\n        subset=[\"researcher_id\"],\n        keep=\"first\",\n    )\n\n    # Drop any dev researcher links that have less than 97% confidence\n    dev_researcher_links = dev_researcher_links.loc[\n        dev_researcher_links[\"predictive_model_confidence\"] &gt;= 0.97\n    ]\n\n    # Get the number of devs who aren't authors\n    n_non_author_devs = 0\n    for dev_id in dev_ids:\n        dev_researcher_ids = dev_researcher_links.loc[\n            dev_researcher_links[\"developer_account_id\"] == dev_id\n        ][\"researcher_id\"].unique()\n\n        if len(dev_researcher_ids) &gt; 3:\n            continue\n\n        if not any(\n            dev_researcher_id in author_ids for dev_researcher_id in dev_researcher_ids\n        ):\n            cc_na_individuals.append(\n                {\n                    \"document_id\": row[\"document_id\"],\n                    \"repository_id\": row[\"repository_id\"],\n                    \"n_authors\": n_authors,\n                    \"n_devs\": n_devs,\n                    \"developer_account_id\": dev_id,\n                    \"researcher_ids_count\": len(dev_researcher_ids),\n                    \"researcher_ids\": dev_researcher_ids.tolist(),\n                }\n            )\n\n# Create cc_na_individuals dataframe\ncc_na_individuals = pd.DataFrame(cc_na_individuals)\n\n# Merge with cc_na_individuals\ncc_na_individuals = cc_na_individuals.merge(\n    docs_w_1_citation,\n    left_on=[\"document_id\", \"repository_id\"],\n    right_on=[\"document_id\", \"repository_id\"],\n)\n\n# Filter out papers with less than 3 authors or 1 dev\ncc_na_individuals = cc_na_individuals.loc[cc_na_individuals[\"n_authors\"] &gt;= 3]\ncc_na_individuals = cc_na_individuals.loc[cc_na_individuals[\"n_devs\"] &gt;= 1]\n\n# Convert datetimes to datetime\ncc_na_individuals[\"publication_date\"] = pd.to_datetime(\n    cc_na_individuals[\"publication_date\"],\n    utc=True,\n)\ncc_na_individuals[\"last_pushed_datetime\"] = pd.to_datetime(\n    cc_na_individuals[\"last_pushed_datetime\"],\n    utc=True,\n)\ncc_na_individuals[\"creation_datetime\"] = pd.to_datetime(\n    cc_na_individuals[\"creation_datetime\"],\n    utc=True,\n)\n\n# Create a \"days_since_last_push\" column\ncc_na_individuals[\"days_from_publication_to_last_push\"] = (\n    cc_na_individuals[\"last_pushed_datetime\"] - cc_na_individuals[\"publication_date\"]\n).dt.days\n\n# Must have a push within 90 days of publication\ncc_na_individuals_no_push_after_pub = cc_na_individuals.loc[\n    cc_na_individuals[\"days_from_publication_to_last_push\"] &lt;= 90\n].copy()\n\n# Get the number of authors which would be the 97th percentile\nn_authors_97th_percentile = cc_na_individuals_no_push_after_pub[\"n_authors\"].quantile(\n    0.97\n)\n\n# Remove rows that are greater than 97th percentile for total authors\ncc_na_individuals_no_push_after_pub = cc_na_individuals_no_push_after_pub.loc[\n    cc_na_individuals_no_push_after_pub[\"n_authors\"]\n    &lt;= cc_na_individuals_no_push_after_pub[\"n_authors\"].quantile(0.97)\n]\n\ncc_na_individuals_no_push_after_pub[\n    [\n        \"document_id\",\n        \"repository_id\",\n        \"developer_account_id\",\n        \"researcher_ids_count\",\n        \"researcher_ids\",\n    ]\n]\n\n100%|██████████| 88551/88551 [02:00&lt;00:00, 735.26it/s]\n\n\n\n\n\n\n\n\n\ndocument_id\nrepository_id\ndeveloper_account_id\nresearcher_ids_count\nresearcher_ids\n\n\n\n\n8\n21\n21\n39\n0\n[]\n\n\n17\n55\n55\n89\n0\n[]\n\n\n47\n82\n81\n164\n0\n[]\n\n\n74\n126\n125\n261\n0\n[]\n\n\n81\n138\n137\n283\n0\n[]\n\n\n...\n...\n...\n...\n...\n...\n\n\n79430\n157265\n153544\n130890\n0\n[]\n\n\n79456\n157277\n153555\n130890\n0\n[]\n\n\n79457\n157279\n153557\n130890\n0\n[]\n\n\n79458\n157280\n153558\n130890\n0\n[]\n\n\n79520\n157301\n153579\n130890\n0\n[]\n\n\n\n\n10121 rows × 5 columns\n\n\n\n\ncc_na_individuals_no_push_after_pub.loc[\n    cc_na_individuals_no_push_after_pub[\"researcher_ids_count\"] &gt; 1\n][\n    [\n        \"document_id\",\n        \"repository_id\",\n        \"developer_account_id\",\n        \"researcher_ids_count\",\n        \"researcher_ids\",\n    ]\n].sort_values(\n    by=\"researcher_ids_count\", ascending=False\n)\n\n\n\n\n\n\n\n\ndocument_id\nrepository_id\ndeveloper_account_id\nresearcher_ids_count\nresearcher_ids\n\n\n\n\n26443\n43020\n41089\n40097\n3\n[72424, 44008, 281382]\n\n\n18545\n25305\n24280\n1576\n3\n[209976, 304662, 91404]\n\n\n39664\n71206\n68300\n28978\n3\n[83498, 174985, 252680]\n\n\n23085\n35663\n34110\n10027\n3\n[97043, 104851, 293211]\n\n\n57340\n110223\n106641\n4140\n3\n[81282, 1093, 87934]\n\n\n...\n...\n...\n...\n...\n...\n\n\n36462\n64096\n61366\n34238\n2\n[60280, 12572]\n\n\n36614\n64322\n61591\n7040\n2\n[185747, 183477]\n\n\n38099\n68257\n65398\n80595\n2\n[6365, 16309]\n\n\n38514\n68939\n66064\n30927\n2\n[53189, 195198]\n\n\n78604\n156533\n152816\n46811\n2\n[269855, 244859]\n\n\n\n\n306 rows × 5 columns\n\n\n\n\nlen(\n    cc_na_individuals_no_push_after_pub.loc[\n        cc_na_individuals_no_push_after_pub[\"researcher_ids_count\"] &gt; 1\n    ]\n) / len(cc_na_individuals_no_push_after_pub)\n\n0.030234166584329612\n\n\n\nlen(\n    cc_na_individuals_no_push_after_pub.loc[\n        cc_na_individuals_no_push_after_pub[\"researcher_ids_count\"] &gt; 1\n    ]\n)\n\n306\n\n\n\ncc_na_individuals_no_push_after_pub.loc[\n    cc_na_individuals_no_push_after_pub[\"researcher_ids_count\"] == 0\n][\n    [\n        \"document_id\",\n        \"repository_id\",\n        \"developer_account_id\",\n        \"researcher_ids_count\",\n        \"researcher_ids\",\n    ]\n].sample(\n    10\n)\n\n\n\n\n\n\n\n\ndocument_id\nrepository_id\ndeveloper_account_id\nresearcher_ids_count\nresearcher_ids\n\n\n\n\n41438\n75381\n72373\n72535\n0\n[]\n\n\n54988\n105139\n101615\n129245\n0\n[]\n\n\n28915\n47837\n45803\n75708\n0\n[]\n\n\n31627\n53557\n51256\n81968\n0\n[]\n\n\n12154\n10266\n9957\n25628\n0\n[]\n\n\n35957\n63047\n60353\n91978\n0\n[]\n\n\n18353\n24868\n23848\n46682\n0\n[]\n\n\n54631\n104535\n101030\n128711\n0\n[]\n\n\n21587\n32648\n31200\n56851\n0\n[]\n\n\n26907\n44112\n42153\n71137\n0\n[]\n\n\n\n\n\n\n\n\ntokens_file = Path.cwd().parent / \".github-tokens.yml\"\ngh_tokens_cycler = GitHubTokensCycler(tokens_file)\n\n# Take a random sample of 20 rows\n# Then, get the:\n# - document id\n# - document DOI\n# - repository id\n# - repository URL\n# - developer account id\n# - developer account URL\n\n\ndef prep_for_annotation(row: pd.Series) -&gt; dict:\n    # Get document details\n    doc_details = documents.loc[documents[\"id\"] == row[\"document_id\"]].iloc[0]\n    doc_doi = doc_details[\"doi\"]\n    doc_doi_url = f\"https://doi.org/{doc_doi}\"\n\n    # Get repository details\n    repo_details = repositories.loc[repositories[\"id\"] == row[\"repository_id\"]].iloc[0]\n    repo_owner = repo_details[\"owner\"]\n    repo_name = repo_details[\"name\"]\n    repo_url = f\"https://github.com/{repo_owner}/{repo_name}\"\n\n    # Get developer account details\n    dev_details = devs.loc[devs[\"id\"] == row[\"developer_account_id\"]].iloc[0]\n    dev_username = dev_details[\"username\"]\n    dev_url = f\"https://github.com/{dev_username}\"\n\n    # Dev commit URL\n    dev_commit_url = (\n        f\"https://github.com/{repo_owner}/{repo_name}/commits?author={dev_username}\"\n    )\n\n    # Get devs contributions to repository\n    results = None\n    try:\n        call_iter = 0\n        while call_iter &lt; 9:\n            api = GhApi(\n                token=next(gh_tokens_cycler),\n            )\n            results = api.repos.get_contributors_stats(\n                owner=repo_owner,\n                repo=repo_name,\n            )\n\n            # If the results are empty, wait and try again\n            if len(results) == 0:\n                time.sleep(20)\n                call_iter += 1\n                results = None\n                continue\n\n            # If we have results, break the loop\n            break\n\n        # If we have results, filter for the developer account\n        if results is None:\n            print(f\"No results found for {repo_url} after {call_iter + 1} iterations.\")\n            raise ValueError(\n                f\"No results found for {repo_url} after {call_iter + 1} iterations.\"\n            )\n\n        # Print number of call iterations\n        print(f\"Number of API call iterations required: {call_iter + 1}\")\n\n        # Get the total number of commits, additions, and deletions for the repo\n        # Iter over each contributor, and iter over each week in \"weeks\", sum all values\n        total_repo_commits = 0\n        total_repo_additions = 0\n        total_repo_deletions = 0\n        for contributor in results:\n            for week in contributor[\"weeks\"]:\n                total_repo_commits += week[\"c\"]\n                total_repo_additions += week[\"a\"]\n                total_repo_deletions += week[\"d\"]\n\n        # If we have results, filter for the developer account\n        filtered_to_contributor = [\n            contributor\n            for contributor in results\n            if contributor[\"author\"][\"login\"] == dev_username\n        ]\n        if len(filtered_to_contributor) &gt; 0:\n            contributor = filtered_to_contributor[0]\n            total_commits = sum(week[\"c\"] for week in contributor[\"weeks\"])\n            total_additions = sum(week[\"a\"] for week in contributor[\"weeks\"])\n            total_deletions = sum(week[\"d\"] for week in contributor[\"weeks\"])\n        else:\n            total_commits = 0\n            total_additions = 0\n            total_deletions = 0\n\n        # Add all of these details to the results\n        return {\n            \"document_id\": row[\"document_id\"],\n            \"document_doi\": doc_doi_url,\n            \"repository_id\": row[\"repository_id\"],\n            \"repository_url\": repo_url,\n            \"developer_account_id\": row[\"developer_account_id\"],\n            \"developer_account_url\": dev_url,\n            \"developer_contribution_url\": dev_commit_url,\n            \"n_commits\": total_commits,\n            \"n_additions\": total_additions,\n            \"n_deletions\": total_deletions,\n            \"total_repo_commits\": total_repo_commits,\n            \"total_repo_additions\": total_repo_additions,\n            \"total_repo_deletions\": total_repo_deletions,\n        }\n\n    except Exception as e:\n        print(f\"Error processing {repo_url} for {dev_username}: {e}\")\n        return {\n            \"document_id\": row[\"document_id\"],\n            \"document_doi\": doc_doi_url,\n            \"repository_id\": row[\"repository_id\"],\n            \"repository_url\": repo_url,\n            \"developer_account_id\": row[\"developer_account_id\"],\n            \"developer_account_url\": dev_url,\n            \"developer_contribution_url\": dev_commit_url,\n        }\n\n\n# Take a random sample of 20 rows\nsampled_rows = cc_na_individuals_no_push_after_pub.drop_duplicates(\n    \"developer_account_id\", keep=\"first\"\n).sample(n=20, random_state=12)\n\nannotation_ready = []\nfor _, row in tqdm(\n    sampled_rows.iterrows(),\n    total=len(sampled_rows),\n):\n    # Prepare the row for annotation\n    annotation_ready.append(prep_for_annotation(row))\n\n    # Convert to DataFrame\n    annotated_df = pd.DataFrame(annotation_ready)\n    annotated_df.to_csv(EXTRA_DATA_DIR / \"cc-na-individuals-sample.csv\", index=False)\n\n# Convert to DataFrame\nannotated_df = pd.DataFrame(annotation_ready)\nannotated_df.to_csv(EXTRA_DATA_DIR / \"cc-na-individuals-sample.csv\", index=False)\n\nExpired GitHub Tokens: 1\n\n\n  0%|          | 0/20 [00:00&lt;?, ?it/s]\n\n\nExpired GitHub Tokens: 1\n\n\n  5%|▌         | 1/20 [00:00&lt;00:09,  2.07it/s]\n\n\nNumber of API call iterations required: 1\n\n\n 10%|█         | 2/20 [00:01&lt;00:11,  1.50it/s]\n\n\nNumber of API call iterations required: 1\n\n\n 15%|█▌        | 3/20 [00:01&lt;00:11,  1.47it/s]\n\n\nNumber of API call iterations required: 1\n\n\n 20%|██        | 4/20 [00:43&lt;04:28, 16.78s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 25%|██▌       | 5/20 [00:44&lt;02:46, 11.07s/it]\n\n\nNumber of API call iterations required: 1\n\n\n 30%|███       | 6/20 [03:50&lt;16:28, 70.62s/it]\n\n\nNo results found for https://github.com/stevenireeves/amrex after 10 iterations.\nError processing https://github.com/stevenireeves/amrex for memmett: No results found for https://github.com/stevenireeves/amrex after 10 iterations.\n\n\n 35%|███▌      | 7/20 [03:50&lt;10:18, 47.61s/it]\n\n\nNumber of API call iterations required: 1\n\n\n 40%|████      | 8/20 [04:31&lt;09:05, 45.45s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 45%|████▌     | 9/20 [07:36&lt;16:19, 89.01s/it]\n\n\nNo results found for https://github.com/stevenireeves/amrex after 10 iterations.\nError processing https://github.com/stevenireeves/amrex for asalmgren: No results found for https://github.com/stevenireeves/amrex after 10 iterations.\n\n\n 50%|█████     | 10/20 [09:18&lt;15:31, 93.13s/it]\n\n\nNumber of API call iterations required: 6\n\n\n 55%|█████▌    | 11/20 [10:20&lt;12:31, 83.49s/it]\n\n\nNumber of API call iterations required: 4\n\n\n 60%|██████    | 12/20 [11:22&lt;10:15, 76.94s/it]\n\n\nNumber of API call iterations required: 4\n\n\n 65%|██████▌   | 13/20 [11:42&lt;06:58, 59.85s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 70%|███████   | 14/20 [12:44&lt;06:01, 60.27s/it]\n\n\nNumber of API call iterations required: 4\n\n\n 75%|███████▌  | 15/20 [13:45&lt;05:02, 60.59s/it]\n\n\nNumber of API call iterations required: 4\n\n\n 80%|████████  | 16/20 [14:06&lt;03:14, 48.66s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 85%|████████▌ | 17/20 [15:07&lt;02:37, 52.51s/it]\n\n\nNumber of API call iterations required: 4\n\n\n 90%|█████████ | 18/20 [15:28&lt;01:25, 42.97s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 95%|█████████▌| 19/20 [16:50&lt;00:54, 54.66s/it]\n\n\nNumber of API call iterations required: 5\n\n\n100%|██████████| 20/20 [17:52&lt;00:00, 53.63s/it]\n\n\nNumber of API call iterations required: 4\n\n\n\n\n\n\n# Take a random sample of 200 rows\nsampled_rows = cc_na_individuals_no_push_after_pub.drop_duplicates(\n    \"developer_account_id\", keep=\"first\"\n).sample(n=200, random_state=12)\n\nannotation_ready = []\nfor _, row in tqdm(\n    sampled_rows.iterrows(),\n    total=len(sampled_rows),\n):\n    # Prepare the row for annotation\n    annotation_ready.append(prep_for_annotation(row))\n\n    # Convert to DataFrame\n    annotated_df = pd.DataFrame(annotation_ready)\n    annotated_df.to_csv(EXTRA_DATA_DIR / \"cc-na-individuals-full.csv\", index=False)\n\n# Convert to DataFrame\nannotated_df = pd.DataFrame(annotation_ready)\nannotated_df.to_csv(EXTRA_DATA_DIR / \"cc-na-individuals-full.csv\", index=False)\n\n  0%|          | 1/200 [00:00&lt;00:47,  4.16it/s]\n\n\nNumber of API call iterations required: 1\n\n\n  1%|          | 2/200 [00:00&lt;01:03,  3.13it/s]\n\n\nNumber of API call iterations required: 1\n\n\n  2%|▏         | 3/200 [00:00&lt;01:07,  2.90it/s]\n\n\nNumber of API call iterations required: 1\n\n\n  2%|▏         | 4/200 [00:01&lt;00:59,  3.32it/s]\n\n\nNumber of API call iterations required: 1\n\n\n  2%|▎         | 5/200 [00:22&lt;25:01,  7.70s/it]\n\n\nNumber of API call iterations required: 2\n\n\n  3%|▎         | 6/200 [03:26&lt;3:39:17, 67.82s/it]\n\n\nNo results found for https://github.com/stevenireeves/amrex after 10 iterations.\nError processing https://github.com/stevenireeves/amrex for memmett: No results found for https://github.com/stevenireeves/amrex after 10 iterations.\n\n\n  4%|▎         | 7/200 [03:27&lt;2:27:18, 45.79s/it]\n\n\nNumber of API call iterations required: 1\n\n\n  4%|▍         | 8/200 [03:27&lt;1:40:27, 31.39s/it]\n\n\nNumber of API call iterations required: 1\n\n\n  4%|▍         | 9/200 [06:33&lt;4:13:30, 79.64s/it]\n\n\nNo results found for https://github.com/stevenireeves/amrex after 10 iterations.\nError processing https://github.com/stevenireeves/amrex for asalmgren: No results found for https://github.com/stevenireeves/amrex after 10 iterations.\n\n\n  5%|▌         | 10/200 [06:33&lt;2:54:35, 55.13s/it]\n\n\nNumber of API call iterations required: 1\n\n\n  6%|▌         | 11/200 [06:33&lt;2:00:45, 38.33s/it]\n\n\nNumber of API call iterations required: 1\n\n\n  6%|▌         | 12/200 [06:34&lt;1:24:06, 26.84s/it]\n\n\nNumber of API call iterations required: 1\n\n\n  6%|▋         | 13/200 [06:54&lt;1:17:42, 24.93s/it]\n\n\nNumber of API call iterations required: 2\n\n\n  7%|▋         | 14/200 [07:15&lt;1:13:11, 23.61s/it]\n\n\nNumber of API call iterations required: 2\n\n\n  8%|▊         | 15/200 [07:56&lt;1:29:09, 28.92s/it]\n\n\nNumber of API call iterations required: 3\n\n\n  8%|▊         | 16/200 [07:57&lt;1:02:29, 20.38s/it]\n\n\nNumber of API call iterations required: 1\n\n\n  8%|▊         | 17/200 [07:57&lt;43:51, 14.38s/it]  \n\n\nNumber of API call iterations required: 1\n\n\n  9%|▉         | 18/200 [10:41&lt;2:59:29, 59.17s/it]\n\n\nNumber of API call iterations required: 9\n\n\n 10%|▉         | 19/200 [10:41&lt;2:05:08, 41.48s/it]\n\n\nNumber of API call iterations required: 1\n\n\n 10%|█         | 20/200 [10:41&lt;1:27:21, 29.12s/it]\n\n\nNumber of API call iterations required: 1\n\n\n 10%|█         | 21/200 [11:02&lt;1:19:16, 26.57s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 11%|█         | 22/200 [12:03&lt;1:49:49, 37.02s/it]\n\n\nNumber of API call iterations required: 4\n\n\n 12%|█▏        | 23/200 [13:05&lt;2:10:57, 44.39s/it]\n\n\nNumber of API call iterations required: 4\n\n\n 12%|█▏        | 24/200 [13:46&lt;2:07:15, 43.38s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 12%|█▎        | 25/200 [14:07&lt;1:46:46, 36.61s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 13%|█▎        | 26/200 [14:27&lt;1:32:24, 31.87s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 14%|█▎        | 27/200 [14:48&lt;1:22:06, 28.47s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 14%|█▍        | 28/200 [16:09&lt;2:07:15, 44.39s/it]\n\n\nNumber of API call iterations required: 5\n\n\n 14%|█▍        | 29/200 [16:50&lt;2:03:26, 43.31s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 15%|█▌        | 30/200 [17:32&lt;2:01:02, 42.72s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 16%|█▌        | 31/200 [18:33&lt;2:15:57, 48.27s/it]\n\n\nNumber of API call iterations required: 4\n\n\n 16%|█▌        | 32/200 [18:54&lt;1:52:10, 40.06s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 16%|█▋        | 33/200 [19:14&lt;1:35:17, 34.24s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 17%|█▋        | 34/200 [20:36&lt;2:13:59, 48.43s/it]\n\n\nNumber of API call iterations required: 5\n\n\n 18%|█▊        | 35/200 [20:57&lt;1:50:31, 40.19s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 18%|█▊        | 36/200 [21:39&lt;1:51:00, 40.61s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 18%|█▊        | 37/200 [22:20&lt;1:50:54, 40.82s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 19%|█▉        | 38/200 [23:22&lt;2:07:50, 47.35s/it]\n\n\nNumber of API call iterations required: 4\n\n\n 20%|█▉        | 39/200 [23:43&lt;1:45:34, 39.35s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 20%|██        | 40/200 [24:04&lt;1:30:15, 33.84s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 20%|██        | 41/200 [25:06&lt;1:52:21, 42.40s/it]\n\n\nNumber of API call iterations required: 4\n\n\n 21%|██        | 42/200 [26:08&lt;2:06:38, 48.09s/it]\n\n\nNumber of API call iterations required: 4\n\n\n 22%|██▏       | 43/200 [26:29&lt;1:44:52, 40.08s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 22%|██▏       | 44/200 [26:50&lt;1:29:28, 34.42s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 22%|██▎       | 45/200 [27:32&lt;1:34:14, 36.48s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 23%|██▎       | 46/200 [28:13&lt;1:37:27, 37.97s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 24%|██▎       | 47/200 [28:34&lt;1:23:45, 32.84s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 24%|██▍       | 48/200 [28:55&lt;1:14:05, 29.25s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 24%|██▍       | 49/200 [29:16&lt;1:07:10, 26.69s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 25%|██▌       | 50/200 [29:56&lt;1:17:19, 30.93s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 26%|██▌       | 51/200 [30:58&lt;1:39:17, 39.99s/it]\n\n\nNumber of API call iterations required: 4\n\n\n 26%|██▌       | 52/200 [31:39&lt;1:39:23, 40.30s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 26%|██▋       | 53/200 [32:40&lt;1:54:22, 46.68s/it]\n\n\nNumber of API call iterations required: 4\n\n\n 27%|██▋       | 54/200 [33:22&lt;1:49:42, 45.09s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 28%|██▊       | 55/200 [34:23&lt;2:00:51, 50.01s/it]\n\n\nNumber of API call iterations required: 4\n\n\n 28%|██▊       | 56/200 [34:44&lt;1:38:57, 41.23s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 28%|██▊       | 57/200 [35:05&lt;1:23:49, 35.17s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 29%|██▉       | 58/200 [35:26&lt;1:13:08, 30.91s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 30%|██▉       | 59/200 [36:07&lt;1:19:51, 33.99s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 30%|███       | 60/200 [36:27&lt;1:09:51, 29.94s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 30%|███       | 61/200 [36:48&lt;1:02:57, 27.18s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 31%|███       | 62/200 [37:30&lt;1:12:52, 31.69s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 32%|███▏      | 63/200 [37:51&lt;1:04:53, 28.42s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 32%|███▏      | 64/200 [38:32&lt;1:12:58, 32.20s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 32%|███▎      | 65/200 [39:34&lt;1:32:11, 40.98s/it]\n\n\nNumber of API call iterations required: 4\n\n\n 33%|███▎      | 66/200 [40:57&lt;1:59:36, 53.55s/it]\n\n\nNumber of API call iterations required: 5\n\n\n 34%|███▎      | 67/200 [41:17&lt;1:36:45, 43.65s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 34%|███▍      | 68/200 [41:58&lt;1:34:07, 42.78s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 34%|███▍      | 69/200 [43:00&lt;1:45:50, 48.47s/it]\n\n\nNumber of API call iterations required: 4\n\n\n 35%|███▌      | 70/200 [44:01&lt;1:53:39, 52.46s/it]\n\n\nNumber of API call iterations required: 4\n\n\n 36%|███▌      | 71/200 [45:03&lt;1:58:48, 55.26s/it]\n\n\nNumber of API call iterations required: 4\n\n\n 36%|███▌      | 72/200 [46:26&lt;2:15:41, 63.61s/it]\n\n\nNumber of API call iterations required: 5\n\n\n 36%|███▋      | 73/200 [47:07&lt;2:00:26, 56.90s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 37%|███▋      | 74/200 [47:28&lt;1:36:39, 46.03s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 38%|███▊      | 75/200 [50:35&lt;3:04:05, 88.36s/it]\n\n\nNo results found for https://github.com/stevenireeves/amrex after 10 iterations.\nError processing https://github.com/stevenireeves/amrex for ax3l: No results found for https://github.com/stevenireeves/amrex after 10 iterations.\n\n\n 38%|███▊      | 76/200 [50:56&lt;2:20:34, 68.02s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 38%|███▊      | 77/200 [51:17&lt;1:50:19, 53.82s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 39%|███▉      | 78/200 [51:37&lt;1:29:08, 43.84s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 40%|███▉      | 79/200 [52:59&lt;1:51:21, 55.22s/it]\n\n\nNumber of API call iterations required: 5\n\n\n 40%|████      | 80/200 [54:00&lt;1:54:01, 57.01s/it]\n\n\nNumber of API call iterations required: 4\n\n\n 40%|████      | 81/200 [54:21&lt;1:31:33, 46.16s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 41%|████      | 82/200 [55:02&lt;1:28:02, 44.77s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 42%|████▏     | 83/200 [55:03&lt;1:01:21, 31.46s/it]\n\n\nNumber of API call iterations required: 1\n\n\n 42%|████▏     | 84/200 [58:06&lt;2:28:47, 76.96s/it]\n\n\nNo results found for https://github.com/mengqidyangge/hierkd after 10 iterations.\nError processing https://github.com/mengqidyangge/hierkd for mengqiDyangge: No results found for https://github.com/mengqidyangge/hierkd after 10 iterations.\n\n\n 42%|████▎     | 85/200 [58:27&lt;1:55:05, 60.05s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 43%|████▎     | 86/200 [58:47&lt;1:31:35, 48.20s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 44%|████▎     | 87/200 [1:00:09&lt;1:49:59, 58.40s/it]\n\n\nNumber of API call iterations required: 5\n\n\n 44%|████▍     | 88/200 [1:01:11&lt;1:50:40, 59.29s/it]\n\n\nNumber of API call iterations required: 4\n\n\n 44%|████▍     | 89/200 [1:01:52&lt;1:39:34, 53.82s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 45%|████▌     | 90/200 [1:03:35&lt;2:05:50, 68.64s/it]\n\n\nNumber of API call iterations required: 6\n\n\n 46%|████▌     | 91/200 [1:04:16&lt;1:49:32, 60.30s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 46%|████▌     | 92/200 [1:04:37&lt;1:27:35, 48.66s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 46%|████▋     | 93/200 [1:04:58&lt;1:12:04, 40.42s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 47%|████▋     | 94/200 [1:05:19&lt;1:00:51, 34.44s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 48%|████▊     | 95/200 [1:06:41&lt;1:25:08, 48.66s/it]\n\n\nNumber of API call iterations required: 5\n\n\n 48%|████▊     | 96/200 [1:07:22&lt;1:20:26, 46.41s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 48%|████▊     | 97/200 [1:10:26&lt;2:30:20, 87.58s/it]\n\n\nNo results found for https://github.com/uhh-iss/zeek after 10 iterations.\nError processing https://github.com/uhh-iss/zeek for dnthayer: No results found for https://github.com/uhh-iss/zeek after 10 iterations.\n\n\n 49%|████▉     | 98/200 [1:11:08&lt;2:05:36, 73.89s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 50%|████▉     | 99/200 [1:12:09&lt;1:58:16, 70.26s/it]\n\n\nNumber of API call iterations required: 4\n\n\n 50%|█████     | 100/200 [1:13:12&lt;1:53:09, 67.90s/it]\n\n\nNumber of API call iterations required: 4\n\n\n 50%|█████     | 101/200 [1:13:32&lt;1:28:42, 53.76s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 51%|█████     | 102/200 [1:14:14&lt;1:21:41, 50.01s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 52%|█████▏    | 103/200 [1:14:55&lt;1:16:36, 47.39s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 52%|█████▏    | 104/200 [1:16:37&lt;1:42:09, 63.85s/it]\n\n\nNumber of API call iterations required: 6\n\n\n 52%|█████▎    | 105/200 [1:16:58&lt;1:20:33, 50.87s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 53%|█████▎    | 106/200 [1:20:01&lt;2:22:01, 90.66s/it]\n\n\nNo results found for https://github.com/disi-unibo-nlp/medgenie after 10 iterations.\nError processing https://github.com/disi-unibo-nlp/medgenie for disi-unibo-nlp: No results found for https://github.com/disi-unibo-nlp/medgenie after 10 iterations.\n\n\n 54%|█████▎    | 107/200 [1:20:22&lt;1:47:56, 69.64s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 54%|█████▍    | 108/200 [1:21:03&lt;1:33:35, 61.04s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 55%|█████▍    | 109/200 [1:21:45&lt;1:24:03, 55.43s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 55%|█████▌    | 110/200 [1:22:07&lt;1:07:47, 45.19s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 56%|█████▌    | 111/200 [1:22:07&lt;47:01, 31.71s/it]  \n\n\nNumber of API call iterations required: 1\n\n\n 56%|█████▌    | 112/200 [1:22:48&lt;50:32, 34.46s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 56%|█████▋    | 113/200 [1:23:50&lt;1:01:52, 42.68s/it]\n\n\nNumber of API call iterations required: 4\n\n\n 57%|█████▋    | 114/200 [1:24:11&lt;52:03, 36.33s/it]  \n\n\nNumber of API call iterations required: 2\n\n\n 57%|█████▊    | 115/200 [1:25:33&lt;1:10:53, 50.04s/it]\n\n\nNumber of API call iterations required: 5\n\n\n 58%|█████▊    | 116/200 [1:26:35&lt;1:15:05, 53.64s/it]\n\n\nNumber of API call iterations required: 4\n\n\n 58%|█████▊    | 117/200 [1:27:37&lt;1:17:27, 55.99s/it]\n\n\nNumber of API call iterations required: 4\n\n\n 59%|█████▉    | 118/200 [1:28:18&lt;1:10:28, 51.57s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 60%|█████▉    | 119/200 [1:28:19&lt;49:07, 36.39s/it]  \n\n\nNumber of API call iterations required: 1\n\n\n 60%|██████    | 120/200 [1:29:00&lt;50:23, 37.79s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 60%|██████    | 121/200 [1:30:22&lt;1:07:05, 50.96s/it]\n\n\nNumber of API call iterations required: 5\n\n\n 61%|██████    | 122/200 [1:33:28&lt;1:58:58, 91.52s/it]\n\n\nNo results found for https://github.com/stevenireeves/amrex after 10 iterations.\nError processing https://github.com/stevenireeves/amrex for adam-m-jcbs: No results found for https://github.com/stevenireeves/amrex after 10 iterations.\n\n\n 62%|██████▏   | 123/200 [1:35:10&lt;2:01:38, 94.79s/it]\n\n\nNumber of API call iterations required: 6\n\n\n 62%|██████▏   | 124/200 [1:35:31&lt;1:31:59, 72.63s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 62%|██████▎   | 125/200 [1:36:13&lt;1:19:09, 63.33s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 63%|██████▎   | 126/200 [1:36:34&lt;1:02:29, 50.67s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 64%|██████▎   | 127/200 [1:37:15&lt;58:19, 47.93s/it]  \n\n\nNumber of API call iterations required: 3\n\n\n 64%|██████▍   | 128/200 [1:37:57&lt;55:09, 45.96s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 64%|██████▍   | 129/200 [1:39:39&lt;1:14:30, 62.96s/it]\n\n\nNumber of API call iterations required: 6\n\n\n 65%|██████▌   | 130/200 [1:40:00&lt;58:43, 50.34s/it]  \n\n\nNumber of API call iterations required: 2\n\n\n 66%|██████▌   | 131/200 [1:40:42&lt;54:47, 47.64s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 66%|██████▌   | 132/200 [1:41:24&lt;52:13, 46.08s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 66%|██████▋   | 133/200 [1:42:26&lt;56:41, 50.77s/it]\n\n\nNumber of API call iterations required: 4\n\n\n 67%|██████▋   | 134/200 [1:44:08&lt;1:12:54, 66.29s/it]\n\n\nNumber of API call iterations required: 6\n\n\n 68%|██████▊   | 135/200 [1:44:50&lt;1:03:47, 58.89s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 68%|██████▊   | 136/200 [1:45:31&lt;57:06, 53.54s/it]  \n\n\nNumber of API call iterations required: 3\n\n\n 68%|██████▊   | 137/200 [1:47:54&lt;1:24:32, 80.51s/it]\n\n\nNumber of API call iterations required: 8\n\n\n 69%|██████▉   | 138/200 [1:48:35&lt;1:10:58, 68.68s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 70%|██████▉   | 139/200 [1:48:57&lt;55:36, 54.69s/it]  \n\n\nNumber of API call iterations required: 2\n\n\n 70%|███████   | 140/200 [1:49:18&lt;44:28, 44.47s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 70%|███████   | 141/200 [1:49:59&lt;42:44, 43.47s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 71%|███████   | 142/200 [1:51:42&lt;59:04, 61.11s/it]\n\n\nNumber of API call iterations required: 6\n\n\n 72%|███████▏  | 143/200 [1:53:44&lt;1:15:40, 79.66s/it]\n\n\nNumber of API call iterations required: 7\n\n\n 72%|███████▏  | 144/200 [1:54:07&lt;58:14, 62.41s/it]  \n\n\nNumber of API call iterations required: 2\n\n\n 72%|███████▎  | 145/200 [1:54:28&lt;45:57, 50.14s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 73%|███████▎  | 146/200 [1:55:50&lt;53:43, 59.70s/it]\n\n\nNumber of API call iterations required: 5\n\n\n 74%|███████▎  | 147/200 [1:56:11&lt;42:23, 48.00s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 74%|███████▍  | 148/200 [1:57:12&lt;45:07, 52.07s/it]\n\n\nNumber of API call iterations required: 4\n\n\n 74%|███████▍  | 149/200 [1:58:14&lt;46:38, 54.88s/it]\n\n\nNumber of API call iterations required: 4\n\n\n 75%|███████▌  | 150/200 [1:58:34&lt;37:09, 44.60s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 76%|███████▌  | 151/200 [1:58:55&lt;30:34, 37.44s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 76%|███████▌  | 152/200 [1:59:36&lt;30:46, 38.47s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 76%|███████▋  | 153/200 [2:00:17&lt;30:44, 39.23s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 77%|███████▋  | 154/200 [2:01:18&lt;35:06, 45.79s/it]\n\n\nNumber of API call iterations required: 4\n\n\n 78%|███████▊  | 155/200 [2:01:59&lt;33:15, 44.34s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 78%|███████▊  | 156/200 [2:03:00&lt;36:14, 49.41s/it]\n\n\nNumber of API call iterations required: 4\n\n\n 78%|███████▊  | 157/200 [2:03:21&lt;29:14, 40.79s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 79%|███████▉  | 158/200 [2:05:24&lt;45:47, 65.41s/it]\n\n\nNumber of API call iterations required: 7\n\n\n 80%|███████▉  | 159/200 [2:07:46&lt;1:00:30, 88.55s/it]\n\n\nNumber of API call iterations required: 8\n\n\n 80%|████████  | 160/200 [2:08:49&lt;53:47, 80.70s/it]  \n\n\nNumber of API call iterations required: 4\n\n\n 80%|████████  | 161/200 [2:09:51&lt;48:54, 75.25s/it]\n\n\nNumber of API call iterations required: 4\n\n\n 81%|████████  | 162/200 [2:11:35&lt;52:59, 83.66s/it]\n\n\nNumber of API call iterations required: 6\n\n\n 82%|████████▏ | 163/200 [2:12:16&lt;43:44, 70.92s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 82%|████████▏ | 164/200 [2:15:03&lt;59:57, 99.92s/it]\n\n\nNumber of API call iterations required: 9\n\n\n 82%|████████▎ | 165/200 [2:15:46&lt;48:18, 82.82s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 83%|████████▎ | 166/200 [2:16:27&lt;39:49, 70.27s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 84%|████████▎ | 167/200 [2:17:49&lt;40:28, 73.60s/it]\n\n\nNumber of API call iterations required: 5\n\n\n 84%|████████▍ | 168/200 [2:19:34&lt;44:19, 83.11s/it]\n\n\nNumber of API call iterations required: 6\n\n\n 84%|████████▍ | 169/200 [2:19:55&lt;33:14, 64.33s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 85%|████████▌ | 170/200 [2:20:15&lt;25:37, 51.24s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 86%|████████▌ | 171/200 [2:20:56&lt;23:16, 48.17s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 86%|████████▌ | 172/200 [2:21:37&lt;21:27, 45.97s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 86%|████████▋ | 173/200 [2:21:58&lt;17:21, 38.56s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 87%|████████▋ | 174/200 [2:22:19&lt;14:22, 33.18s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 88%|████████▊ | 175/200 [2:23:41&lt;19:53, 47.75s/it]\n\n\nNumber of API call iterations required: 5\n\n\n 88%|████████▊ | 176/200 [2:23:42&lt;13:29, 33.73s/it]\n\n\nNumber of API call iterations required: 1\n\n\n 88%|████████▊ | 177/200 [2:25:03&lt;18:26, 48.09s/it]\n\n\nNumber of API call iterations required: 5\n\n\n 89%|████████▉ | 178/200 [2:27:06&lt;25:51, 70.54s/it]\n\n\nNumber of API call iterations required: 7\n\n\n 90%|████████▉ | 179/200 [2:27:48&lt;21:37, 61.78s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 90%|█████████ | 180/200 [2:28:29&lt;18:32, 55.62s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 90%|█████████ | 181/200 [2:28:50&lt;14:18, 45.21s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 91%|█████████ | 182/200 [2:29:51&lt;15:03, 50.17s/it]\n\n\nNumber of API call iterations required: 4\n\n\n 92%|█████████▏| 183/200 [2:30:33&lt;13:27, 47.48s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 92%|█████████▏| 184/200 [2:31:14&lt;12:09, 45.62s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 92%|█████████▎| 185/200 [2:31:17&lt;08:12, 32.80s/it]\n\n\nNumber of API call iterations required: 1\n\n\n 93%|█████████▎| 186/200 [2:31:58&lt;08:16, 35.43s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 94%|█████████▎| 187/200 [2:35:03&lt;17:21, 80.12s/it]\n\n\nNo results found for https://github.com/uhh-iss/zeek after 10 iterations.\nError processing https://github.com/uhh-iss/zeek for jrolli: No results found for https://github.com/uhh-iss/zeek after 10 iterations.\n\n\n 94%|█████████▍| 188/200 [2:36:04&lt;14:52, 74.40s/it]\n\n\nNumber of API call iterations required: 4\n\n\n 94%|█████████▍| 189/200 [2:36:25&lt;10:41, 58.29s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 95%|█████████▌| 190/200 [2:37:06&lt;08:51, 53.19s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 96%|█████████▌| 191/200 [2:39:08&lt;11:05, 73.90s/it]\n\n\nNumber of API call iterations required: 7\n\n\n 96%|█████████▌| 192/200 [2:39:29&lt;07:43, 57.94s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 96%|█████████▋| 193/200 [2:39:51&lt;05:29, 47.08s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 97%|█████████▋| 194/200 [2:40:12&lt;03:55, 39.29s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 98%|█████████▊| 195/200 [2:40:13&lt;02:19, 27.81s/it]\n\n\nNumber of API call iterations required: 1\n\n\n 98%|█████████▊| 196/200 [2:40:54&lt;02:07, 31.83s/it]\n\n\nNumber of API call iterations required: 3\n\n\n 98%|█████████▊| 197/200 [2:41:15&lt;01:26, 28.68s/it]\n\n\nNumber of API call iterations required: 2\n\n\n 99%|█████████▉| 198/200 [2:41:17&lt;00:41, 20.73s/it]\n\n\nNumber of API call iterations required: 1\n\n\n100%|█████████▉| 199/200 [2:42:39&lt;00:39, 39.04s/it]\n\n\nNumber of API call iterations required: 5\n\n\n100%|██████████| 200/200 [2:44:22&lt;00:00, 49.31s/it]\n\n\nNumber of API call iterations required: 6"
  },
  {
    "objectID": "extra-notebooks/cc-na-analysis.html",
    "href": "extra-notebooks/cc-na-analysis.html",
    "title": "",
    "section": "",
    "text": "from pathlib import Path\n\nimport pandas as pd\n\nEXTRA_DATA_DIR = Path.cwd().parent / \"extra-data\"\n\ninitial_set = pd.read_csv(EXTRA_DATA_DIR / \"cc-na-initial-set.csv\")\neva_set = pd.read_csv(EXTRA_DATA_DIR / \"cc-na-eva.csv\")\nnic_set = pd.read_csv(EXTRA_DATA_DIR / \"cc-na-nic.csv\")\nnic_set[\"is_bot_account\"] = \"no\"\nnic_set = nic_set.drop(\n    columns=[col for col in nic_set.columns if col.startswith(\"Unnamed:\")]\n)\n\n# Combine the datasets\ncombined_set = pd.concat(\n    [initial_set, eva_set, nic_set], ignore_index=True\n).reset_index(drop=True)\ncombined_set\n\n\n\n\n\n\n\n\ndocument_id\ndocument_doi\nrepository_id\nrepository_url\ndeveloper_account_id\ndeveloper_account_url\ndeveloper_contribution_url\nn_commits\nn_additions\nn_deletions\ntotal_repo_commits\ntotal_repo_additions\ntotal_repo_deletions\ncontrib_type\nis_actually_author\nis_bot_account\nnotes\n\n\n\n\n0\n107848\nhttps://doi.org/10.1088/1748-0221/18/11/P11028\n104357\nhttps://github.com/dnicotra/trackhhl\n131471\nhttps://github.com/dmark04\nhttps://github.com/dnicotra/trackhhl/commits?a...\n0.0\n0.0\n0.0\n11.0\n924.0\n712.0\nother\nno\nno\nNaN\n\n\n1\n44180\nhttps://doi.org/10.1371/journal.pone.0263125\n42221\nhttps://github.com/lukemelas/efficientnet-pytorch\n71221\nhttps://github.com/robotrapta\nhttps://github.com/lukemelas/efficientnet-pyto...\n1.0\n1.0\n1.0\n120.0\n8902.0\n3609.0\ndoc\nunclear\nno\nNaN\n\n\n2\n33751\nhttps://doi.org/10.29012/jpc.870\n32267\nhttps://github.com/microsoft/prv_accountant\n36697\nhttps://github.com/s-zanella\nhttps://github.com/microsoft/prv_accountant/co...\n1.0\n1.0\n1.0\n35.0\n3604.0\n883.0\ncode\nno\nno\nNaN\n\n\n3\n56650\nhttps://doi.org/10.1007/s41095-021-0229-5\n54205\nhttps://github.com/menghaoguo/pct\n2123\nhttps://github.com/Menghao999\nhttps://github.com/menghaoguo/pct/commits?auth...\n1.0\n11.0\n1.0\n30.0\n1201.0\n65.0\ncode\nyes\nno\nNaN\n\n\n4\n21394\nhttps://doi.org/10.1109/TPAMI.2017.2769085\n20529\nhttps://github.com/hbilen/dynamic-image-nets\n42202\nhttps://github.com/Blssel\nhttps://github.com/hbilen/dynamic-image-nets/c...\n1.0\n2.0\n2.0\n1403.0\n163258.0\n86480.0\nother\nno\nno\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n195\n41129\nhttps://doi.org/10.1038/s41598-021-87709-7\n39281\nhttps://github.com/gcampede/terrorism-metagraphs\n67511\nhttps://github.com/gcampede\nhttps://github.com/gcampede/terrorism-metagrap...\n17.0\n30595.0\n29.0\n17.0\n30595.0\n29.0\ncode\nyes\nno\nFirst author repo owner\n\n\n196\n61912\nhttps://doi.org/10.1145/3511808.3557275\n59280\nhttps://github.com/intellabs/nlp-architect\n90822\nhttps://github.com/chinnikrishna2231\nhttps://github.com/intellabs/nlp-architect/com...\n4.0\n1479.0\n125.0\n451.0\n601777.0\n111623.0\ncode\nno\nno\nNaN\n\n\n197\n53557\nhttps://doi.org/10.1109/ACCESS.2021.3112879\n51256\nhttps://github.com/gtlidar/tamp-manipulation\n81979\nhttps://github.com/edrumwri\nhttps://github.com/gtlidar/tamp-manipulation/c...\n176.0\n0.0\n0.0\n25004.0\n0.0\n0.0\ncode\nno\nno\nAlt branch\n\n\n198\n155656\nhttps://doi.org/10.1613/jair.1.13326\n151946\nhttps://github.com/kebaek/minigrid\n166678\nhttps://github.com/YX-S-Z\nhttps://github.com/kebaek/minigrid/commits?aut...\n6.0\n40.0\n13.0\n41.0\n10517.0\n1970.0\ncode\nyes\nno\nFirst author\n\n\n199\n136972\nhttps://doi.org/10.1109/ICASSP49357.2023.10096189\n133157\nhttps://github.com/splend1d/t5lephone\n73942\nhttps://github.com/Splend1d\nhttps://github.com/splend1d/t5lephone/commits?...\n17.0\n108653.0\n88215.0\n19.0\n108747.0\n88216.0\ncode\nyes\nno\nRepo owner\n\n\n\n\n200 rows × 17 columns\n\n\n\n\n# Get the count and pct of bot accounts\nbot_count = combined_set[\"is_bot_account\"].value_counts()\nbot_pct = combined_set[\"is_bot_account\"].value_counts(normalize=True) * 100\nbot_count, bot_pct\n\n(is_bot_account\n no     198\n yes      2\n Name: count, dtype: int64,\n is_bot_account\n no     99.0\n yes     1.0\n Name: proportion, dtype: float64)\n\n\n\n# Get the count and pct of actual authors we missed (is_actually_author)\nauthor_cls_counts = combined_set[\"is_actually_author\"].value_counts()\nauthor_cls_pct = combined_set[\"is_actually_author\"].value_counts(normalize=True) * 100\nauthor_cls_counts, author_cls_pct\n\n(is_actually_author\n no         78\n unclear    61\n yes        61\n Name: count, dtype: int64,\n is_actually_author\n no         39.0\n unclear    30.5\n yes        30.5\n Name: proportion, dtype: float64)\n\n\n\n# Drop bot accounts\n# Then groupby author cls and get the count of contribution type (contrib_type)\ncombined_set.loc[combined_set[\"is_bot_account\"] == \"no\"].groupby(\"is_actually_author\")[\n    \"contrib_type\"\n].value_counts().unstack().fillna(0).astype(int)\n\n\n\n\n\n\n\ncontrib_type\ncode\ndoc\nother\n\n\nis_actually_author\n\n\n\n\n\n\n\nno\n59\n13\n4\n\n\nunclear\n50\n8\n3\n\n\nyes\n49\n12\n0\n\n\n\n\n\n\n\n\n# Same thing but with pct\ncombined_set.loc[combined_set[\"is_bot_account\"] == \"no\"].groupby(\"is_actually_author\")[\n    \"contrib_type\"\n].value_counts(normalize=True).unstack().fillna(0) * 100\n\n\n\n\n\n\n\ncontrib_type\ncode\ndoc\nother\n\n\nis_actually_author\n\n\n\n\n\n\n\nno\n77.631579\n17.105263\n5.263158\n\n\nunclear\n81.967213\n13.114754\n4.918033\n\n\nyes\n80.327869\n19.672131\n0.000000\n\n\n\n\n\n\n\n\ndef _get_stats_for_author_cls(\n    data: pd.DataFrame, author_cls: str | list[str], repo_owner_filter: str = \"all\"\n) -&gt; pd.DataFrame:\n    # Get repo owner from repository_url\n    data[\"repo_owner\"] = data[\"repository_url\"].str.split(\"/\").str[3].str.lower()\n    # Get contributor name from developer_account_url\n    data[\"contributor_name\"] = (\n        data[\"developer_account_url\"].str.split(\"/\").str[3].str.lower()\n    )\n\n    # Handle ignore_repo_owners\n    if repo_owner_filter == \"none\":\n        # Filter out rows where repo_owner == contributor_name\n        data = data.loc[data[\"repo_owner\"] != data[\"contributor_name\"]].copy()\n    elif repo_owner_filter == \"only\":\n        # Filter to only rows where repo_owner == contributor_name\n        data = data.loc[data[\"repo_owner\"] == data[\"contributor_name\"]].copy()\n    # If repo_owner_filter is \"all\", do nothing\n\n    # Filter to author cls\n    if isinstance(author_cls, str):\n        # If author_cls is a string, filter to that single class\n        data = data.loc[data[\"is_actually_author\"] == author_cls].copy()\n    elif isinstance(author_cls, list):\n        # If author_cls is a list, filter to those classes\n        data = data.loc[data[\"is_actually_author\"].isin(author_cls)].copy()\n\n    # Scope down to just is_actually_author == author_cls\n    # And which contribution type is \"code\"\n    # Take the quantiles, mean, and std of their commits (n_commits)\n    # divided by total commits (total_repo_commits)\n    commit_stats = (\n        data.loc[(data[\"is_bot_account\"] == \"no\") & (data[\"contrib_type\"] == \"code\")]\n        .dropna(subset=[\"n_commits\", \"total_repo_commits\"])\n        .apply(lambda x: x[\"n_commits\"] / x[\"total_repo_commits\"], axis=1)\n        .describe()\n        .round(3)\n    )\n\n    # Same thing but for n_additions and total_repo_additions\n    addition_stats = (\n        data.loc[\n            (data[\"is_bot_account\"] == \"no\")\n            & (data[\"contrib_type\"] == \"code\")\n            & (data[\"total_repo_additions\"] &gt; 0)\n        ]\n        .dropna(subset=[\"n_additions\", \"total_repo_additions\"])\n        .apply(lambda x: x[\"n_additions\"] / x[\"total_repo_additions\"], axis=1)\n        .describe()\n        .round(3)\n    )\n\n    # Same thing but with n_deletions and total_repo_deletions\n    deletion_stats = (\n        data.loc[\n            (data[\"is_bot_account\"] == \"no\")\n            & (data[\"contrib_type\"] == \"code\")\n            & (data[\"total_repo_deletions\"] &gt; 0)\n        ]\n        .dropna(subset=[\"n_deletions\", \"total_repo_deletions\"])\n        .apply(lambda x: x[\"n_deletions\"] / x[\"total_repo_deletions\"], axis=1)\n        .describe()\n        .round(3)\n    )\n\n    # Combine n_additions and n_deletions into a single column\n    # Combined total additions and deletions into a single column\n    data[\"n_additions_deletions\"] = data[\"n_additions\"].fillna(0) + data[\n        \"n_deletions\"\n    ].fillna(0)\n    data[\"total_repo_additions_deletions\"] = data[\"total_repo_additions\"].fillna(\n        0\n    ) + data[\"total_repo_deletions\"].fillna(0)\n    # Calculate the ratio of additions and deletions to total repo additions and deletions\n    abs_stats = (\n        data.loc[\n            (data[\"is_bot_account\"] == \"no\")\n            & (data[\"contrib_type\"] == \"code\")\n            & (data[\"total_repo_additions_deletions\"] &gt; 0)\n        ]\n        .dropna(subset=[\"n_additions_deletions\", \"total_repo_additions_deletions\"])\n        .apply(\n            lambda x: x[\"n_additions_deletions\"] / x[\"total_repo_additions_deletions\"],\n            axis=1,\n        )\n        .describe()\n        .round(3)\n    )\n\n    # Create frame\n    stats_frame = pd.DataFrame(\n        {\n            \"commit_stats\": commit_stats,\n            \"addition_stats\": addition_stats,\n            \"deletion_stats\": deletion_stats,\n            \"abs_stats\": abs_stats,\n        }\n    ).T\n\n    return stats_frame\n\n\n_get_stats_for_author_cls(combined_set, \"no\")\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ncommit_stats\n53.0\n0.178\n0.307\n0.001\n0.007\n0.029\n0.107\n1.0\n\n\naddition_stats\n51.0\n0.227\n0.380\n0.000\n0.001\n0.007\n0.192\n1.0\n\n\ndeletion_stats\n51.0\n0.193\n0.358\n0.000\n0.000\n0.006\n0.149\n1.0\n\n\nabs_stats\n51.0\n0.222\n0.379\n0.000\n0.001\n0.010\n0.144\n1.0\n\n\n\n\n\n\n\n\n_get_stats_for_author_cls(combined_set, \"unclear\")\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ncommit_stats\n50.0\n0.747\n0.366\n0.004\n0.505\n1.0\n1.0\n1.0\n\n\naddition_stats\n50.0\n0.722\n0.420\n0.000\n0.310\n1.0\n1.0\n1.0\n\n\ndeletion_stats\n45.0\n0.737\n0.417\n0.000\n0.722\n1.0\n1.0\n1.0\n\n\nabs_stats\n50.0\n0.733\n0.404\n0.000\n0.417\n1.0\n1.0\n1.0\n\n\n\n\n\n\n\n\n# My take away here is that we at least have 25% of the 30% making sizable contributions\n# Reasoning being, 75th percentile is 25% making ~10% of total commits and abs additions/deletions\n# Is that _worthy_ of authorship? Who knows, but 10% of total commits/additions/deletions is decent\n# That is still ~7.5% of all papers\n\n# Given that 25th percentile of unclear authors is also pretty high but not 100%,\n# its likely that _some_ of those are also true \"not authors\"\n0.25 * 30\n\n7.5\n\n\n\n# If we ignore repo owners, we can see the stats are a bit more distributed\n# And I would argue that its maybe somewhere between 25% and 50% of unclear, non-repo owner authors\n_get_stats_for_author_cls(combined_set, \"no\", repo_owner_filter=\"only\")\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ncommit_stats\n4.0\n0.804\n0.275\n0.417\n0.704\n0.900\n1.0\n1.0\n\n\naddition_stats\n4.0\n0.956\n0.084\n0.830\n0.953\n0.997\n1.0\n1.0\n\n\ndeletion_stats\n4.0\n0.915\n0.141\n0.706\n0.891\n0.976\n1.0\n1.0\n\n\nabs_stats\n4.0\n0.953\n0.087\n0.822\n0.948\n0.995\n1.0\n1.0\n\n\n\n\n\n\n\n\n# If we ignore repo owners, we can see the stats are a bit more distributed\n# And I would argue that its maybe somewhere between 25% and 50% of unclear, non-repo owner authors\n_get_stats_for_author_cls(combined_set, \"unclear\", repo_owner_filter=\"none\")\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ncommit_stats\n20.0\n0.454\n0.399\n0.004\n0.041\n0.346\n0.841\n1.0\n\n\naddition_stats\n20.0\n0.380\n0.443\n0.000\n0.016\n0.094\n0.917\n1.0\n\n\ndeletion_stats\n20.0\n0.486\n0.492\n0.000\n0.003\n0.431\n0.999\n1.0\n\n\nabs_stats\n20.0\n0.392\n0.440\n0.000\n0.011\n0.127\n0.919\n1.0\n\n\n\n\n\n\n\n\n# If we ignore repo owners, we can see the stats are a bit more distributed\n# And I would argue that its maybe somewhere between 25% and 50% of unclear, non-repo owner authors\n_get_stats_for_author_cls(combined_set, \"unclear\", repo_owner_filter=\"only\")\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ncommit_stats\n30.0\n0.943\n0.152\n0.444\n1.0\n1.0\n1.0\n1.0\n\n\naddition_stats\n30.0\n0.949\n0.187\n0.016\n1.0\n1.0\n1.0\n1.0\n\n\ndeletion_stats\n25.0\n0.938\n0.180\n0.158\n1.0\n1.0\n1.0\n1.0\n\n\nabs_stats\n30.0\n0.961\n0.124\n0.407\n1.0\n1.0\n1.0\n1.0\n\n\n\n\n\n\n\n\n# I think an estimate of 10% of all papers have a code-contributing non-author is reasonable\n# Down from 30% to 10%, to me that is still a lot. Esp considering we are mostly dealing with\n# analysis code. There are likely papers which rely on RSE developed tools which exist in separate repos\n# and are not captured here.\n\n\n_get_stats_for_author_cls(combined_set, [\"unclear\", \"no\"])\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ncommit_stats\n103.0\n0.455\n0.440\n0.001\n0.019\n0.317\n1.0\n1.0\n\n\naddition_stats\n101.0\n0.472\n0.469\n0.000\n0.004\n0.243\n1.0\n1.0\n\n\ndeletion_stats\n96.0\n0.448\n0.472\n0.000\n0.002\n0.149\n1.0\n1.0\n\n\nabs_stats\n101.0\n0.475\n0.467\n0.000\n0.004\n0.338\n1.0\n1.0\n\n\n\n\n\n\n\n\n_get_stats_for_author_cls(combined_set, [\"unclear\", \"no\"], repo_owner_filter=\"none\")\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ncommit_stats\n69.0\n0.222\n0.332\n0.001\n0.008\n0.048\n0.317\n1.0\n\n\naddition_stats\n67.0\n0.229\n0.375\n0.000\n0.001\n0.013\n0.272\n1.0\n\n\ndeletion_stats\n67.0\n0.237\n0.398\n0.000\n0.000\n0.006\n0.174\n1.0\n\n\nabs_stats\n67.0\n0.229\n0.375\n0.000\n0.001\n0.013\n0.259\n1.0\n\n\n\n\n\n\n\n\n_get_stats_for_author_cls(combined_set, [\"unclear\", \"no\"], repo_owner_filter=\"only\")\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ncommit_stats\n34.0\n0.926\n0.171\n0.417\n1.0\n1.0\n1.0\n1.0\n\n\naddition_stats\n34.0\n0.950\n0.177\n0.016\n1.0\n1.0\n1.0\n1.0\n\n\ndeletion_stats\n29.0\n0.935\n0.174\n0.158\n1.0\n1.0\n1.0\n1.0\n\n\nabs_stats\n34.0\n0.960\n0.119\n0.407\n1.0\n1.0\n1.0\n1.0"
  }
]