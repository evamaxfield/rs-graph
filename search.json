[
  {
    "objectID": "qss-code-authors.html",
    "href": "qss-code-authors.html",
    "title": "Code Contribution and Authorship",
    "section": "",
    "text": "Contemporary scientific research fundamentally depends on specialized software tools and computational methods (Edwards et al. 2013; Mayernik et al. 2017; Howison et al. 2015). While these tools - from analysis scripts for data processing to infrastructure for data collection (Hasselbring et al. 2024) - have become essential to modern research practices, their development and maintenance face persistent challenges in receiving appropriate academic recognition (Muna et al. 2016). This tension is particularly acute given software’s expanding roles: enabling reproducible research and large-scale experiments (Krafczyk et al. 2019; Trisovic et al. 2021), serving as a detailed log of research methodology (Ram 2013), and increasingly being shared alongside research articles as a crucial research artifact (Cao et al. 2023; Trujillo, Hébert-Dufresne, and Bagrow 2022).\nThis misalignment between software’s importance and its recognition has significant consequences for academic careers. Software contributions are often relegated to acknowledgments rather than warranting authorship (Philippe et al. 2019), and this lack of formal credit can significantly impact career advancement in academia (Carver et al. 2022; Biagioli and Galison 2014). The academic community continues to grapple with developing appropriate systems for software citation and credit attribution that adequately reflect these contributions (Merow et al. 2023; Westner et al. 2024; Katz et al. 2020).\nRecent initiatives like the Contributor Roles Taxonomy (CRediT) have attempted to address these challenges by expanding academic authorship criteria to include specialized roles (Brand et al. 2015), but have not fully resolved the issues surrounding software contribution recognition. While previous research has used CRediT and similar systems to understand research labor distribution (Larivière, Pontille, and Sugimoto 2020; Larivière et al. 2016; Sauermann and Haeussler 2017; K. Li, Zhang, and Larivière 2023; Lu et al. 2019), these frameworks remain largely centered on traditional author lists. This traditional focus perpetuates existing problems, including historical and systematic bias, and relies heavily on self-reporting without external verification (Haeussler and Sauermann 2013; Gøtzsche et al. 2007; Ni et al. 2021). The limitations of current contribution frameworks, combined with the growing importance of software in research, highlight the need for a systematic examination of how code contributions actually relate to academic recognition and impact.\nThese challenges raise several critical questions about the relationship between software development and academic recognition:\n\nHow does the distribution of coding contributions within research teams relate to article impact?\nHow do traditional academic roles align with actual code contributions?\nHow are code-contribution patterns associated with an individual’s scientific impact metrics?\n\nThe emergence of public code repositories alongside published research creates a unique opportunity to move beyond self-reported contribution data. Version control systems maintain detailed records of who contributes what code and when, providing an unprecedented window into the actual patterns of software development in scientific research. However, leveraging this data requires solving significant technical challenges in connecting repository information with traditional academic publishing records.\nTo address these questions, we developed a novel predictive model that enables systematic matching between scientific article authors and source code developer accounts. This approach was necessary due to the lack of standardized identifiers (like ORCID) for developers (Haak et al. 2012) and inconsistencies in naming and email conventions across platforms. We leverage recent advances in transformer-based architectures and semantic embeddings to handle subtle variations in identity information (Y. Li et al. 2020; Brunner and Stockinger 2020).\nBy applying our model across a corpus of 138596 paired research articles and repositories, we uncover several striking patterns in the relationship between code contributions and academic recognition. Our analysis of source-code repositories contribution histories reveals that approximately 25% of articles have non-author code-contributors. We find that code-contributing authors are associated with modest increases in article-level impact metrics (~4.5% increase in citations per code-contributing author), though these effects become statistically non-significant when controlling for domain, article type, and open access status. First authors emerge as significantly more likely to be code contributors compared to other positions across all conditions tested. And most notably, we find a clear negative relationship between coding frequency and scholarly impact - authors who contribute code more frequently show progressively lower h-indices compared to their non-coding peers, a pattern that persists across various controls for an author’s number of total works, author position, domain, and article type.\nThese findings reveal fundamental tensions between software development and traditional academic recognition systems. While our results demonstrate the prevalence of software contributions in modern research (both credited and un-credited), they also suggest that current academic metrics may systematically undervalue technical contributions. This quantitative evidence provides an empirical foundation for ongoing discussions about academic credit systems and raises important questions about how institutions can better align recognition with the full spectrum of contributions that drive scientific progress."
  },
  {
    "objectID": "qss-code-authors.html#linking-scientific-articles-and-source-code-repositories",
    "href": "qss-code-authors.html#linking-scientific-articles-and-source-code-repositories",
    "title": "Code Contribution and Authorship",
    "section": "3.1 Linking Scientific Articles and Source Code Repositories",
    "text": "3.1 Linking Scientific Articles and Source Code Repositories\nModern scientific research increasingly requires the public sharing of research code, creating unique opportunities to study the relationship between academic authorship and software development. Many journals and platforms now require or recommend code and data sharing (Stodden, Guo, and Ma 2013; Sharma et al. 2024), creating traceable links between publications and code that enable systematic study of both article-repository and author-developer relationships (Hata et al. 2021; Kelley and Garijo 2021; Stankovski and Garijo 2024; Milewicz, Pinto, and Rodeghero 2019).\nOur data collection process leverages multiple sources of linked scientific articles and code repositories to ensure broad coverage of multiple different domains and article types. Our dataset combines article-source-code-repository pairs from:\n\nPLOS: Traditional research articles\nJOSS and SoftwareX: Specialized software-focused publications\nPapers with Code (ArXiv): preprints\n\nTo reduce the complexity of dataset processing and enrichment, we filter out any article-source-code-repository pairs which store code somewhere other than GitHub. While this decision prioritizes processing simplicity, we acknowledge that while GitHub is the predominate host of scientific software, it is also stored and shared on other platforms, which should be investigated as a part of future research (Trujillo, Hébert-Dufresne, and Bagrow 2022).\nThrough integration of multiple data sources, we extract detailed information about both the academic and software development aspects of each project:\nThrough integration of multiple data sources, we extract detailed information about both the academic and software development aspects of each project. We utilize the Semantic Scholar API for DOI resolution to ensure that we find the latest version of each article, which is particularly important for working with preprints as they may have been published in a journal since their inclusion in the Papers with Code dataset. We then utilize the OpenAlex API to gather publication metadata (open access status, domain, publication date), author details (name, author position, corresponding author status), and article- and individual-level metrics (citation counts, FWCI, h-index). The GitHub API provides similar information for source code repositories, including repository metadata (name, description, languages, creation date), contributor details (username, name, email), and repository-level metrics (star count, fork count, issue count).\nTaken together, we form one of the largest and most comprehensively annotated collections of paired scientific articles and associated source-code repositories. In total, we collect and enrich data for 163292 article-repository pairs."
  },
  {
    "objectID": "qss-code-authors.html#a-predictive-model-for-matching-article-authors-and-source-code-contributors",
    "href": "qss-code-authors.html#a-predictive-model-for-matching-article-authors-and-source-code-contributors",
    "title": "Code Contribution and Authorship",
    "section": "3.2 A Predictive Model for Matching Article Authors and Source Code Contributors",
    "text": "3.2 A Predictive Model for Matching Article Authors and Source Code Contributors\n\n3.2.1 Annotated Dataset Creation\nThe development of an accurate author-developer matching model requires high-quality labeled training data that captures the complexity of real-world identity matching. Entity matching between authors and developers is non-trivial due to multiple forms of name variation and incomplete information. These variations can include differences in formatting (e.g., “J. Doe” vs “Jane Doe”), institutional versus personal email addresses, and incomplete or outdated information.\nWe developed an annotation process to create a robust training dataset while maximizing efficiency and accuracy. We focused our annotation efforts on JOSS articles to increase positive match density, as these software-focused publications typically have higher overlap between authors and developers. For each JOSS author, we generated three random pairings with developer accounts from the article’s associated repository. From the full set of generated pairs, we randomly sampled 3,000 for annotation which two independent annotators then labeled as either a match or non-match. After completing all annotations, we systematically resolved any disagreements between the annotators through discussion and additional verification.\nThe resulting annotated dataset provides a comprehensive foundation for training our predictive model while highlighting common patterns in author-developer identity matching. After resolving all annotated pairs, our final dataset contains:\n\nMatch Distribution:\n\n451 (15.0%) positive matches\n2548 (85.0%) negative matches\n\nUnique Individuals:\n\n2027 unique authors\n2733 unique developer accounts\n\nDeveloper Profile Completeness:\n\n2191 (80.2%) accounts have associated names\n839 (30.7%) accounts have associated emails\n\n\n\n\n3.2.2 Training and Evaluation\nOur training and evaluation methodology begins with careful dataset preparation to prevent data leakage between training and test sets. To ensure complete separation of both authors and developers, we randomly selected 10% of unique authors and 10% of unique developers, designating any pairs containing these selected entities for the test set. Due to the combinatorial nature of our author-developer pairs, this entity-based splitting strategy resulted in 2442 (81.4%) pairs for training and 557 (18.6%) pairs for testing.\nFor our predictive model, we evaluate three transformer-based architectures that have demonstrated strong performance in entity matching tasks: DeBERTa-v3-base (He, Gao, and Chen 2021; He et al. 2021), mBERT (bert-base-multilingual-cased) (Devlin et al. 2018), and DistilBERT (Sanh et al. 2019). While BERT-based architectures have been widely studied, they continue to achieve state-of-the-art results across various natural language processing tasks, particularly in scenarios requiring precise entity matching and relationship identification (Tran et al. 2024; Yu et al. 2024; Jeong and Kim 2022).\nWe conducted systematic evaluation of these base models across different combinations of developer-account features, ranging from using only the username to incorporating full profile information (username, display name, and email address). All models were fine-tuned using the Adam optimizer with a learning rate of 1e-05, batch sizes of 8 for both training and evaluation, and a linear learning rate scheduler. Given the size of our dataset and the binary nature of our classification task, models were trained for a single epoch to prevent overfitting.\nModel performance was assessed using standard binary classification metrics, with particular emphasis on the F1 score for positive (matching) pairs due to the inherent class imbalance in author-developer matching. This evaluation framework allows us to directly compare model architectures and feature combinations while accounting for both precision and recall in identifying correct matches.\nOur comprehensive model evaluation revealed that fine-tuning DeBERTa-v3-base (He, Gao, and Chen 2021) with developer username and display name as input features produces optimal performance for author-developer matching. This model configuration achieves a binary F1 score of 0.944, with accuracy of 0.984, precision of 0.938, and recall of 0.95. A complete comparison of model architectures and feature combinations can be found in Table 4.\nAnalysis of each model’s performance revealed that including developer display names has the largest positive impact on model performance compared to username alone. While this effect might be partially attributed to the higher availability of display names in our dataset compared to email addresses, the performance improvement is notable. We additionally observe that mBERT’s performance was comparable with DeBERTa’s while additionally using the developer email address as an additional input feature, but selected the DeBERTa configuration for its relative simplicity and more recent and comprehensive pretraining. DeBERTa’s consistent strong performance across various feature combinations, combined with its more extensive pretraining dataset, suggests better generalization potential for future applications.\nTo facilitate the reuse of our work, we have made our trained model and supporting code publicly available. Complete fine-tuning, evaluation, and inference code is available as the Python package: sci-soft-models, and the fine-tuned model has been deployed to HuggingFace (evamxb/dev-author-em-clf)."
  },
  {
    "objectID": "qss-code-authors.html#linking-authors-and-github-developer-accounts",
    "href": "qss-code-authors.html#linking-authors-and-github-developer-accounts",
    "title": "Code Contribution and Authorship",
    "section": "3.3 Linking Authors and GitHub Developer Accounts",
    "text": "3.3 Linking Authors and GitHub Developer Accounts\nOur trained entity-matching model enables comprehensive identification of author-developer relationships across all possible author and developer-account combinations within each article-repository pair. This broad application accounts for the complex realities of academic software development practices, particularly the common occurrence of researchers maintaining multiple developer accounts across different projects or institutions, and account transitions as researchers move between roles.\nWhile our model demonstrates strong performance, we acknowledge certain limitations in our approach. Notably, the model’s performance can be affected by shorter names (both usernames and display names) where less textual information is available for matching. Additionally, while organization accounts (such as research lab accounts used for project management) present a potential challenge for accurate matching, our filtering mechanisms applied before analysis help minimize their impact in modeling.\nThe resulting dataset represents, to our knowledge, the first large-scale collection of linked article-repository and author-developer-account pairs, particularly in the physical sciences. Specifically, our dataset contains 138596 article-repository pairs, with 295806 distinct authors and 152170 distinct developer accounts. From these distinct entities, we identify 108754 annotated author-developer pairs. A detailed breakdown of these counts by data source, domain, document type, and open access status is available in Table 1.\n\n\n\n\nTable 1: Counts of Article-Repository Pairs, Authors, and Developers by Data Sources, Domains, Document Types, and Access Status.\n\n\n\n\n\n\nCategory\nSubset\nArticle-Repository Pairs\nAuthors\nDevelopers\n\n\nBy Domain\nPhysical Sciences\n116600\n240545\n130592\n\n\nSocial Sciences\n8838\n29269\n14043\n\n\nLife Sciences\n7729\n31649\n12150\n\n\nHealth Sciences\n5172\n25979\n7248\n\n\nBy Document Type\npreprint\n72177\n170301\n87311\n\n\nresearch article\n63528\n173183\n78935\n\n\nsoftware article\n2891\n9294\n12868\n\n\nBy Access Status\nOpen\n132856\n286874\n147831\n\n\nClosed\n5740\n23668\n9352\n\n\nBy Data Source\npwc\n129615\n262889\n134926\n\n\nplos\n6090\n30233\n8784\n\n\njoss\n2336\n7105\n11362\n\n\nsoftwarex\n555\n2244\n1628\n\n\nTotal\n\n138596\n295806\n152170"
  },
  {
    "objectID": "qss-code-authors.html#software-development-dynamics-within-research-teams",
    "href": "qss-code-authors.html#software-development-dynamics-within-research-teams",
    "title": "Code Contribution and Authorship",
    "section": "4.1 Software Development Dynamics Within Research Teams",
    "text": "4.1 Software Development Dynamics Within Research Teams\nUnderstanding the composition and dynamics of software development teams provides essential context for analyzing how code contributions relate to academic recognition and impact. To ensure reliable analysis, we focus on a subset of our data that includes only article-repository pairs with at least one article citation, repository commit activity that at the latest falls within 90 days of publication, and research teams of typical size (removing those with fewer than 3 authors or above the 97th percentile of team sizes).\nWithin this filtered dataset, we categorized individuals into three groups: code-contributing authors (CC-A) who both authored papers and contributed code to associated repositories, non-code-contributing authors (NCC-A) who authored papers but showed no evidence of code contributions, and code-contributing non-authors (CC-NA) who contributed code but received no authorship recognition. This categorization revealed that papers in our dataset typically have 4.9 ± 1.9 total authors, with 1.0 ± 0.7 code-contributing authors and 3.9 ± 2.0 non-code-contributing authors. Beyond the author list, papers averaged 0.4 ± 1.7 code-contributing non-authors. Table 2 provides a detailed breakdown of these distributions by domain, article type, and open access status.\nPerhaps most striking is our finding that 6237 papers (27.4%) have at least one code contributor who did not receive authorship recognition. Within this substantial subset of papers, we found an average of 1.6 ± 3.0 unrecognized code contributors per paper. The presence of only one code-contributing author per paper, on average, aligns with previous research by Larivière, Pontille, and Sugimoto (2020) showing that technical tasks like data curation, formal analysis, visualization, and software development typically fall to first authors. However, our finding that over a quarter of papers have unrecognized code contributors suggests a more complex dynamic between software development and authorship recognition.\n\n\n\n\nTable 2: Mean and Standard Deviation of Non-Code-Contributing Authors (NCC-A), Code-Contributing Authors (CC-A), and Code-Contributing Non-Authors (CC-NA) Research Team Members by Domain, Article Type, and Open Access Status. Only includes research teams from article-repository pairs with a most recent commit no later than 90 days after publication and excludes research teams which are in the top 3% of total author sizes.\n\n\n\n\n\n\nControl\nSubset\nTotal Authors\nNCC-A\nCC-A\nCC-NA\n\n\nOA Status\nClosed\n5.1 ± 1.9\n4.0 ± 1.9\n1.1 ± 0.7\n0.5 ± 2.1\n\n\nOpen\n4.9 ± 1.9\n3.9 ± 2.0\n1.0 ± 0.7\n0.4 ± 1.7\n\n\nDomain\nHealth Sciences\n6.1 ± 2.5\n5.1 ± 2.5\n1.0 ± 0.6\n0.4 ± 1.2\n\n\nLife Sciences\n5.2 ± 2.1\n4.2 ± 2.2\n1.0 ± 0.7\n0.4 ± 1.2\n\n\nPhysical Sciences\n4.8 ± 1.8\n3.8 ± 1.9\n1.0 ± 0.7\n0.5 ± 1.8\n\n\nSocial Sciences\n4.5 ± 1.7\n3.5 ± 1.8\n1.1 ± 0.7\n0.3 ± 1.1\n\n\nArticle Type\npreprint\n4.8 ± 1.8\n3.8 ± 1.9\n1.1 ± 0.7\n0.5 ± 2.2\n\n\nresearch article\n4.9 ± 1.9\n3.9 ± 2.0\n1.0 ± 0.7\n0.4 ± 1.6\n\n\nsoftware article\n4.7 ± 1.9\n3.2 ± 1.9\n1.5 ± 1.4\n0.9 ± 1.1\n\n\nOverall\n\n4.9 ± 1.9\n3.9 ± 2.0\n1.0 ± 0.7\n0.4 ± 1.7\n\n\n\n\n\n\n\n\nWhen examining these patterns over time and across different team sizes (Figure 1), we found that both the number of code-contributing authors and unrecognized contributors has remained relatively stable. This stability suggests that while the exclusion of code contributors from authorship isn’t worsening, it represents a persistent feature of academic software development rather than a historical artifact or transition period in academic practices.\n\n\n\n\n\n\n\n\nFigure 1: Mean number of Non-Code-Contributing Authors, Code-Contributing Authors, and Code-Contributing Non-Authors by Publication Year and by Total Number of Authors. Only includes article-repository pairs with a most recent commit no later than 90 days after publication and excludes research-teams which are in the top 3% of total author sizes for publication years with 50 or more articles.\n\n\n\n\n\n\n4.1.1 Modeling Article Citations\nBuilding upon previous work examining the effects of team size and team diversity on scientific impact and software quality (see Section 2), we investigate how the number of code contributors within a research team may be associated with an article’s research impact. We hypothesize that more code contributors may signal greater technical complexity in research, which may be associated with higher citation counts as the community builds upon more technically sophisticated works.\nUsing our filtered dataset of article-repository pairs, we conducted multiple regression analyses to examine these relationships while controlling for various factors. Without controlling for domain, open access, or article type differences (Table 5), we found a modest positive association between the number of code-contributing authors and article citations, with each code-contributing author being associated with a 4.5% increase in article citations (p &lt; 0.001).\nWhen controlling for article type (Table 8), we observed divergent patterns between preprints and research articles. For preprints, each code-contributing non-author was associated with a statistically significant 2.9% decrease in citations (p &lt; 0.01). In contrast, research articles showed more positive associations: we found a significant positive relationship between code-contributing authors and citations (p &lt; 0.001), though we cannot estimate the precise magnitude due to the non-significant main effect in the model. Additionally, each code-contributing non-author was associated with a 0.8% increase in expected citations for research articles (p &lt; 0.001).\nOverall, while we find statistically significant associations between code contributions and citation counts, these effects are relatively modest in magnitude."
  },
  {
    "objectID": "qss-code-authors.html#characteristics-of-scientific-code-contributors",
    "href": "qss-code-authors.html#characteristics-of-scientific-code-contributors",
    "title": "Code Contribution and Authorship",
    "section": "4.2 Characteristics of Scientific Code Contributors",
    "text": "4.2 Characteristics of Scientific Code Contributors\n\n4.2.1 Author Positions of Code Contributing Authors\nBuilding upon previous work examining the relationship between authorship position and research contributions, we investigate how author position may relate to code contribution patterns. We hypothesize that first authors, who traditionally contribute the bulk of intellectual and experimental work, would be most likely to contribute code to a project, while middle and last authors, who often provide oversight and guidance, would be less likely to contribute code.\nTo analyze these patterns within our filtered dataset of article-repository pairs (excluding those with most recent commit dates more than 90 days after publication), we first conducted chi-square tests of independence between author position and code contribution status. These tests revealed significant associations both overall and when controlling for research domain, article type, and open access status (all p &lt; 0.01). Following these significant results, detailed post-hoc binomial tests (Table 9) revealed clear position-based differences: 68.6% of first authors contributed code to their projects, compared to only 9.0% of middle authors and 9.1% of last authors. These differences remained statistically significant across all tested scenarios, regardless of research domain, article type, or open access status.\nThese patterns strongly align with traditional academic authorship conventions, where first authors typically take primary responsibility for both intellectual and technical aspects of the research, while middle and last authors more commonly provide oversight and guidance. The consistency of these findings across different subsets of our data suggests a deeply embedded relationship between author position and technical contributions in academic software development.\n\n\n4.2.2 Corresponding Status of Code Contributing Authors\nBuilding upon our analysis of author position, we next examine how corresponding author status relates to code contribution patterns. We hypothesize that corresponding authors, who traditionally maintain research artifacts and serve as primary points of contact, would be more likely to contribute code compared to non-corresponding authors, as this role often involves responsibility for project resources and materials.\nTo analyze these relationships within our filtered dataset of article-repository pairs, we conducted chi-square tests of independence between corresponding author status and code contribution status. Surprisingly, these tests revealed patterns contrary to our initial hypothesis. Both corresponding and non-corresponding authors were significantly less likely to be code contributors than would be expected by chance (p &lt; 0.01), with only 30.2% of corresponding authors and 20.5% of non-corresponding authors contributing code. Detailed post-hoc binomial tests (Table 10) revealed this pattern holds true across nearly all conditions, with only two notable exceptions: corresponding authors in software-focused articles and those in closed-access publications showed no significant difference in their likelihood to contribute code.\nThese findings challenge conventional assumptions about the relationship between corresponding authorship and technical contributions. While corresponding authors are traditionally responsible for maintaining research artifacts, our results suggest this responsibility may not typically extend to direct engagement with software development.\n\n\n4.2.3 Modeling Author H-Index\n\n\n\n\nTable 3: Counts of Researcher Coding Status Used in H5\n\n\n\n\n\n\nControl\nSubset\nAny Coding\nMajority Coding\nAlways Coding\nTotal\n\n\nFreq. Author Pos.\nFirst\n1689\n4952\n3322\n11444\n\n\nMiddle\n12184\n3327\n701\n31911\n\n\nLast\n2413\n568\n191\n10188\n\n\nFreq. Domain\nHealth Sciences\n345\n202\n86\n1501\n\n\nLife Sciences\n369\n241\n129\n1436\n\n\nPhysical Sciences\n15289\n8179\n3821\n49311\n\n\nSocial Sciences\n283\n225\n178\n1295\n\n\nFreq. Article Type\nPreprint\n9572\n4963\n2214\n28948\n\n\nResearch Article\n6669\n3765\n1871\n24217\n\n\nSoftware Article\n45\n119\n129\n378\n\n\n\n\n\n\n\n\nBuilding upon previous work examining career implications for researchers who prioritize software development Section 2, we investigated how varying levels of code contribution relate to scholarly impact through h-index metrics. To ensure a robust analysis, we applied several key data filtering steps. We only included researchers with at least three publications in our dataset, removed those with more than three developer account associations, and used each researcher’s most common (or most recent) domain, article type, and author position, with ties broken by the most recent occurrence. We also removed h-index outliers by excluding researchers below the bottom 3rd and above the top 97th percentiles.\nWe categorized researchers’ coding contributions into mutually exclusive groups: non-coders (no code contributions), any coding (code contribution in less than half of article-repository pairs), majority coding (code contribution in at least half, but not all, article-repository pairs), and always coding (code contribution in every article-repository pair).\nOur analysis revealed a consistent and statistically significant negative relationship between code contribution frequency and h-index across multiple analytical controls. In our initial uncontrolled analysis (Table 11), we observed increasingly negative h-index effects as coding frequency increased. Compared to non-coding authors, researchers were associated with progressively lower h-indices: occasional code contributors showed a ~27.3% lower h-index (p &lt; 0.001), majority code contributors demonstrated a ~53.5% lower h-index (p &lt; 0.001), and always coding authors exhibited a ~62.1% lower h-index (p &lt; 0.001).\nWhen controlling for author position (Table 12), we found a general pattern of reduced h-indices with increased code contribution, with one notable exception. Occasional coding first authors were associated with a ~14.9% higher h-index (p &lt; 0.001), while always coding first authors saw a ~21.6% reduction compared to non-coding first authors (p &lt; 0.001). For middle and last authors, the pattern was more consistently negative. Middle authors who occasionally coded showed a ~26.6% lower h-index (p &lt; 0.001), and those always coding demonstrated a ~52.9% lower h-index (p &lt; 0.001). Similarly, last authors who occasionally coded experienced a ~13.1% lower h-index (p &lt; 0.001), with always coding authors showing a ~45.7% lower h-index (p &lt; 0.001).\nWhen controlling for research domain (Table 13), majority coding scientists showed significant h-index reductions across all domains. Health sciences researchers saw the most dramatic reduction at ~76.5% (p &lt; 0.001), followed by physical sciences at ~52.3% (p &lt; 0.001), social sciences at ~51.4% (p &lt; 0.001), and life sciences at ~47.1% (p &lt; 0.001).\nAnalyzing by common article type (Table 14) revealed similar patterns. For authors primarily publishing preprints, the h-index reductions were substantial: ~25.6% for occasional coding, ~53.5% for majority coding, and ~62.9% for always coding authors. Authors primarily publishing software articles showed slightly different but still significant reductions: ~33.1% for majority coding and ~33.0% for always coding authors.\nTaken as a whole, these findings indicate that the more frequently an author contributes code, the lower their h-index is likely to be relative to their peers, with the notable exception of first authors who occasionally contribute code. What makes these results particularly striking is that each of our models includes publication count as an input feature, suggesting that these h-index reductions persist even when accounting for total research output."
  },
  {
    "objectID": "qss-code-authors.html#extended-modeling-and-analysis-results-and-supporting-tables",
    "href": "qss-code-authors.html#extended-modeling-and-analysis-results-and-supporting-tables",
    "title": "Code Contribution and Authorship",
    "section": "7.1 Extended Modeling and Analysis Results and Supporting Tables",
    "text": "7.1 Extended Modeling and Analysis Results and Supporting Tables\n\n7.1.1 Full Comparison of Models and Optional Features for Author-Developer-Account Matching\n\n\n\n\nTable 4: Comparison of Models for Author-Developer-Account Matching\n\n\n\n\n\n\n\n\n\n\nOptional Feats.\nModel\nAccuracy\nPrecision\nRecall\nF1\n\n\n\n\n0\nname\ndeberta\n0.984\n0.938\n0.950\n0.944\n\n\n1\nname, email\nbert-multilingual\n0.984\n0.938\n0.950\n0.944\n\n\n2\nname, email\ndeberta\n0.982\n0.907\n0.975\n0.940\n\n\n3\nname\nbert-multilingual\n0.982\n0.938\n0.938\n0.938\n\n\n4\nname\ndistilbert\n0.978\n0.936\n0.912\n0.924\n\n\n5\nname, email\ndistilbert\n0.978\n0.936\n0.912\n0.924\n\n\n6\nemail\ndeberta\n0.957\n0.859\n0.838\n0.848\n\n\n7\nemail\nbert-multilingual\n0.950\n0.894\n0.738\n0.808\n\n\n8\nn/a\ndeberta\n0.946\n0.847\n0.762\n0.803\n\n\n9\nn/a\nbert-multilingual\n0.941\n0.862\n0.700\n0.772\n\n\n10\nn/a\ndistilbert\n0.856\n0.000\n0.000\n0.000\n\n\n11\nemail\ndistilbert\n0.856\n0.000\n0.000\n0.000\n\n\n\n\n\n\n\n\n\n\n\n\n7.1.2 Linear Models for Software Development Dynamics Within Research Teams\n\n\n\n\nTable 5: Article Citations by Code Contributorship of Research Team\n\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ncited_by_count\nNo. Observations:\n22127\n\n\nModel:\nGLM\nDf Residuals:\n22122\n\n\nModel Family:\nNegativeBinomial\nDf Model:\n4\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-76535.\n\n\nDate:\nSat, 01 Feb 2025\nDeviance:\n22306.\n\n\nTime:\n01:40:16\nPearson chi2:\n3.39e+04\n\n\nNo. Iterations:\n13\nPseudo R-squ. (CS):\n0.2875\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n0.9900\n0.026\n38.607\n0.000\n0.940\n1.040\n\n\nTotal Authors\n0.0691\n0.004\n18.481\n0.000\n0.062\n0.076\n\n\nCode-Contrib. Authors\n0.0437\n0.010\n4.203\n0.000\n0.023\n0.064\n\n\nCode-Contrib. Non-Authors\n-0.0011\n0.004\n-0.253\n0.800\n-0.009\n0.007\n\n\nYears Since Publication\n0.3904\n0.004\n96.885\n0.000\n0.383\n0.398\n\n\n\n\n\n\n\n\n\n\n\n\nTable 6: Article Citations by Code Contributorship of Research Team Controlled by Open Access Status\n\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ncited_by_count\nNo. Observations:\n22127\n\n\nModel:\nGLM\nDf Residuals:\n22119\n\n\nModel Family:\nNegativeBinomial\nDf Model:\n7\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-76467.\n\n\nDate:\nSat, 01 Feb 2025\nDeviance:\n22169.\n\n\nTime:\n01:40:16\nPearson chi2:\n3.37e+04\n\n\nNo. Iterations:\n13\nPseudo R-squ. (CS):\n0.2920\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n0.6109\n0.062\n9.846\n0.000\n0.489\n0.732\n\n\nTotal Authors\n0.0696\n0.004\n18.600\n0.000\n0.062\n0.077\n\n\nCode-Contrib. Authors\n0.0540\n0.046\n1.171\n0.242\n-0.036\n0.144\n\n\nCode-Contrib. Non-Authors\n0.0007\n0.015\n0.049\n0.961\n-0.029\n0.031\n\n\nYears Since Publication\n0.3810\n0.004\n93.042\n0.000\n0.373\n0.389\n\n\nIs Open Access\n0.4194\n0.061\n6.904\n0.000\n0.300\n0.539\n\n\nCode-Contrib. Authors * Is Open Access\n-0.0104\n0.047\n-0.221\n0.825\n-0.103\n0.082\n\n\nCode-Contrib. Non-Authors * Is Open Access\n-0.0012\n0.016\n-0.075\n0.941\n-0.032\n0.030\n\n\n\n\n\n\n\n\n\n\n\n\nTable 7: Article Citations by Code Contributorship of Research Team Controlled by Domain\n\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ncited_by_count\nNo. Observations:\n22127\n\n\nModel:\nGLM\nDf Residuals:\n22113\n\n\nModel Family:\nNegativeBinomial\nDf Model:\n13\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-76442.\n\n\nDate:\nSat, 01 Feb 2025\nDeviance:\n22120.\n\n\nTime:\n01:40:16\nPearson chi2:\n3.35e+04\n\n\nNo. Iterations:\n13\nPseudo R-squ. (CS):\n0.2935\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n0.8816\n0.068\n12.926\n0.000\n0.748\n1.015\n\n\nTotal Authors\n0.0709\n0.004\n18.631\n0.000\n0.063\n0.078\n\n\nCode-Contrib. Authors\n0.0278\n0.053\n0.522\n0.601\n-0.077\n0.132\n\n\nCode-Contrib. Non-Authors\n0.0133\n0.026\n0.507\n0.612\n-0.038\n0.065\n\n\nYears Since Publication\n0.3970\n0.004\n97.365\n0.000\n0.389\n0.405\n\n\nDomain Life Sciences\n-0.2104\n0.077\n-2.729\n0.006\n-0.362\n-0.059\n\n\nDomain Physical Sciences\n0.1370\n0.064\n2.131\n0.033\n0.011\n0.263\n\n\nDomain Social Sciences\n-0.1800\n0.082\n-2.203\n0.028\n-0.340\n-0.020\n\n\nCode-Contrib. Authors * Domain Life Sciences\n0.0678\n0.064\n1.053\n0.292\n-0.058\n0.194\n\n\nCode-Contrib. Authors * Domain Physical Sciences\n0.0046\n0.055\n0.085\n0.932\n-0.102\n0.111\n\n\nCode-Contrib. Authors * Domain Social Sciences\n0.0981\n0.066\n1.477\n0.140\n-0.032\n0.228\n\n\nCode-Contrib. Non-Authors * Domain Life Sciences\n-0.0414\n0.035\n-1.195\n0.232\n-0.109\n0.026\n\n\nCode-Contrib. Non-Authors * Domain Physical Sciences\n-0.0151\n0.027\n-0.568\n0.570\n-0.067\n0.037\n\n\nCode-Contrib. Non-Authors * Domain Social Sciences\n-0.0366\n0.036\n-1.015\n0.310\n-0.107\n0.034\n\n\n\n\n\n\n\n\n\n\n\n\nTable 8: Article Citations by Code Contributorship of Research Team Controlled by Article Type\n\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ncited_by_count\nNo. Observations:\n22127\n\n\nModel:\nGLM\nDf Residuals:\n22116\n\n\nModel Family:\nNegativeBinomial\nDf Model:\n10\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-76090.\n\n\nDate:\nSat, 01 Feb 2025\nDeviance:\n21416.\n\n\nTime:\n01:40:16\nPearson chi2:\n3.26e+04\n\n\nNo. Iterations:\n13\nPseudo R-squ. (CS):\n0.3156\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n0.5266\n0.042\n12.589\n0.000\n0.445\n0.609\n\n\nTotal Authors\n0.0688\n0.004\n18.353\n0.000\n0.061\n0.076\n\n\nCode-Contrib. Authors\n-0.0348\n0.027\n-1.283\n0.200\n-0.088\n0.018\n\n\nCode-Contrib. Non-Authors\n-0.0296\n0.010\n-3.103\n0.002\n-0.048\n-0.011\n\n\nYears Since Publication\n0.4023\n0.004\n99.431\n0.000\n0.394\n0.410\n\n\nArticle Type Research Article\n0.4730\n0.038\n12.345\n0.000\n0.398\n0.548\n\n\nArticle Type Software Article\n-0.4740\n0.131\n-3.618\n0.000\n-0.731\n-0.217\n\n\nCode-Contrib. Authors * Article Type Research Article\n0.1037\n0.029\n3.519\n0.000\n0.046\n0.162\n\n\nCode-Contrib. Authors * Article Type Software Article\n-0.0594\n0.066\n-0.904\n0.366\n-0.188\n0.069\n\n\nCode-Contrib. Non-Authors * Article Type Research Article\n0.0375\n0.011\n3.523\n0.000\n0.017\n0.058\n\n\nCode-Contrib. Non-Authors * Article Type Software Article\n0.0892\n0.076\n1.176\n0.240\n-0.060\n0.238\n\n\n\n\n\n\n\n\n\n\n7.1.3 Post-Hoc Tests for Coding vs Non-Coding Authors by Position\n\n\n\n\nTable 9: Counts of Code-Contributing Authors (‘Coding’) as well as Total Authors by Position and Bonferroni Corrected p-values from Post-Hoc Binomial Tests\n\n\n\n\n\n\nControl\nSubset\nPosition\nCoding\nTotal\np\n\n\nDomain\nHealth Sciences\nFirst\n1575\n2401\n0.000***\n\n\nMiddle\n540\n11491\n0.000***\n\n\nLast\n234\n2349\n0.000***\n\n\nLife Sciences\nFirst\n2586\n3895\n0.000***\n\n\nMiddle\n852\n11875\n0.000***\n\n\nLast\n491\n3784\n0.000***\n\n\nPhysical Sciences\nFirst\n28919\n41987\n0.000***\n\n\nMiddle\n10507\n111738\n0.000***\n\n\nLast\n3433\n40410\n0.000***\n\n\nSocial Sciences\nFirst\n2813\n4038\n0.000***\n\n\nMiddle\n1021\n9249\n0.000***\n\n\nLast\n411\n3855\n0.000***\n\n\nArticle Type\nPreprint\nFirst\n13421\n19523\n0.000***\n\n\nMiddle\n5134\n52925\n0.000***\n\n\nLast\n1493\n18598\n0.000***\n\n\nResearch Article\nFirst\n21940\n32081\n0.000***\n\n\nMiddle\n7345\n89991\n0.000***\n\n\nLast\n2909\n31188\n0.000***\n\n\nSoftware Article\nFirst\n532\n717\n0.000***\n\n\nMiddle\n441\n1437\n0.000***\n\n\nLast\n167\n612\n0.000***\n\n\nOpen Access Status\nClosed Access\nFirst\n2637\n3742\n0.000***\n\n\nMiddle\n918\n10965\n0.000***\n\n\nLast\n279\n3642\n0.000***\n\n\nOpen Access\nFirst\n33256\n48579\n0.000***\n\n\nMiddle\n12002\n133388\n0.000***\n\n\nLast\n4290\n46756\n0.000***\n\n\nOverall\nOverall\nFirst\n35893\n52321\n0.000***\n\n\nMiddle\n12920\n144353\n0.000***\n\n\nLast\n4569\n50398\n0.000***\n\n\n\n\n\n\n\n\n\n\n7.1.4 Post-Hoc Tests for Coding vs Non-Coding Authors by Corresponding Status\n\n\n\n\nTable 10: Counts of Code-Contributing Authors (‘Coding’) as well as Total Authors by Corresponding Status and Bonferroni Corrected p-values from Post-Hoc Binomial Tests\n\n\n\n\n\n\nControl\nSubset\nIs Corresponding\nCoding\nTotal\np\n\n\nDomain\nLife Sciences\nCorresponding\n1772\n8019\n0.000***\n\n\nNot Corresponding\n2157\n11535\n0.000***\n\n\nPhysical Sciences\nCorresponding\n5248\n12487\n0.000***\n\n\nNot Corresponding\n37611\n181648\n0.000***\n\n\nSocial Sciences\nCorresponding\n803\n2458\n0.000***\n\n\nNot Corresponding\n3442\n14684\n0.000***\n\n\nArticle Type\nPreprint\nCorresponding\n772\n1036\n0.000***\n\n\nNot Corresponding\n19276\n90010\n0.000***\n\n\nResearch Article\nCorresponding\n7716\n27339\n0.000***\n\n\nNot Corresponding\n24478\n125921\n0.000***\n\n\nSoftware Article\nCorresponding\n213\n438\n1.000\n\n\nNot Corresponding\n927\n2328\n0.000***\n\n\nOpen Access Status\nClosed Access\nCorresponding\n253\n468\n0.174\n\n\nNot Corresponding\n3581\n17881\n0.000***\n\n\nOpen Access\nCorresponding\n8448\n28345\n0.000***\n\n\nNot Corresponding\n41100\n200378\n0.000***\n\n\nOverall\nOverall\nCorresponding\n8701\n28813\n0.000***\n\n\nNot Corresponding\n44681\n218259\n0.000***\n\n\n\n\n\n\n\n\n\n\n7.1.5 Linear Models for Characterizing Code-Contributing Author H-Index\n\n\n\n\nTable 11: Code-Contributing Authors H-Index by Coding Status\n\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nh_index\nNo. Observations:\n49355\n\n\nModel:\nGLM\nDf Residuals:\n49350\n\n\nModel Family:\nGaussian\nDf Model:\n4\n\n\nLink Function:\nLog\nScale:\n197.81\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-2.0051e+05\n\n\nDate:\nSat, 01 Feb 2025\nDeviance:\n9.7618e+06\n\n\nTime:\n01:40:19\nPearson chi2:\n9.76e+06\n\n\nNo. Iterations:\n43\nPseudo R-squ. (CS):\n0.1799\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n3.1910\n0.004\n832.251\n0.000\n3.184\n3.199\n\n\nWorks Count\n0.0001\n1.98e-06\n66.574\n0.000\n0.000\n0.000\n\n\nAny Coding\n-0.3188\n0.007\n-43.308\n0.000\n-0.333\n-0.304\n\n\nMajority Coding\n-0.7653\n0.014\n-55.218\n0.000\n-0.792\n-0.738\n\n\nAlways Coding\n-0.9701\n0.025\n-39.065\n0.000\n-1.019\n-0.921\n\n\n\n\n\n\n\n\n\n\n\n\nTable 12: Researcher H-Index by Coding Status Controlled by Most Freq. Author Position\n\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nh_index\nNo. Observations:\n49355\n\n\nModel:\nGLM\nDf Residuals:\n49342\n\n\nModel Family:\nGaussian\nDf Model:\n12\n\n\nLink Function:\nLog\nScale:\n180.01\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-1.9818e+05\n\n\nDate:\nSat, 01 Feb 2025\nDeviance:\n8.8819e+06\n\n\nTime:\n01:40:19\nPearson chi2:\n8.88e+06\n\n\nNo. Iterations:\n44\nPseudo R-squ. (CS):\n0.2717\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n2.3772\n0.034\n70.483\n0.000\n2.311\n2.443\n\n\nWorks Count\n0.0001\n1.97e-06\n67.398\n0.000\n0.000\n0.000\n\n\nAny Coding\n0.1388\n0.043\n3.224\n0.001\n0.054\n0.223\n\n\nMajority Coding\n-0.0674\n0.039\n-1.734\n0.083\n-0.144\n0.009\n\n\nAlways Coding\n-0.2432\n0.044\n-5.481\n0.000\n-0.330\n-0.156\n\n\nCommon Author Position Last\n1.0456\n0.034\n30.622\n0.000\n0.979\n1.113\n\n\nCommon Author Position Middle\n0.7432\n0.034\n21.809\n0.000\n0.676\n0.810\n\n\nAny Coding * Common Author Position Last\n-0.2793\n0.045\n-6.265\n0.000\n-0.367\n-0.192\n\n\nAny Coding * Common Author Position Middle\n-0.4483\n0.044\n-10.200\n0.000\n-0.534\n-0.362\n\n\nMajority Coding * Common Author Position Last\n-0.3515\n0.048\n-7.310\n0.000\n-0.446\n-0.257\n\n\nMajority Coding * Common Author Position Middle\n-0.6074\n0.044\n-13.824\n0.000\n-0.694\n-0.521\n\n\nAlways Coding * Common Author Position Last\n-0.3680\n0.074\n-4.970\n0.000\n-0.513\n-0.223\n\n\nAlways Coding * Common Author Position Middle\n-0.5090\n0.067\n-7.641\n0.000\n-0.640\n-0.378\n\n\n\n\n\n\n\n\n\n\n\n\nTable 13: Researcher H-Index by Coding Status Controlled by Most Freq. Domain\n\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nh_index\nNo. Observations:\n49355\n\n\nModel:\nGLM\nDf Residuals:\n49338\n\n\nModel Family:\nGaussian\nDf Model:\n16\n\n\nLink Function:\nLog\nScale:\n196.53\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-2.0034e+05\n\n\nDate:\nSat, 01 Feb 2025\nDeviance:\n9.6964e+06\n\n\nTime:\n01:40:19\nPearson chi2:\n9.70e+06\n\n\nNo. Iterations:\n48\nPseudo R-squ. (CS):\n0.1865\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n3.3235\n0.018\n187.923\n0.000\n3.289\n3.358\n\n\nWorks Count\n0.0001\n2.06e-06\n66.499\n0.000\n0.000\n0.000\n\n\nAny Coding\n-0.3929\n0.044\n-8.865\n0.000\n-0.480\n-0.306\n\n\nMajority Coding\n-1.4465\n0.101\n-14.290\n0.000\n-1.645\n-1.248\n\n\nAlways Coding\n-1.2212\n0.186\n-6.577\n0.000\n-1.585\n-0.857\n\n\nCommon Domain Life Sciences\n0.0976\n0.025\n3.841\n0.000\n0.048\n0.147\n\n\nCommon Domain Physical Sciences\n-0.1455\n0.018\n-8.024\n0.000\n-0.181\n-0.110\n\n\nCommon Domain Social Sciences\n-0.1612\n0.030\n-5.314\n0.000\n-0.221\n-0.102\n\n\nAny Coding * Common Domain Life Sciences\n0.1064\n0.058\n1.847\n0.065\n-0.006\n0.219\n\n\nAny Coding * Common Domain Physical Sciences\n0.0769\n0.045\n1.709\n0.087\n-0.011\n0.165\n\n\nAny Coding * Common Domain Social Sciences\n0.0053\n0.073\n0.073\n0.942\n-0.137\n0.148\n\n\nMajority Coding * Common Domain Life Sciences\n0.8106\n0.117\n6.920\n0.000\n0.581\n1.040\n\n\nMajority Coding * Common Domain Physical Sciences\n0.7001\n0.102\n6.849\n0.000\n0.500\n0.900\n\n\nMajority Coding * Common Domain Social Sciences\n0.7242\n0.133\n5.461\n0.000\n0.464\n0.984\n\n\nAlways Coding * Common Domain Life Sciences\n0.3295\n0.211\n1.561\n0.119\n-0.084\n0.743\n\n\nAlways Coding * Common Domain Physical Sciences\n0.2526\n0.188\n1.347\n0.178\n-0.115\n0.620\n\n\nAlways Coding * Common Domain Social Sciences\n0.3009\n0.220\n1.368\n0.171\n-0.130\n0.732\n\n\n\n\n\n\n\n\n\n\n\n\nTable 14: Researcher H-Index by Coding Status Controlled by Most Freq. Article Type\n\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nh_index\nNo. Observations:\n49355\n\n\nModel:\nGLM\nDf Residuals:\n49342\n\n\nModel Family:\nGaussian\nDf Model:\n12\n\n\nLink Function:\nLog\nScale:\n194.43\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-2.0008e+05\n\n\nDate:\nSat, 01 Feb 2025\nDeviance:\n9.5937e+06\n\n\nTime:\n01:40:19\nPearson chi2:\n9.59e+06\n\n\nNo. Iterations:\n47\nPseudo R-squ. (CS):\n0.1970\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n3.0971\n0.006\n527.713\n0.000\n3.086\n3.109\n\n\nWorks Count\n0.0001\n1.99e-06\n64.413\n0.000\n0.000\n0.000\n\n\nAny Coding\n-0.2953\n0.011\n-27.851\n0.000\n-0.316\n-0.274\n\n\nMajority Coding\n-0.7661\n0.021\n-37.076\n0.000\n-0.807\n-0.726\n\n\nAlways Coding\n-0.9917\n0.039\n-25.315\n0.000\n-1.068\n-0.915\n\n\nCommon Article Type Research Article\n0.1816\n0.008\n23.690\n0.000\n0.167\n0.197\n\n\nCommon Article Type Software Article\n0.2247\n0.056\n4.040\n0.000\n0.116\n0.334\n\n\nAny Coding * Common Article Type Research Article\n-0.0209\n0.015\n-1.425\n0.154\n-0.050\n0.008\n\n\nAny Coding * Common Article Type Software Article\n0.1893\n0.100\n1.889\n0.059\n-0.007\n0.386\n\n\nMajority Coding * Common Article Type Research Article\n0.0071\n0.028\n0.256\n0.798\n-0.047\n0.062\n\n\nMajority Coding * Common Article Type Software Article\n0.3646\n0.090\n4.049\n0.000\n0.188\n0.541\n\n\nAlways Coding * Common Article Type Research Article\n0.0043\n0.052\n0.083\n0.934\n-0.097\n0.105\n\n\nAlways Coding * Common Article Type Software Article\n0.3653\n0.107\n3.400\n0.001\n0.155\n0.576"
  },
  {
    "objectID": "qss-code-authors.html#analysis-of-project-duration-and-percentage-code-contributors-who-are-authors",
    "href": "qss-code-authors.html#analysis-of-project-duration-and-percentage-code-contributors-who-are-authors",
    "title": "Code Contribution and Authorship",
    "section": "7.2 Analysis of Project Duration and Percentage Code-Contributors Who Are Authors",
    "text": "7.2 Analysis of Project Duration and Percentage Code-Contributors Who Are Authors\nIn our pre-registered analysis plan (https://osf.io/fc74m), we originally hypothesized about the relationship between project duration and authorship recognition. Specifically, we posited that sustained technical engagement and academic recognition might be meaningfully related, with longer project durations potentially leading to higher rates of code-contributor authorship. We saw repository histories as providing a unique opportunity to examine this relationship, leading us to hypothesize that projects with longer commit durations would be associated with higher percentages of developers receiving authorship recognition (pre-registered as H2).\nHowever, our analysis found no evidence to support this hypothesis. When examining the relationship between a repository’s commit duration and the percentage of developers who receive authorship recognition, we found no significant correlation (r = -0.00, p = n.s.). This suggests that the length of time a project has been in development has no meaningful relationship with the proportion of developers who are recognized as authors.\nWe ultimately decided to move this analysis to the appendix for two key methodological reasons. First, our approach of using repository-level commit duration as a proxy for individual contribution patterns proved too coarse-grained. A more precise analysis would need to examine individual-level contribution durations and patterns rather than overall project length. Second, our method did not account for the varying levels of contribution that different developers make to a repository. Simply correlating overall project duration with authorship rates fails to capture the nuanced ways that sustained, meaningful technical contributions might influence authorship decisions.\nThese limitations suggest potential directions for future work that could more rigorously examine the relationship between long-term technical engagement and academic recognition. Such work might benefit from more granular analysis of individual contribution patterns, perhaps incorporating measures of contribution significance and sustainability rather than just temporal duration."
  }
]